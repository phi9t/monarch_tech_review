<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/favicon.ico" />
    <!-- Preload is necessary because we show these images when we disconnect from the server,
    but at that point we cannot load these images from the server -->
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/gradient-yHQUC_QB.png" as="image" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/noise-60BoTA8O.png" as="image" />
    <!-- Preload the fonts -->
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/Lora-VariableFont_wght-B2ootaw-.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/PTSans-Regular-CxL0S8W7.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/PTSans-Bold-D9fedIX3.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/FiraMono-Regular-BTCkDNvf.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/FiraMono-Medium-DU3aDxX5.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/FiraMono-Bold-CLVRCuM9.ttf" as="font" crossorigin="anonymous" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta name="description" content="a marimo app" />
    <link rel="apple-touch-icon" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/apple-touch-icon.png" />
    <link rel="manifest" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/manifest.json" />

    <script data-marimo="true">
      function __resizeIframe(obj) {
        const scrollbarHeight = 20; // Max between windows, mac, and linux

        function setHeight() {
          // Guard against race condition where iframe isn't ready
          if (!obj.contentWindow?.document?.documentElement) {
            return;
          }
          const element = obj.contentWindow.document.documentElement;
          // If there is no vertical scrollbar, we don't need to resize the iframe
          if (element.scrollHeight === element.clientHeight) {
            return;
          }

          // Create a new height that includes the scrollbar height if it's visible
          const hasHorizontalScrollbar = element.scrollWidth > element.clientWidth;
          const newHeight = element.scrollHeight + (hasHorizontalScrollbar ? scrollbarHeight : 0);

          // Only update the height if it's different from the current height
          if (obj.style.height !== `${newHeight}px`) {
            obj.style.height = `${newHeight}px`;
          }
        }

        // Resize the iframe to the height of the content and bottom scrollbar height
        setHeight();

        // Resize the iframe when the content changes
        const resizeObserver = new ResizeObserver((_entries) => {
          setHeight();
        });
        // Only observe if iframe content is ready
        if (obj.contentWindow?.document?.body) {
          resizeObserver.observe(obj.contentWindow.document.body);
        }
      }
    </script>
    <marimo-filename hidden>06_rdma_weight_sync.py</marimo-filename>
    <!-- TODO(Trevor): Legacy, required by VS Code plugin. Remove when plugin is updated (see marimo/server/_templates/template.py) -->
    <marimo-version data-version="{{ version }}" hidden></marimo-version>
    <marimo-user-config data-config="{{ user_config }}" hidden></marimo-user-config>
    <marimo-server-token data-token="{{ server_token }}" hidden></marimo-server-token>
    <!-- /TODO -->
    <title>06 rdma weight sync</title>
    <script type="module" crossorigin crossorigin="anonymous" src="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/index-DGasP9Lh.js"></script>
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/preload-helper-DItdS47A.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/clsx-D8GwTfvk.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/cn-BKtXLv3a.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/chunk-LvLJmgfZ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/react-BGmjiNul.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/compiler-runtime-DeeZ7FnK.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/jsx-runtime-ZmTK25f3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/badge-Ce8wRjuQ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/hotkeys-BHHWjLlp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useEventListener-DIUKKfEy.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/button-YC1gW_kJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/react-dom-C9fstfnp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/Combination-CMPwuAmi.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/menu-items-CJhvWPOk.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dist-uzvC4uAK.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/createLucideIcon-CnW3RofX.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/check-DdfN0k2d.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/select-V5IdpNiR.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/tooltip-CEc2ajau.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/use-toast-rmUWldD_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/_Uint8Array-BGESiCQL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/_baseIsEqual-B9N9Mw_N.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useEvent-DO6uJBas.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/invariant-CAG_dYON.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/_baseFor-Duhs3RiJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/merge-BBX6ug-N.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/zod-Cg4WLWh2.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/utils-DXvhzCGS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/constants-B6Cb__3x.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/Deferred-CrO5-0RA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/config-CIrPQIbt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/uuid-DercMavo.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/DeferredRequestRegistry-CO2AyNfd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/requests-BsVD4CdD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/isSymbol-BGkTcW3U.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/toString-DlRqgfqz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/_hasUnicode-CWqKLxBC.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/assertNever-CBU83Y6o.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/_arrayReduce-TT0iOGKY.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useLifecycle-D35CBukS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useNonce-_Aax6sXd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useTheme-DUdVAZI8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/once-Bul8mtFs.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/capabilities-MM7JYRxj.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/createReducer-Dnna-AUO.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dist-DBwNzi3C.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dist-ChS0Dc_R.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dist-CtsanegT.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dist-BIKFl48f.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dist-B0VqT_4z.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dist-TiFCI16_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dist-Cayq-K1c.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dist-BYyu59D8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dist-Gqv0jSNr.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/stex-CtmkcLz7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/toDate-CgbKQM5E.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/cjs-CH5Rj0g8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/_baseProperty-NKyJO2oh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/now-6sUe0ZdD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/debounce-B3mjKxHe.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/toInteger-CDcO32Gx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/database-zap-B9y7063w.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/main-U5Goe76G.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/cells-BpZ7g6ok.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/spinner-DaIKav-i.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/chevron-right-DwagBitu.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dropdown-menu-B-6unW-7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/kbd-C3JY7O_u.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/renderShortcut-DEwfrKeS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/multi-map-C8GlnP-4.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/alert-BrGyZf9c.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/alert-dialog-DwQffb13.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dialog-CxGKN4C_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dist-CdxIjAOP.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/label-Be1daUcS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useDebounce-D5NcotGm.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/textarea-DBO30D7K.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/numbers-iQunIAXf.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/SSRProvider-CEHRCdjA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/context-JwD-oSsl.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useNumberFormatter-c6GXymzg.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/usePress-Bup4EGrp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/input-pAun1m1X.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/links-DHZUhGz-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/popover-Gz-GJzym.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/switch-8sn_4qbh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/table-C8uQmBAN.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/mode-DX8pdI-l.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useAsyncData-C4XRy1BE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/errors-2SszdW9t.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/error-banner-DUzsIXtq.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/copy-Bv2DBpIS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/memoize-BCOZVFBt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/get-6uJrSKbw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/capitalize-CmNnkG9y.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/copy-CQ15EONK.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/plus-BD5o34_i.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/refresh-cw-CQd-1kjx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/trash-2-CyqGun26.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/triangle-alert-B65rDESJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/ai-model-dropdown-71lgLrLy.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/defaultLocale-D_rSvXvJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/precisionRound-BMPhtTJQ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/defaultLocale-C92Rrpmf.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/vega-loader.browser-CRZ52CKf.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/tooltip-BGrCWNss.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/ErrorBoundary-ChCiwl15.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useInstallPackage-Bdnnp5fe.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/ImperativeModal-CUbWEBci.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/cell-link-Bw5bzt4a.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/datasource-B0OJBphG.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/state-BfXVTTtD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/MarimoErrorOutput-5rudBbo3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/copy-icon-BhONVREY.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/html-to-image-DjukyIj4.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/focus-D51fcwZX.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/LazyAnyLanguageCodeMirror-yzHjsVJt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/chunk-5FQGJX7Z-CVUXBqX6.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/katex-Dc8yG8NU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/markdown-renderer-DhMlG2dP.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/command-DhzFN2CJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/download-BhCZMKuQ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useRunCells-24p6hn99.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/purify.es-DNVQZNFu.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/RenderHTML-CQZqVk1Z.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useIframeCapabilities-DuIDx9mD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/formats-W1SWxSE3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/en-US-pRRbZZHE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/isValid-DcYggVWP.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/dates-Dhn1r-h6.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/maps-t9yNKYA8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/extends-B2LJnKU3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/emotion-is-prop-valid.esm-DD4AwVTU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useDateFormatter-CS4kbWl2.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/range-D2UKkEg-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/table-DZR6ewbN.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/JsonOutput-CknFTI_u.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/file-Cs1JbsV6.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/play-BPIh-ZEU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/chat-components-CGlO4yUw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/isEmpty-CgX_-6Mt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/chat-display-B4mGvJ0X.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useDeleteCell-5uYlTcQZ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/icons-BhEXrzsb.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/process-output-CagdHMzs.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/blob-CuXvdYPX.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/objectWithoutPropertiesLoose-DaPAPabU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/esm-DpMp6qko.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/add-cell-with-ai-pVFp5LZG.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/chart-no-axes-column-W42b2ZIs.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/square-function-CqXXKtIq.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/spec-D1kBp3jX.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/column-preview-CxMrs0B_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/toggle-jWKnIArU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/globals-DKH14XH0.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/share-CbPtIlnM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/_baseSet-5Rdwpmr3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/react-resizable-panels.browser.esm-Ctj_10o2.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/utilities.esm-CIPARd6-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/floating-outline-DcxjrFFt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useAddCell-BmeZUK02.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/eye-off-BhExYOph.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/readonly-python-code-DyP9LVLc.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/file-video-camera-DW3v07j2.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/types-DuQOSW7G.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/refresh-ccw-DLEiQDS3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/form-DUA_Rz_a.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/field-BEg1eC0P.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useBoolean-B1Xeh6vA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/useDeepCompareMemoize-ZPd9PxYl.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/types-CS34eOZi.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/prop-types-BiQYf0aU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/es-D8BOePqo.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/hasIn-CycJImp8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/_baseFlatten-CUZNxU8H.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/flatten-D-7VEN0q.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/pick-B_6Qi5aM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/code-xml-XLwHyDBr.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/download-B9SUL40m.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/house-DhFkiXz7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/settings-DOXWMfVd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/square-C8Tw_XXG.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/bundle.esm-2AjO7UK5.js">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/cells-jmgGt1lS.css">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/markdown-renderer-DdDKmWlR.css">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/JsonOutput-B7vuddcd.css">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.7/dist/assets/index-CikhHYAB.css">
  
<script data-marimo="true">
    window.__MARIMO_STATIC__ = {};
    window.__MARIMO_STATIC__.files = {};
</script>
</head>
  <body>
    <div id="root"></div>
    <!-- This is a portal for the data editor to render in -->
    <div id="portal" data-testid="glide-portal" style="position: fixed; left: 0; top: 0; z-index: 9999"></div>
    <script data-marimo="true">
      window.__MARIMO_MOUNT_CONFIG__ = {
            "filename": "06_rdma_weight_sync.py",
            "mode": "read",
            "version": "0.19.7",
            "serverToken": "static",
            "config": {"ai": {"custom_providers": {}, "models": {"custom_models": [], "displayed_models": []}}, "completion": {"activate_on_typing": true, "copilot": false, "signature_hint_on_typing": false}, "diagnostics": {"sql_linter": true}, "display": {"cell_output": "below", "code_editor_font_size": 14, "dataframes": "rich", "default_table_max_columns": 50, "default_table_page_size": 10, "default_width": "medium", "reference_highlighting": true, "theme": "light"}, "formatting": {"line_length": 79}, "keymap": {"overrides": {}, "preset": "default"}, "language_servers": {"pylsp": {"enable_flake8": false, "enable_mypy": true, "enable_pydocstyle": false, "enable_pyflakes": false, "enable_pylint": false, "enable_ruff": true, "enabled": false}}, "mcp": {"mcpServers": {}, "presets": []}, "package_management": {"manager": "uv"}, "runtime": {"auto_instantiate": false, "auto_reload": "off", "default_csv_encoding": "utf-8", "default_sql_output": "auto", "on_cell_change": "autorun", "output_max_bytes": 8000000, "reactive_tests": true, "std_stream_max_bytes": 1000000, "watcher_on_save": "lazy"}, "save": {"autosave": "after_delay", "autosave_delay": 1000, "format_on_save": false}, "server": {"browser": "default", "follow_symlink": false}, "snippets": {"custom_paths": [], "include_default_snippets": true}},
            "configOverrides": {},
            "appConfig": {"sql_output": "auto", "width": "medium"},
            "view": {"showAppCode": true},
            "notebook": {"cells": [{"code": "import marimo as mo", "code_hash": "1d0db38904205bec4d6f6f6a1f6cec3e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Hbol", "name": "_"}, {"code": "mo.md(r\"\"\"\n# RDMA \u0026 Weight Synchronization\n\nThis notebook explores efficient weight synchronization for async RL systems:\n\n1. **The Bandwidth Hierarchy** - NVLink, InfiniBand, PCIe\n2. **The Problem: Collectives Are Blocking** - Why RL needs something different\n3. **How RDMA Works** - ibverbs, one-sided operations\n4. **The Magic Pointer Pattern** - Control plane vs data plane separation\n5. **CPU Staging** - Decoupling trainer and generator timing\n6. **Circular Weight Buffers** - Versioning without memory churn\n7. **Weight Re-sharding** - Handling different tensor layouts\n8. **Putting It All Together** - The complete pattern\n\"\"\")", "code_hash": "5d6ef71d15248a566ba53502517e14e6", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "MJUe", "name": "_"}, {"code": "mo.md(r\"\"\"\n## 1. The Bandwidth Hierarchy\n\nModern HPC clusters have multiple interconnects with vastly different bandwidths:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                              NODE A                                                      \u2502\n\u2502                                                                                                          \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502    \u2502                              NVSwitch / NVLink Fabric                                         \u2502     \u2502\n\u2502    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502     \u2502\n\u2502    \u2502  \u2502GPU 0 \u2502 \u2502GPU 1 \u2502 \u2502GPU 2 \u2502 \u2502GPU 3 \u2502 \u2502GPU 4 \u2502 \u2502GPU 5 \u2502 \u2502GPU 6 \u2502 \u2502GPU 7 \u2502                      \u2502     \u2502\n\u2502    \u2502  \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518                      \u2502     \u2502\n\u2502    \u2502     ########################################################################  900 GB/s NVLink \u2502     \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                              \u2502                                                           \u2502\n\u2502                                         ======  64 GB/s PCIe                                             \u2502\n\u2502                                              \u2502                                                           \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  ------ 48 GB/s ------ \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502    \u2502  CPU 0  \u2502     CPU interconnect   \u2502  CPU 1  \u2502 ====== 64 GB/s \u2550\u2550\u2502 NIC 0 \u2502          \u2502 NIC 1 \u2502          \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518      PCIe        \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518          \u2502\n\u2502         \u2502                                  \u2502                           \u2502                  \u2502              \u2502\n\u2502         \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 64 GB/s PCIe \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a              \u2502\n\u2502                                                                        \u2502                  \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                         \u2502                  \u2502\n                                                                       ======  50 GB/s   ======\n                                                                    IB NDR400         IB NDR400\n                                                                         \u2502                  \u2502\n                                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                        \u2502                                                    \u2502\n                                                        \u2502              InfiniBand Switch                     \u2502\n                                                        \u2502                                                    \u2502\n                                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                         \u2502                  \u2502\n                                                                       ======  50 GB/s   ======\n                                                                    IB NDR400         IB NDR400\n                                                                         \u2502                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                                        \u2502                  \u2502              \u2502\n\u2502         \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 64 GB/s PCIe \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a              \u2502\n\u2502         \u2502                                  \u2502                           \u2502                  \u2502              \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510                        \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510      PCIe        \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510          \u2502\n\u2502    \u2502  CPU 0  \u2502     CPU interconnect   \u2502  CPU 1  \u2502 ====== 64 GB/s \u2550\u2550\u2502 NIC 0 \u2502          \u2502 NIC 1 \u2502          \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ------ 48 GB/s ------  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502                                              \u2502                                                           \u2502\n\u2502                                           ======  64 GB/s PCIe                                           \u2502\n\u2502                                              \u2502                                                           \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502    \u2502     ########################################################################  900 GB/s NVLink \u2502     \u2502\n\u2502    \u2502  \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510                      \u2502     \u2502\n\u2502    \u2502  \u2502GPU 0 \u2502 \u2502GPU 1 \u2502 \u2502GPU 2 \u2502 \u2502GPU 3 \u2502 \u2502GPU 4 \u2502 \u2502GPU 5 \u2502 \u2502GPU 6 \u2502 \u2502GPU 7 \u2502                      \u2502     \u2502\n\u2502    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502     \u2502\n\u2502    \u2502                              NVSwitch / NVLink Fabric                                         \u2502     \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                              NODE B                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nBandwidth encoding (line intensity):\n  ########  NVLink/NVSwitch   900 GB/s   (GPU \u2194 GPU, same node)\n  ========  PCIe Gen5 / IB     50-64 GB/s (CPU\u2194GPU, CPU\u2194NIC, cross-node)\n  --------  CPU interconnect   48 GB/s   (CPU \u2194 CPU, same node)\n```\n\nRDMA can transfer between any registered memory (CPU or GPU) via the NICs.\n\n| Interconnect | Bandwidth | Latency | Use Case |\n|--------------|-----------|---------|----------|\n| **NVLink/NVSwitch** | 900 GB/s | ~1 \u03bcs | Same-node GPU\u2194GPU |\n| **InfiniBand NDR400** | 50 GB/s | ~1-2 \u03bcs | Cross-node RDMA |\n| **PCIe Gen5 x16** | 64 GB/s | ~1-2 \u03bcs | CPU\u2194GPU, CPU\u2194NIC |\n| **CPU interconnect** | 48 GB/s | ~100 ns | CPU\u2194CPU (same node) |\n\n**Key observations:**\n\n1. **NVLink dominates** - 900 GB/s is ~18x faster than cross-node RDMA. Same-node GPU\u2194GPU\n   communication is nearly free compared to crossing the network.\n\n2. **RDMA \u003E\u003E Ethernet** - InfiniBand/RoCE at 50 GB/s is ~4x faster than 100GbE (12.5 GB/s),\n   plus kernel bypass and lower latency. Worth the complexity for HPC workloads.\n\n3. **PCIe is faster than you'd think** - At 64 GB/s, CPU\u2194GPU transfers aren't the bottleneck\n   people often assume. The real cost is synchronization, not bandwidth.\n\n**Rule of thumb**: Place the most bandwidth-intensive, frequent operations on NVLink\n(gradients, activations). Use RDMA for cross-node communication (weight sync, sharding).\nPCIe is fine for occasional CPU\u2194GPU transfers.\n\nWe'll focus primarily on **NVLink** and **RDMA** for this notebook. Most people use these\nvia **collectives**, exposed through PyTorch distributed:\n\n```python\nimport torch.distributed as dist\n\n# Initialize process group - NCCL uses NVLink (same-node) and RDMA (cross-node)\ndist.init_process_group(backend=\"nccl\")\n\n# All-reduce: average gradients across all GPUs\ndist.all_reduce(gradients, op=dist.ReduceOp.SUM)\ngradients /= world_size\n\n# All-gather: collect tensors from all ranks\ngathered = [torch.empty_like(tensor) for _ in range(world_size)]\ndist.all_gather(gathered, tensor)\n\n# Broadcast: send from rank 0 to all others\ndist.broadcast(weights, src=0)\n```\n\nThis works great for training. But for RL weight sync, we need something different...\n\"\"\")", "code_hash": "899037a1e41177991a152496300fd26d", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "vblA", "name": "_"}, {"code": "import torch", "code_hash": "e2ddceb8064dfb2235ad898d34eee19a", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "bkHC", "name": "_"}, {"code": "mo.md(r\"\"\"\n## 2. The Problem: Collectives Are Blocking\n\nCollectives work great for training - everyone computes gradients, then synchronizes.\nBut async RL has a different access pattern.\n\n### High Variance in Generation Times\n\nGenerators have wildly different completion times:\n- Some prompts \u2192 10 tokens (fast)\n- Other prompts \u2192 1000 tokens (slow)\n\nWith collectives, fast generators wait for slow ones:\n\n```\nGenerator 0: \u251c\u2500\u2500 gen (fast) \u2500\u2500\u2524  \u26a0\ufe0f WAITING...\nGenerator 1: \u251c\u2500\u2500\u2500\u2500\u2500\u2500 gen (slow) \u2500\u2500\u2500\u2500\u2500\u2500\u2524\nGenerator 2: \u251c\u2500\u2500 gen (fast) \u2500\u2500\u2524  \u26a0\ufe0f WAITING...\n                                      \u2193\n                          all_gather(weights)  # Everyone waits!\n```\n\n### What About send/recv?\n\nPyTorch distributed does have point-to-point primitives:\n\n```python\n# Sender side\ndist.send(tensor, dst=receiver_rank)\n\n# Receiver side\ndist.recv(tensor, src=sender_rank)\n```\n\nBut this is **two-sided** - both sender and receiver must coordinate:\n- Receiver must call `recv()` before sender's `send()` completes\n- Trainer would need to wait until generators are ready to receive\n- Still blocking on coordination!\n\n### The One-Sided Solution: RDMA\n\nWhat if the sender could write directly to the receiver's memory without coordination?\n\n```\nTwo-sided (send/recv):\n  Sender: \"I have data\"  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba  Receiver: \"I'm ready\"\n  Sender: sends data     \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba  Receiver: receives data\n                         2 messages required\n\nOne-sided (RDMA):\n  Sender: writes directly to receiver's memory\n                         No coordination needed!\n```\n\nThis is what RDMA enables: **one-sided memory operations**.\n\"\"\")", "code_hash": "80fe0a96a7dd614040bf54159f19bd79", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "lEQa", "name": "_"}, {"code": "mo.md(r\"\"\"\n## 3. How RDMA Works (ibverbs)\n\nRDMA (Remote Direct Memory Access) lets one machine read/write another machine's memory\ndirectly, bypassing the kernel and CPU on both sides.\n\n### The ibverbs Stack\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Application (PyTorch, Monarch, etc.)                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  libibverbs  (userspace RDMA API)                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Provider driver (mlx5, efa, rxe, etc.)                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Hardware (InfiniBand NIC, RoCE NIC, etc.)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nWe'll focus on **InfiniBand** and **RoCE** (RDMA over Converged Ethernet).\nOther transports like AWS EFA exist but we won't cover them here.\n\n### Key RDMA Operations\n\n| Operation | Description |\n|-----------|-------------|\n| `RDMA_WRITE` | Write to remote memory (one-sided) |\n| `RDMA_READ` | Read from remote memory (one-sided) |\n| `SEND/RECV` | Two-sided messaging (like TCP) |\n\nThe magic is in `RDMA_WRITE` and `RDMA_READ` - they're **one-sided**:\n- Remote CPU is not involved\n- Remote application doesn't need to call anything\n- NIC handles everything in hardware\n\n### Memory Registration\n\nBefore RDMA, memory must be **registered** with the NIC:\n\n```python\n# Conceptually (actual ibverbs API is in C)\nmr = rdma_register_memory(buffer, size)\n# Returns:\n#   - lkey: local access key (for local operations)\n#   - rkey: remote access key (share with remote peer)\n#   - addr: physical/virtual address\n```\n\nThe `(addr, rkey)` pair is a **remote-accessible pointer**. Share it with a peer,\nand they can read/write your memory directly.\n\n### Queue Pair Setup\n\nBefore any RDMA operations, you need to establish a **Queue Pair (QP)** between\nsender and receiver. This is a one-time connection setup:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Sender    \u2502                           \u2502  Receiver   \u2502\n\u2502             \u2502                           \u2502             \u2502\n\u2502  Create QP  \u2502 \u2500\u2500\u2500 exchange QP info \u2500\u2500\u2500\u25ba \u2502  Create QP  \u2502\n\u2502  (qp_num,   \u2502 \u25c4\u2500\u2500 (qp_num, lid, gid) \u2500\u2500 \u2502             \u2502\n\u2502   lid, gid) \u2502                           \u2502             \u2502\n\u2502             \u2502                           \u2502             \u2502\n\u2502  Move QP to \u2502                           \u2502  Move QP to \u2502\n\u2502  RTR \u2192 RTS  \u2502                           \u2502  RTR \u2192 RTS  \u2502\n\u2502             \u2502                           \u2502             \u2502\n\u2502  Now ready  \u2502 \u2550\u2550\u2550 RDMA operations \u2550\u2550\u2550\u2550\u25ba \u2502  Now ready  \u2502\n\u2502  for RDMA!  \u2502                           \u2502  for RDMA!  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nThis is where **Monarch actors** shine. Because you can spawn arbitrary actors,\nyou can create **RDMA Manager actors** that:\n- Initialize QPs on their respective hosts\n- Exchange QP info via actor messages\n- Manage the connection lifecycle\n\n```python\n# Monarch pattern: RDMA managers as actors\nclass RDMAManager(Actor):\n    def __init__(self):\n        self.qp = create_queue_pair()\n        self.qp_info = get_qp_info(self.qp)  # (qp_num, lid, gid)\n\n    @endpoint\n    def get_qp_info(self) -\u003E QpInfo:\n        return self.qp_info\n\n    @endpoint\n    def connect(self, remote_qp_info: QpInfo):\n        # Transition QP: INIT \u2192 RTR \u2192 RTS\n        connect_qp(self.qp, remote_qp_info)\n\n# Setup: exchange QP info via actor messages, then RDMA is ready\ntrainer_info = trainer_rdma.get_qp_info.call_one().get()\ngenerator_rdma.connect.call_one(trainer_info).get()\n```\n\nThe actor abstraction makes RDMA connection management natural and composable.\n\n### Monarch Using Monarch: RdmaController\n\nHere's the cool part: **Monarch uses itself** to manage RDMA infrastructure. Looking at\nthe actual Python code in `monarch/_src/rdma/rdma.py`:\n\n```python\n# From Monarch's RDMA implementation\nfrom monarch._src.actor.proc_mesh import get_or_spawn_controller\n\nclass RdmaController(Actor):\n    '''Singleton controller that coordinates RDMA initialization.'''\n\n    def __init__(self):\n        # Track which proc meshes have RDMA initialized\n        self._manager_futures: dict[ProcMesh, Future[RdmaManager]] = {}\n\n    @endpoint\n    async def init_rdma_on_mesh(self, proc_mesh: ProcMesh) -\u003E None:\n        '''Lazily initialize RDMA on a proc mesh.'''\n        if proc_mesh not in self._manager_futures:\n            self._manager_futures[proc_mesh] = Future(\n                coro=RdmaManager.create(proc_mesh)\n            )\n        await self._manager_futures[proc_mesh]\n\n# Cached initialization - only runs once per process\n@functools.cache\ndef _ensure_init_rdma_manager():\n    async def task():\n        controller = await get_or_spawn_controller(\"rdma_controller\", RdmaController)\n        await controller.init_rdma_on_mesh.call_one(current_proc_mesh())\n    return spawn_task(task())\n```\n\nThis is **Monarch building Monarch** - the RDMA subsystem uses the same patterns:\n\n- `get_or_spawn_controller(\"rdma_controller\", RdmaController)` ensures one global controller\n- The controller lazily initializes RDMA managers per proc mesh\n- `@functools.cache` ensures we only bootstrap once per process\n- Under the hood, the actual RDMA operations are in Rust (`RdmaManagerActor`)\n\nIt's actors all the way down.\n\n### Why This Matters for Weight Sync\n\nRemember: CPU memory AND GPU memory can both be registered for RDMA.\n\n```\nTrainer:\n  1. Register weight buffer with RDMA NIC\n  2. Get (addr, rkey) handle\n  3. Share handle with generators (tiny message)\n\nGenerator:\n  1. Receive handle\n  2. RDMA_READ directly from trainer's memory\n  3. No coordination with trainer needed!\n```\n\nThe trainer doesn't even know when generators pull weights. True one-sided.\n\"\"\")", "code_hash": "31ba79419d9ad47dff056f46fc102fbb", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "PKri", "name": "_"}, {"code": "mo.md(r\"\"\"\n## 4. The Magic Pointer Pattern\n\nNow here's the key insight from our RDMA discussion: to represent remote data,\nwe only need a **tiny handle** - the `(addr, rkey, size)` tuple.\n\nMonarch wraps this in `RDMABuffer`. Let's see how small it actually is:\n\"\"\")", "code_hash": "be16282c591c150987f44e4b1d502bd2", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Xref", "name": "_"}, {"code": "# Central imports for all RDMA examples in this notebook\nimport time\nfrom monarch.actor import Actor, endpoint, this_host, current_rank\nfrom monarch.rdma import RDMABuffer, RDMAAction, is_rdma_available", "code_hash": "93cd1380c225976a61d1164279118d8e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "SFPL", "name": "_"}, {"code": "# Measure actual size of RDMABuffer handles\nimport pickle\n\ndef show_fallback():\n    \"\"\"Fallback: show expected sizes based on RDMABuffer structure.\"\"\"\n    print(\"(RDMA not available - showing expected handle sizes)\\n\")\n    print(\"RDMABuffer contains: addr (8B) + rkey (4B) + size (8B) + owner (~100B)\")\n    print(\"Total serialized size: ~150-200 bytes regardless of tensor size\\n\")\n\n    sizes = [(\"1 KB\", 1024), (\"1 MB\", 1024**2), (\"1 GB\", 1024**3)]\n    handle_bytes = 150  # approximate\n\n    for name, tensor_bytes in sizes:\n        ratio = tensor_bytes / handle_bytes\n        print(f\"{name:\u003C8} tensor \u2192 ~150 byte handle \u2192 {ratio:,.0f}x compression\")\n\n    print(\"\\n\u2192 Handle size is O(1) regardless of tensor size!\")\n\ntry:\n    if not is_rdma_available():\n        show_fallback()\n    else:\n        class BufferSizeDemo(Actor):\n            \"\"\"Actor that creates RDMABuffers and measures their size.\"\"\"\n\n            @endpoint\n            def measure_buffer_sizes(self) -\u003E list:\n                import pickle as _pickle\n                results = []\n                sizes = [\n                    (\"1 KB\", 256),\n                    (\"1 MB\", 256 * 1024),\n                    (\"10 MB\", 256 * 1024 * 10),\n                ]\n\n                for name, numel in sizes:\n                    tensor = torch.randn(numel)\n                    tensor_bytes = tensor.numel() * tensor.element_size()\n\n                    byte_tensor = tensor.view(torch.uint8).flatten()\n                    buffer = RDMABuffer(byte_tensor)\n                    handle_bytes = len(_pickle.dumps(buffer))\n\n                    results.append((name, tensor_bytes, handle_bytes))\n\n                return results\n\n        host = this_host()\n        proc = host.spawn_procs({\"procs\": 1})\n        demo = proc.spawn(\"buffer_demo\", BufferSizeDemo)\n\n        results = demo.measure_buffer_sizes.call_one().get()\n\n        print(\"RDMABuffer handle size vs actual tensor size:\\n\")\n        print(f\"{'Tensor Size':\u003C12} {'Actual Bytes':\u003C15} {'Handle Size':\u003C15} {'Ratio':\u003C10}\")\n        print(\"-\" * 55)\n\n        for name, tensor_bytes, handle_bytes in results:\n            ratio = tensor_bytes / handle_bytes\n            print(f\"{name:\u003C12} {tensor_bytes:\u003E12,} B   {handle_bytes:\u003E6} B        {ratio:\u003E8,.0f}x\")\n\n        print(\"\\n\u2192 Handle size is O(1) regardless of tensor size!\")\n\nexcept Exception as e:\n    print(f\"(RDMA setup failed: {e})\\n\")\n    show_fallback()", "code_hash": "33f52127ad06527402c41752fe53ec8f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "BYtC", "name": "_"}, {"code": "mo.md(r\"\"\"\n### The Magic Pointer\n\nThis is the core pattern: **separate control plane from data plane**.\n\n- **Control plane** (actor messages): Send tiny handle (~100 bytes)\n- **Data plane** (RDMA): Bulk transfer of actual data (~10 GB)\n\nThink of `RDMABuffer` as a **magic pointer** - it's a pointer that works across machines:\n\n```\nTrainer                              Generator\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 weights     \u2502                     \u2502 local copy  \u2502\n\u2502 (10 GB)     \u2502                     \u2502 (empty)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                                   \u2502\n       \u2502  1. Create RDMABuffer             \u2502\n       \u2502     (register memory, get handle) \u2502\n       \u2502                                   \u2502\n       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 2. Send handle \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502  (~100 bytes via actor)\n       \u2502                                   \u2502\n       \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500 3. RDMA read \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  (~10 GB via hardware)\n       \u2502        (no trainer involvement!)  \u2502\n```\n\nThe trainer doesn't even know when generators pull weights. True one-sided.\n\n### RDMABuffer in Action\n\nFrom `monarch.rdma`:\n\n```python\nfrom monarch.rdma import RDMABuffer\n\n# Trainer side: register weights\nweights = torch.randn(1024, 1024, device=\"cuda\")\nbuffer = RDMABuffer(weights.view(torch.uint8).flatten())\n\n# Return buffer as part of an endpoint response\n# This is a TINY message - just the handle!\n@endpoint\ndef get_weight_handle(self) -\u003E RDMABuffer:\n    return self.buffer\n\n# Generator side: receive handle, pull directly into GPU\nhandle = trainer.get_weight_handle.call_one().get()  # Tiny message\ngpu_weights = model.weights.view(torch.uint8).flatten()\nhandle.read_into(gpu_weights).get()                   # Bulk RDMA \u2192 GPU\n```\n\nSee the [GRPO Actor example](https://meta-pytorch.org/monarch/generated/examples/grpo_actor.html)\nfor a minimal implementation showing RDMA data flow. We'll build a more complete\nversion in the following sections.\n\"\"\")", "code_hash": "70a1235b5a21ad6ce9e6c68db0e1da21", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "RGSE", "name": "_"}, {"code": "mo.md(r\"\"\"\n### The Cost of Memory Registration\n\nRDMA memory registration is **expensive**:\n- Pins physical pages (prevents swapping)\n- Creates IOMMU/DMA mappings in the NIC\n- Can take milliseconds for large buffers\n\n**Don't register on the hot path!** Here are three approaches:\n\"\"\")", "code_hash": "a6ad4e4137408bf3a9b4bc76f35bd22a", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Kclp", "name": "_"}, {"code": "mo.md(r\"\"\"\n#### Approach 1: Naive (Bad)\n\nRe-register every transfer - pays MR cost every time:\n\"\"\")", "code_hash": "0bf1eb19c2ff23a73ee6c14e9b288a44", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "emfo", "name": "_"}, {"code": "class NaiveSender(Actor):\n    \"\"\"Creates new RDMABuffer handles every transfer. Expensive!\"\"\"\n\n    def __init__(self, layer_sizes: list):\n        self.layer_sizes = layer_sizes\n        self.layers = [torch.zeros(size, dtype=torch.float32) for size in layer_sizes]\n        for i, layer in enumerate(self.layers):\n            layer.fill_(float(i + 1))\n\n    @endpoint\n    def get_fresh_handles(self) -\u003E list:\n        # BAD: New registration every call!\n        handles = []\n        for size, layer in zip(self.layer_sizes, self.layers):\n            byte_view = layer.view(torch.uint8).flatten()\n            handles.append((size, RDMABuffer(byte_view)))\n        return handles\n\nclass NaiveReceiver(Actor):\n    \"\"\"Receives from naive sender - pays MR cost every step.\"\"\"\n\n    def __init__(self, layer_sizes: list):\n        self.layer_sizes = layer_sizes\n        self.layers = [torch.zeros(size, dtype=torch.float32) for size in layer_sizes]\n        self.rank = current_rank().rank\n\n    @endpoint\n    def receive_step(self, sender: NaiveSender) -\u003E dict:\n        start = time.perf_counter()\n        handles = sender.get_fresh_handles.call_one().get()\n        for i, (size, handle) in enumerate(handles):\n            byte_view = self.layers[i].view(torch.uint8).flatten()\n            handle.read_into(byte_view).get()\n        elapsed_ms = (time.perf_counter() - start) * 1000\n        return {\"elapsed_ms\": elapsed_ms}\n\nprint(\"NaiveSender: Re-registers all parameters on every call\")", "code_hash": "c552a873dcbe31e85ad569761b4bb7ca", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Hstk", "name": "_"}, {"code": "mo.md(r\"\"\"\n#### Approach 2: Contiguous Buffer (Good)\n\nAllocate one buffer, register once, pack params into it:\n\"\"\")", "code_hash": "62ef4c4311f1e1111bdc67ff94370563", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "nWHF", "name": "_"}, {"code": "class ContiguousSender(Actor):\n    \"\"\"One buffer, one MR, registered at startup.\"\"\"\n\n    def __init__(self, layer_sizes: list):\n        self.layer_sizes = layer_sizes\n        total_size = sum(layer_sizes)\n\n        # One contiguous buffer\n        self.buffer = torch.zeros(total_size, dtype=torch.float32)\n        offset = 0\n        for i, size in enumerate(layer_sizes):\n            self.buffer[offset : offset + size].fill_(float(i + 1))\n            offset += size\n\n        # Register ONCE at startup\n        byte_view = self.buffer.view(torch.uint8).flatten()\n        self.handle = RDMABuffer(byte_view)\n\n    @endpoint\n    def get_handle(self) -\u003E tuple:\n        return (len(self.buffer), self.handle)  # Same handle every time!\n\nclass ContiguousReceiver(Actor):\n    \"\"\"Receives from contiguous sender - fast after first step.\"\"\"\n\n    def __init__(self, total_size: int):\n        self.buffer = torch.zeros(total_size, dtype=torch.float32)\n        self.rank = current_rank().rank\n\n    @endpoint\n    def receive_step(self, sender: ContiguousSender) -\u003E dict:\n        start = time.perf_counter()\n        size, handle = sender.get_handle.call_one().get()\n        byte_view = self.buffer.view(torch.uint8).flatten()\n        handle.read_into(byte_view).get()\n        elapsed_ms = (time.perf_counter() - start) * 1000\n        return {\"elapsed_ms\": elapsed_ms}\n\nprint(\"ContiguousSender: Registers once, reuses same handle\")", "code_hash": "e9811c0fdbc3ed8017f0094270e5c842", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "iLit", "name": "_"}, {"code": "mo.md(r\"\"\"\n#### Approach 3: Scattered + RDMAAction (Good)\n\nRegister each parameter once, build RDMAAction transfer plan once, execute repeatedly:\n\"\"\")", "code_hash": "50aa8d04b492714022d299fecd65cec3", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ZHCJ", "name": "_"}, {"code": "class ScatteredSender(Actor):\n    \"\"\"Multiple buffers, each registered once at startup.\"\"\"\n\n    def __init__(self, layer_sizes: list):\n        self.layer_sizes = layer_sizes\n        self.layers = []\n        self.handles = []\n\n        for i, size in enumerate(layer_sizes):\n            layer = torch.zeros(size, dtype=torch.float32)\n            layer.fill_(float(i + 1))\n            self.layers.append(layer)\n            # Register ONCE at startup\n            byte_view = layer.view(torch.uint8).flatten()\n            self.handles.append(RDMABuffer(byte_view))\n\n    @endpoint\n    def get_handles(self) -\u003E list:\n        return [(size, handle) for size, handle in zip(self.layer_sizes, self.handles)]\n\nclass ScatteredReceiver(Actor):\n    \"\"\"Receives from scattered sender with RDMAAction batching.\"\"\"\n\n    def __init__(self, layer_sizes: list):\n        self.layer_sizes = layer_sizes\n        self.layers = [torch.zeros(size, dtype=torch.float32) for size in layer_sizes]\n        self.rank = current_rank().rank\n\n    @endpoint\n    def receive_step(self, sender: ScatteredSender) -\u003E dict:\n        start = time.perf_counter()\n        handles = sender.get_handles.call_one().get()\n\n        # Batch all transfers with RDMAAction\n        action = RDMAAction()\n        for i, (size, handle) in enumerate(handles):\n            byte_view = self.layers[i].view(torch.uint8).flatten()\n            action.read_into(handle, byte_view)\n        action.submit().get()\n\n        elapsed_ms = (time.perf_counter() - start) * 1000\n        return {\"elapsed_ms\": elapsed_ms}\n\nprint(\"ScatteredSender: Registers each layer once\")\nprint(\"ScatteredReceiver: Uses RDMAAction to batch transfers\")", "code_hash": "f34fae3c9b55f895f8314e3c1cb3f031", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ROlb", "name": "_"}, {"code": "mo.md(r\"\"\"\n**Key insight**: Register in `__init__`, not in your transfer endpoint!\n\nFor the scattered approach, RDMAAction is like a **transfer plan** - you build it once\nwith all the handles, then just call `submit()` whenever you need to sync.\n\nLet's benchmark to see the difference:\n\"\"\")", "code_hash": "c929cc5041f7654386ee971423e91ad3", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "qnkX", "name": "_"}, {"code": "def run_benchmark():\n    \"\"\"Compare the three approaches over multiple steps.\"\"\"\n    layer_sizes = [1000, 5000, 2000]  # 8000 floats total\n    total_size = sum(layer_sizes)\n    num_steps = 5\n\n    host = this_host()\n    sender_procs = host.spawn_procs({\"procs\": 1})\n    receiver_procs = host.spawn_procs({\"procs\": 1})\n\n    print(\"=== RDMA Registration Benchmark ===\")\n    print(f\"Transferring {total_size} floats ({total_size * 4 / 1024:.1f} KB) x {num_steps} steps\\n\")\n\n    results = {}\n\n    # Naive approach\n    naive_sender = sender_procs.spawn(\"naive_s\", NaiveSender, layer_sizes)\n    naive_receiver = receiver_procs.spawn(\"naive_r\", NaiveReceiver, layer_sizes)\n    times = []\n    for step in range(num_steps):\n        r = naive_receiver.receive_step.call_one(naive_sender).get()\n        times.append(r[\"elapsed_ms\"])\n    results[\"Naive\"] = times\n    print(f\"Naive (re-register each step):\")\n    for i, t in enumerate(times):\n        print(f\"  Step {i+1}: {t:.2f}ms\")\n    print(f\"  Average: {sum(times)/len(times):.2f}ms\\n\")\n\n    # Contiguous approach\n    cont_sender = sender_procs.spawn(\"cont_s\", ContiguousSender, layer_sizes)\n    cont_receiver = receiver_procs.spawn(\"cont_r\", ContiguousReceiver, total_size)\n    times = []\n    for step in range(num_steps):\n        r = cont_receiver.receive_step.call_one(cont_sender).get()\n        times.append(r[\"elapsed_ms\"])\n    results[\"Contiguous\"] = times\n    print(f\"Contiguous (register once):\")\n    for i, t in enumerate(times):\n        print(f\"  Step {i+1}: {t:.2f}ms\")\n    print(f\"  Average: {sum(times)/len(times):.2f}ms\\n\")\n\n    # Scattered + RDMAAction approach\n    scat_sender = sender_procs.spawn(\"scat_s\", ScatteredSender, layer_sizes)\n    scat_receiver = receiver_procs.spawn(\"scat_r\", ScatteredReceiver, layer_sizes)\n    times = []\n    for step in range(num_steps):\n        r = scat_receiver.receive_step.call_one(scat_sender).get()\n        times.append(r[\"elapsed_ms\"])\n    results[\"Scattered\"] = times\n    print(f\"Scattered + RDMAAction (register once, batch):\")\n    for i, t in enumerate(times):\n        print(f\"  Step {i+1}: {t:.2f}ms\")\n    print(f\"  Average: {sum(times)/len(times):.2f}ms\\n\")\n\n    print(\"=== Summary ===\")\n    print(\"Naive: Pays MR cost EVERY step (slow)\")\n    print(\"Smart: Pays MR cost once, subsequent steps are fast\")\n\n    return results\n\nbenchmark_results = run_benchmark()", "code_hash": "6c9d83ed28b68f212e2735ac50230c0b", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "TqIu", "name": "_"}, {"code": "mo.md(r\"\"\"\n### Two Weight Sync Patterns\n\nWith RDMABuffer as our building block, there are two main approaches:\n\n| Pattern | How it works | Trade-offs |\n|---------|--------------|------------|\n| **CPU Staging** | GPU \u2192 CPU buffer \u2192 RDMA \u2192 CPU \u2192 GPU | One MR, simple, but copies |\n| **Direct GPU** | GPU \u2192 RDMA \u2192 GPU (GPUDirect) | No copies, but one MR per param |\n\n**Pattern 1: CPU Staging (Contiguous Buffer)**\n\nPack all parameters into one contiguous CPU buffer, register once:\n\n```python\nclass Trainer(Actor):\n    def __init__(self):\n        # Calculate total size for all parameters\n        total_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n\n        # Allocate ONE contiguous buffer, register ONCE\n        self.staging_buffer = torch.empty(total_bytes, dtype=torch.uint8)\n        self.handle = RDMABuffer(self.staging_buffer)\n\n        # Track where each param lives in the buffer\n        self.param_offsets = compute_offsets(model)\n\n    def pack_weights(self):\n        '''Copy all params into contiguous buffer.'''\n        for name, param in model.named_parameters():\n            offset = self.param_offsets[name]\n            self.staging_buffer[offset:offset+size].copy_(param.view(torch.uint8))\n\n    @endpoint\n    def get_weight_handle(self) -\u003E RDMABuffer:\n        self.pack_weights()\n        return self.handle  # Same handle, new data\n```\n\n**Pattern 2: Direct GPU MRs**\n\nRegister each GPU parameter directly, no CPU copies:\n\n```python\nclass Trainer(Actor):\n    def __init__(self):\n        # Register each param ONCE at startup\n        self.handles = {}\n        for name, param in model.named_parameters():\n            byte_view = param.data.view(torch.uint8).flatten()\n            self.handles[name] = RDMABuffer(byte_view)\n\n    @endpoint\n    def get_param_handles(self) -\u003E dict[str, RDMABuffer]:\n        # Handles are reused - data updates in place\n        return self.handles\n```\n\nBoth patterns amortize MR registration cost across training iterations.\nLet's look at CPU staging in more detail (it's more common in async RL).\n\"\"\")", "code_hash": "b95b07f45da7a3e33ec1b3389ffa3e1d", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Vxnm", "name": "_"}, {"code": "mo.md(r\"\"\"\n## 5. CPU Staging Pattern\n\n### GPU-Native RDMA Works!\n\nFirst, let's be clear: **GPU-native RDMA works** and is fast:\n- GPUDirect RDMA: NIC reads directly from GPU memory\n- No CPU copy needed (when hardware supports it)\n- Great for synchronous transfers\n\n### Why CPU Staging for Async RL?\n\nThe issue isn't bandwidth - it's **timing**:\n\n```\nDirect GPU\u2192GPU RDMA:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Generator GPU is mid-inference                       \u2502\n\u2502 \u251c\u2500\u2500 layer 1 \u2500\u2500\u2524 [RDMA arrives, needs sync!]         \u2502\n\u2502               \u2193                                      \u2502\n\u2502         cudaDeviceSynchronize()  \u2190 Blocks inference! \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nWith CPU staging, nothing on the critical path blocks:\n\n```\nTrainer GPU \u2500\u2500\u25ba CPU staging buffer (RDMA registered)\n                      \u2502\n                      \u2502 [Sits here, ready anytime]\n                      \u2502\n                      \u25bc\nGenerator grabs when ready \u2500\u2500\u25ba Generator GPU\n```\n\nThe CPU buffer is a **temporal decoupling point**.\n\"\"\")", "code_hash": "a8e57a271297ac88d2492cc4df33416e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "DnEU", "name": "_"}, {"code": "def demonstrate_cpu_staging():\n    \"\"\"Demonstrate the CPU staging pattern.\"\"\"\n    if not torch.cuda.is_available():\n        print(\"CUDA not available - showing conceptual flow\")\n        return\n\n    # Trainer side: GPU weights \u2192 CPU staging buffer (RDMA registered)\n    trainer_weights = torch.randn(1000, 1000, device=\"cuda:0\")\n\n    # Pin memory for efficient transfers and RDMA registration\n    cpu_staging = torch.empty_like(trainer_weights, device=\"cpu\").pin_memory()\n\n    # D2H: Trainer dumps to CPU (async, non-blocking for trainer)\n    cpu_staging.copy_(trainer_weights, non_blocking=True)\n    torch.cuda.synchronize()  # Just for timing demo\n\n    print(\"Trainer: Weights copied to CPU staging buffer (RDMA registered)\")\n    print(f\"  GPU memory: {trainer_weights.device}\")\n    print(f\"  CPU staging: pinned={cpu_staging.is_pinned()}\")\n\n    # Generator side: RDMA pulls from trainer's CPU \u2192 directly to generator's GPU\n    # (In this demo we simulate the RDMA transfer with a local copy)\n    generator_gpu_weights = torch.empty_like(cpu_staging, device=\"cuda:0\")\n    generator_gpu_weights.copy_(cpu_staging, non_blocking=True)  # Simulates RDMA \u2192 GPU\n    torch.cuda.synchronize()\n\n    print(\"Generator: Weights loaded directly to GPU (via RDMA)\")\n    print(f\"  Weights match: {torch.allclose(trainer_weights, generator_gpu_weights)}\")\n\ndemonstrate_cpu_staging()", "code_hash": "dd06d4354b46dece7fc2c1062c403486", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ulZA", "name": "_"}, {"code": "mo.md(r\"\"\"\n## 6. Circular Weight Buffers\n\n### The Versioning Problem\n\nIn async RL, trainer updates weights continuously. Generators need to:\n1. **Grab the latest** weights (not stale ones)\n2. **Not block** waiting for updates\n3. **Avoid memory churn** (re-registering RDMA buffers is expensive)\n\n### Solution: Circular Buffer\n\n```\nTrainer writes:     v0 \u2192 v1 \u2192 v2 \u2192 v3 \u2192 v4 \u2192 v5 \u2192 ...\n                     \u2193    \u2193    \u2193\nBuffer slots:      [slot0][slot1][slot2]  (circular, reused)\n                     v3    v4    v5\n\nGenerator reads: \"Give me latest\" \u2192 v5\n```\n\nBenefits:\n- **Pre-registered RDMA buffers** - no memory registration on hot path\n- **Lock-free reads** - generators always get a consistent snapshot\n- **Bounded memory** - only N versions in flight\n\"\"\")", "code_hash": "1f92550528443774fa943b82fb79d765", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ecfG", "name": "_"}, {"code": "from threading import Lock as _Lock\nfrom typing import Tuple as _Tuple, Optional as _Opt\n\nclass CircularWeightBuffer:\n    \"\"\"Circular buffer for versioned weight storage.\"\"\"\n\n    def __init__(self, template_tensor: torch.Tensor, n_slots: int = 3):\n        self.n_slots = n_slots\n        self.slots = [\n            torch.empty_like(template_tensor).pin_memory()\n            if template_tensor.device.type == \"cpu\"\n            else torch.empty_like(template_tensor, device=\"cpu\").pin_memory()\n            for _ in range(n_slots)\n        ]\n        self.latest_version = 0\n        self._lock = _Lock()\n\n        # In production: pre-register all slots with RDMA\n        # self.rdma_handles = [RDMABuffer(slot) for slot in self.slots]\n\n    def publish(self, weights: torch.Tensor) -\u003E int:\n        \"\"\"Trainer publishes new weights. Returns version number.\"\"\"\n        with self._lock:\n            slot_idx = self.latest_version % self.n_slots\n            self.slots[slot_idx].copy_(weights)\n            self.latest_version += 1\n            return self.latest_version - 1\n\n    def get_latest(self) -\u003E _Tuple[torch.Tensor, int]:\n        \"\"\"Generator gets latest weights. Non-blocking.\"\"\"\n        with self._lock:\n            if self.latest_version == 0:\n                raise RuntimeError(\"No weights published yet\")\n            slot_idx = (self.latest_version - 1) % self.n_slots\n            version = self.latest_version - 1\n            # Return a copy to avoid races\n            return self.slots[slot_idx].clone(), version\n\n    def get_version(self, version: int) -\u003E _Opt[torch.Tensor]:\n        \"\"\"Get specific version if still available.\"\"\"\n        with self._lock:\n            oldest_available = max(0, self.latest_version - self.n_slots)\n            if version \u003C oldest_available or version \u003E= self.latest_version:\n                return None\n            slot_idx = version % self.n_slots\n            return self.slots[slot_idx].clone()\n\n# Demo\n_template = torch.randn(100, 100)\nweight_buffer = CircularWeightBuffer(_template, n_slots=3)\n\n# Trainer publishes versions\nfor _v in range(5):\n    _new_weights = torch.randn(100, 100) * (_v + 1)\n    published_v = weight_buffer.publish(_new_weights)\n    print(f\"Published version {published_v}\")\n\n# Generator grabs latest\nlatest_weights, latest_version = weight_buffer.get_latest()\nprint(f\"\\nGenerator got version {latest_version}, weights mean: {latest_weights.mean():.2f}\")\n\n# Try to get old version (might be evicted)\nold_weights = weight_buffer.get_version(1)\nprint(f\"Version 1 available: {old_weights is not None}\")", "code_hash": "f46a13dd70424ef26feb00afffdfb71a", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Pvdt", "name": "_"}, {"code": "mo.md(r\"\"\"\n## 7. Weight Re-sharding\n\n### The Sharding Mismatch Problem\n\nTrainer and Generator often have **different tensor layouts**:\n\n| Role | Parallelism | Sharding |\n|------|-------------|----------|\n| Trainer | FSDP (8 GPUs) | `Shard(0)` - rows split across 8 GPUs |\n| Generator | TP (2 GPUs) | `Shard(1)` - columns split across 2 GPUs |\n\nDirect weight transfer doesn't work - we need **re-sharding**.\n\n```\nTrainer (row-sharded):          Generator (column-sharded):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 GPU 0: rows 0-127\u2502            \u2502 GPU 0   \u2502 GPU 1   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524     \u2192      \u2502 cols    \u2502 cols    \u2502\n\u2502 GPU 1: rows 128+ \u2502            \u2502 0-511   \u2502 512+    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\"\"\")", "code_hash": "cf2298bef2f9d4413e1e9abdee5a7da6", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ZBYS", "name": "_"}, {"code": "mo.md(r\"\"\"\n### Approach 1: Gather Then Slice\n\nSimple but inefficient:\n\n```\n1. Each receiver gathers ALL sender shards \u2192 full tensor\n2. Each receiver slices out its portion\n```\n\n**Pros**: Simple, works with any sharding\n**Cons**: 2x redundant data transfer (each receiver gets everything)\n\n### Approach 2: Routed/Direct Transfer\n\nOptimal but complex:\n\n```\n1. Pre-compute which sender chunks overlap with which receiver regions\n2. Send only the exact chunks needed\n3. Receivers place chunks directly in correct positions\n```\n\n**Pros**: Minimal bandwidth (no redundancy)\n**Cons**: Complex overlap computation\n\"\"\")", "code_hash": "cf55cb42141c833974d8ecb1b6877238", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "aLJB", "name": "_"}, {"code": "from dataclasses import dataclass\nfrom typing import List, Tuple\n\n@dataclass\nclass ShardMetadata:\n    \"\"\"Metadata describing a tensor shard.\"\"\"\n    rank: int\n    global_shape: Tuple[int, ...]\n    offset: Tuple[int, ...]  # Start position in global tensor\n    local_shape: Tuple[int, ...]  # Shape of this shard\n\ndef compute_shard_metadata(\n    global_shape: Tuple[int, int],\n    num_ranks: int,\n    shard_dim: int,\n) -\u003E List[ShardMetadata]:\n    \"\"\"Compute shard metadata for a given sharding.\"\"\"\n    shards = []\n    dim_size = global_shape[shard_dim]\n    shard_size = dim_size // num_ranks\n\n    for rank in range(num_ranks):\n        offset = [0, 0]\n        local_shape = list(global_shape)\n\n        offset[shard_dim] = rank * shard_size\n        local_shape[shard_dim] = shard_size\n\n        shards.append(ShardMetadata(\n            rank=rank,\n            global_shape=global_shape,\n            offset=tuple(offset),\n            local_shape=tuple(local_shape),\n        ))\n\n    return shards\n\n# Demo: Trainer has row-sharding, Generator has column-sharding\n_global_shape = (1024, 1024)\n\ntrainer_shards = compute_shard_metadata(_global_shape, num_ranks=4, shard_dim=0)\ngenerator_shards = compute_shard_metadata(_global_shape, num_ranks=2, shard_dim=1)\n\nprint(\"Trainer shards (row-wise, 4 GPUs):\")\nfor s in trainer_shards:\n    print(f\"  Rank {s.rank}: offset={s.offset}, shape={s.local_shape}\")\n\nprint(\"\\nGenerator shards (column-wise, 2 GPUs):\")\nfor s in generator_shards:\n    print(f\"  Rank {s.rank}: offset={s.offset}, shape={s.local_shape}\")", "code_hash": "01a93cde7e5ea82b6a4aa8a7a8c4acf8", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "nHfw", "name": "_"}, {"code": "@dataclass\nclass TransferChunk:\n    \"\"\"A chunk to transfer from sender to receiver.\"\"\"\n    sender_rank: int\n    receiver_rank: int\n    sender_offset: Tuple[int, int]  # Where to read from sender\n    receiver_offset: Tuple[int, int]  # Where to write in receiver\n    shape: Tuple[int, int]  # Shape of the chunk\n\ndef compute_overlap(\n    sender: ShardMetadata,\n    receiver: ShardMetadata,\n) -\u003E TransferChunk | None:\n    \"\"\"Compute overlap between sender and receiver shards.\"\"\"\n    # Find intersection in global coordinates\n    s_start = sender.offset\n    s_end = (s_start[0] + sender.local_shape[0], s_start[1] + sender.local_shape[1])\n\n    r_start = receiver.offset\n    r_end = (r_start[0] + receiver.local_shape[0], r_start[1] + receiver.local_shape[1])\n\n    # Compute intersection\n    inter_start = (max(s_start[0], r_start[0]), max(s_start[1], r_start[1]))\n    inter_end = (min(s_end[0], r_end[0]), min(s_end[1], r_end[1]))\n\n    # Check if there's actual overlap\n    if inter_start[0] \u003E= inter_end[0] or inter_start[1] \u003E= inter_end[1]:\n        return None\n\n    shape = (inter_end[0] - inter_start[0], inter_end[1] - inter_start[1])\n\n    # Convert to local coordinates\n    sender_local = (inter_start[0] - s_start[0], inter_start[1] - s_start[1])\n    receiver_local = (inter_start[0] - r_start[0], inter_start[1] - r_start[1])\n\n    return TransferChunk(\n        sender_rank=sender.rank,\n        receiver_rank=receiver.rank,\n        sender_offset=sender_local,\n        receiver_offset=receiver_local,\n        shape=shape,\n    )\n\ndef compute_transfer_plan(\n    sender_shards: List[ShardMetadata],\n    receiver_shards: List[ShardMetadata],\n) -\u003E List[TransferChunk]:\n    \"\"\"Compute all transfers needed for re-sharding.\"\"\"\n    transfers = []\n    for sender in sender_shards:\n        for receiver in receiver_shards:\n            chunk = compute_overlap(sender, receiver)\n            if chunk is not None:\n                transfers.append(chunk)\n    return transfers\n\n# Compute transfer plan\ntransfer_plan = compute_transfer_plan(trainer_shards, generator_shards)\n\nprint(f\"Transfer plan: {len(transfer_plan)} chunks needed\\n\")\nfor chunk in transfer_plan:\n    print(f\"Sender {chunk.sender_rank} \u2192 Receiver {chunk.receiver_rank}\")\n    print(f\"  Read from sender offset {chunk.sender_offset}, shape {chunk.shape}\")\n    print(f\"  Write to receiver offset {chunk.receiver_offset}\")\n    print()", "code_hash": "dd9ef142fcda82825b003d755d4b8bc5", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "xXTn", "name": "_"}, {"code": "mo.md(r\"\"\"\n### The DTensor Dream (and Reality)\n\nIf trainer and generator both used **DTensor** with the same sharding spec, re-sharding\nwould be automatic - the framework handles overlap computation and transfers.\n\n```python\n# Ideal world: DTensor handles it\ntrainer_weights: DTensor  # Shard(0) across 8 GPUs\ngenerator_weights: DTensor  # Shard(1) across 2 GPUs\n\n# Re-sharding is just redistribution\ngenerator_weights = trainer_weights.redistribute(generator_placement)\n```\n\n**In practice, it's harder**:\n- vLLM does its own sharding and weight fusing (not DTensor-compatible)\n- Training frameworks (FSDP, etc.) have different abstractions\n- You often need custom overlap computation like we showed above\n\nThe routed approach (compute overlaps, send only needed chunks) is 2x faster than\nnaive gather-then-slice, but requires this manual coordination.\n\n**For cross-node RDMA transfers**, the key insight remains: pre-compute the transfer\nplan once, then each sync just executes the planned RDMA operations with RDMAAction.\n\"\"\")", "code_hash": "451e5f375cd885f5faa4be78edff6345", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "AjVT", "name": "_"}, {"code": "mo.md(r\"\"\"\n## 8. Putting It All Together\n\nThe full async RL weight sync pattern:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         TRAINER                                  \u2502\n\u2502  1. Train step completes                                        \u2502\n\u2502  2. Copy weights to CPU staging buffer (non-blocking D2H)       \u2502\n\u2502  3. Publish to circular buffer with version tag                 \u2502\n\u2502  4. Continue training (no blocking!)                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              CIRCULAR BUFFER (CPU, RDMA-registered)             \u2502\n\u2502  [slot 0: v3] [slot 1: v4] [slot 2: v5]                        \u2502\n\u2502                                 \u2191 latest                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u25bc                     \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   GENERATOR 0   \u2502   \u2502   GENERATOR 1   \u2502   \u2502   GENERATOR 2   \u2502\n\u2502                 \u2502   \u2502                 \u2502   \u2502                 \u2502\n\u2502 After gen done: \u2502   \u2502 After gen done: \u2502   \u2502 After gen done: \u2502\n\u2502 1. Get latest   \u2502   \u2502 1. Get latest   \u2502   \u2502 1. Get latest   \u2502\n\u2502    version      \u2502   \u2502    version      \u2502   \u2502    version      \u2502\n\u2502 2. RDMA read    \u2502   \u2502 2. RDMA read    \u2502   \u2502 2. RDMA read    \u2502\n\u2502    \u2192 GPU        \u2502   \u2502    \u2192 GPU        \u2502   \u2502    \u2192 GPU        \u2502\n\u2502 3. Re-shard if  \u2502   \u2502 3. Re-shard if  \u2502   \u2502 3. Re-shard if  \u2502\n\u2502    needed       \u2502   \u2502    needed       \u2502   \u2502    needed       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n**Key properties:**\n- Trainer never blocks waiting for generators\n- Generators pull directly to GPU when *they're* ready\n- Re-sharding happens locally on each generator\n- Circular buffer bounds memory, reuses RDMA registrations\n\"\"\")", "code_hash": "806222a0a76efaa0bba91baefc514224", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "pHFh", "name": "_"}, {"code": "mo.md(r\"\"\"\n### Code Pattern\n\n```python\n# Trainer side\nclass Trainer(Actor):\n    def __init__(self):\n        self.weight_buffer = CircularWeightBuffer(\n            template=self.model.state_dict_tensor(),\n            n_slots=3,\n        )\n\n    @endpoint\n    def get_weight_handle(self) -\u003E Tuple[RDMABuffer, int]:\n        '''Return handle to latest weights and version.'''\n        slot_idx = (self.weight_buffer.latest_version - 1) % 3\n        handle = self.weight_buffer.rdma_handles[slot_idx]\n        version = self.weight_buffer.latest_version - 1\n        return handle, version\n\n    async def train_loop(self):\n        while True:\n            loss = self.train_step()\n            if self.step % sync_interval == 0:\n                # Non-blocking publish\n                self.weight_buffer.publish(self.model.weights)\n\n# Generator side\nclass Generator(Actor):\n    def __init__(self, trainer_ref):\n        self.trainer = trainer_ref\n        self.current_version = -1\n\n    async def maybe_sync_weights(self):\n        handle, version = await self.trainer.get_weight_handle.call_one().get()\n        if version \u003E self.current_version:\n            # Pull via RDMA directly into GPU memory\n            gpu_weights = self.model.weights.view(torch.uint8).flatten()\n            await handle.read_into(gpu_weights)\n            self.current_version = version\n\n    async def generate_loop(self):\n        while True:\n            await self.maybe_sync_weights()\n            output = self.generate(prompt)\n            self.submit_to_buffer(output)\n```\n\"\"\")", "code_hash": "26dba11147530b7414a96869242e3902", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "NCOB", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Summary\n\n### Key Takeaways\n\n1. **Bandwidth hierarchy matters**: NVLink (450 GB/s) \u003E\u003E InfiniBand (50-100 GB/s) \u003E\u003E PCIe\n   - Know your hardware, optimize for the right interconnect\n\n2. **Collectives vs P2P**: Collectives are synchronized; RL needs async P2P\n   - High variance in generation times makes blocking expensive\n\n3. **Magic pointer pattern**: Tiny handle over control plane, bulk data over data plane\n   - ~100 bytes to describe 10 GB transfer\n\n4. **CPU staging**: Temporal decoupling for async RL\n   - GPU-native RDMA works for sync cases\n   - CPU staging ensures nothing blocks on the critical path\n\n5. **Circular buffers**: Version weights without memory churn\n   - Pre-register RDMA buffers, reuse slots\n   - Generators grab latest, never stale\n\n6. **Weight re-sharding**: Different layouts need overlap computation\n   - Routed approach is 2x faster than gather\n   - Pre-compute transfer plan, minimize redundant data\n\n### Next Steps\n\nSee **07_async_rl_e2e.py** for a complete async RL system that uses these patterns.\n\"\"\")", "code_hash": "a3abb4c511005ddfec0279d1c84e6846", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "aqbW", "name": "_"}], "metadata": {"marimo_version": "0.19.7"}, "version": "1"},
            "session": {"cells": [{"code_hash": "1d0db38904205bec4d6f6f6a1f6cec3e", "console": [], "id": "Hbol", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "5d6ef71d15248a566ba53502517e14e6", "console": [], "id": "MJUe", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch1 id=\"rdma-weight-synchronization\"\u003ERDMA \u0026amp; Weight Synchronization\u003C/h1\u003E\n\u003Cspan class=\"paragraph\"\u003EThis notebook explores efficient weight synchronization for async RL systems:\u003C/span\u003E\n\u003Col\u003E\n\u003Cli\u003E\u003Cstrong\u003EThe Bandwidth Hierarchy\u003C/strong\u003E - NVLink, InfiniBand, PCIe\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EThe Problem: Collectives Are Blocking\u003C/strong\u003E - Why RL needs something different\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EHow RDMA Works\u003C/strong\u003E - ibverbs, one-sided operations\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EThe Magic Pointer Pattern\u003C/strong\u003E - Control plane vs data plane separation\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003ECPU Staging\u003C/strong\u003E - Decoupling trainer and generator timing\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003ECircular Weight Buffers\u003C/strong\u003E - Versioning without memory churn\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EWeight Re-sharding\u003C/strong\u003E - Handling different tensor layouts\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EPutting It All Together\u003C/strong\u003E - The complete pattern\u003C/li\u003E\n\u003C/ol\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "899037a1e41177991a152496300fd26d", "console": [], "id": "vblA", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"1-the-bandwidth-hierarchy\"\u003E1. The Bandwidth Hierarchy\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EModern HPC clusters have multiple interconnects with vastly different bandwidths:\u003C/span\u003E\n\u003Cdiv class=\"language-text codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                              NODE A                                                      \u2502\n\u2502                                                                                                          \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502    \u2502                              NVSwitch / NVLink Fabric                                         \u2502     \u2502\n\u2502    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502     \u2502\n\u2502    \u2502  \u2502GPU 0 \u2502 \u2502GPU 1 \u2502 \u2502GPU 2 \u2502 \u2502GPU 3 \u2502 \u2502GPU 4 \u2502 \u2502GPU 5 \u2502 \u2502GPU 6 \u2502 \u2502GPU 7 \u2502                      \u2502     \u2502\n\u2502    \u2502  \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518                      \u2502     \u2502\n\u2502    \u2502     ########################################################################  900 GB/s NVLink \u2502     \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                              \u2502                                                           \u2502\n\u2502                                         ======  64 GB/s PCIe                                             \u2502\n\u2502                                              \u2502                                                           \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  ------ 48 GB/s ------ \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502    \u2502  CPU 0  \u2502     CPU interconnect   \u2502  CPU 1  \u2502 ====== 64 GB/s \u2550\u2550\u2502 NIC 0 \u2502          \u2502 NIC 1 \u2502          \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518      PCIe        \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518          \u2502\n\u2502         \u2502                                  \u2502                           \u2502                  \u2502              \u2502\n\u2502         \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 64 GB/s PCIe \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a              \u2502\n\u2502                                                                        \u2502                  \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                         \u2502                  \u2502\n                                                                       ======  50 GB/s   ======\n                                                                    IB NDR400         IB NDR400\n                                                                         \u2502                  \u2502\n                                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                        \u2502                                                    \u2502\n                                                        \u2502              InfiniBand Switch                     \u2502\n                                                        \u2502                                                    \u2502\n                                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                         \u2502                  \u2502\n                                                                       ======  50 GB/s   ======\n                                                                    IB NDR400         IB NDR400\n                                                                         \u2502                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                                        \u2502                  \u2502              \u2502\n\u2502         \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 64 GB/s PCIe \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a              \u2502\n\u2502         \u2502                                  \u2502                           \u2502                  \u2502              \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510                        \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510      PCIe        \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510          \u2502\n\u2502    \u2502  CPU 0  \u2502     CPU interconnect   \u2502  CPU 1  \u2502 ====== 64 GB/s \u2550\u2550\u2502 NIC 0 \u2502          \u2502 NIC 1 \u2502          \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ------ 48 GB/s ------  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502                                              \u2502                                                           \u2502\n\u2502                                           ======  64 GB/s PCIe                                           \u2502\n\u2502                                              \u2502                                                           \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502    \u2502     ########################################################################  900 GB/s NVLink \u2502     \u2502\n\u2502    \u2502  \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510                      \u2502     \u2502\n\u2502    \u2502  \u2502GPU 0 \u2502 \u2502GPU 1 \u2502 \u2502GPU 2 \u2502 \u2502GPU 3 \u2502 \u2502GPU 4 \u2502 \u2502GPU 5 \u2502 \u2502GPU 6 \u2502 \u2502GPU 7 \u2502                      \u2502     \u2502\n\u2502    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502     \u2502\n\u2502    \u2502                              NVSwitch / NVLink Fabric                                         \u2502     \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                              NODE B                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nBandwidth encoding (line intensity):\n  ########  NVLink/NVSwitch   900 GB/s   (GPU \u2194 GPU, same node)\n  ========  PCIe Gen5 / IB     50-64 GB/s (CPU\u2194GPU, CPU\u2194NIC, cross-node)\n  --------  CPU interconnect   48 GB/s   (CPU \u2194 CPU, same node)\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003ERDMA can transfer between any registered memory (CPU or GPU) via the NICs.\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EInterconnect\u003C/th\u003E\n\u003Cth\u003EBandwidth\u003C/th\u003E\n\u003Cth\u003ELatency\u003C/th\u003E\n\u003Cth\u003EUse Case\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Cstrong\u003ENVLink/NVSwitch\u003C/strong\u003E\u003C/td\u003E\n\u003Ctd\u003E900 GB/s\u003C/td\u003E\n\u003Ctd\u003E~1 \u03bcs\u003C/td\u003E\n\u003Ctd\u003ESame-node GPU\u2194GPU\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Cstrong\u003EInfiniBand NDR400\u003C/strong\u003E\u003C/td\u003E\n\u003Ctd\u003E50 GB/s\u003C/td\u003E\n\u003Ctd\u003E~1-2 \u03bcs\u003C/td\u003E\n\u003Ctd\u003ECross-node RDMA\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Cstrong\u003EPCIe Gen5 x16\u003C/strong\u003E\u003C/td\u003E\n\u003Ctd\u003E64 GB/s\u003C/td\u003E\n\u003Ctd\u003E~1-2 \u03bcs\u003C/td\u003E\n\u003Ctd\u003ECPU\u2194GPU, CPU\u2194NIC\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Cstrong\u003ECPU interconnect\u003C/strong\u003E\u003C/td\u003E\n\u003Ctd\u003E48 GB/s\u003C/td\u003E\n\u003Ctd\u003E~100 ns\u003C/td\u003E\n\u003Ctd\u003ECPU\u2194CPU (same node)\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EKey observations:\u003C/strong\u003E\u003C/span\u003E\n\u003Col\u003E\n\u003Cli\u003E\u003Cstrong\u003ENVLink dominates\u003C/strong\u003E - 900 GB/s is ~18x faster than cross-node RDMA. Same-node GPU\u2194GPU\n   communication is nearly free compared to crossing the network.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003ERDMA \u0026gt;\u0026gt; Ethernet\u003C/strong\u003E - InfiniBand/RoCE at 50 GB/s is ~4x faster than 100GbE (12.5 GB/s),\n   plus kernel bypass and lower latency. Worth the complexity for HPC workloads.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EPCIe is faster than you'd think\u003C/strong\u003E - At 64 GB/s, CPU\u2194GPU transfers aren't the bottleneck\n   people often assume. The real cost is synchronization, not bandwidth.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ERule of thumb\u003C/strong\u003E: Place the most bandwidth-intensive, frequent operations on NVLink\n(gradients, activations). Use RDMA for cross-node communication (weight sync, sharding).\nPCIe is fine for occasional CPU\u2194GPU transfers.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe'll focus primarily on \u003Cstrong\u003ENVLink\u003C/strong\u003E and \u003Cstrong\u003ERDMA\u003C/strong\u003E for this notebook. Most people use these\nvia \u003Cstrong\u003Ecollectives\u003C/strong\u003E, exposed through PyTorch distributed:\u003C/span\u003E\n\u003Cdiv class=\"language-python codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"kn\"\u003Eimport\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nn\"\u003Etorch.distributed\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"k\"\u003Eas\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nn\"\u003Edist\u003C/span\u003E\n\n\u003Cspan class=\"c1\"\u003E# Initialize process group - NCCL uses NVLink (same-node) and RDMA (cross-node)\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Edist\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Einit_process_group\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Ebackend\u003C/span\u003E\u003Cspan class=\"o\"\u003E=\u003C/span\u003E\u003Cspan class=\"s2\"\u003E\u0026quot;nccl\u0026quot;\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\n\u003Cspan class=\"c1\"\u003E# All-reduce: average gradients across all GPUs\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Edist\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eall_reduce\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Egradients\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Eop\u003C/span\u003E\u003Cspan class=\"o\"\u003E=\u003C/span\u003E\u003Cspan class=\"n\"\u003Edist\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003EReduceOp\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003ESUM\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Egradients\u003C/span\u003E \u003Cspan class=\"o\"\u003E/=\u003C/span\u003E \u003Cspan class=\"n\"\u003Eworld_size\u003C/span\u003E\n\n\u003Cspan class=\"c1\"\u003E# All-gather: collect tensors from all ranks\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Egathered\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Etorch\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eempty_like\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Etensor\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E \u003Cspan class=\"k\"\u003Efor\u003C/span\u003E \u003Cspan class=\"n\"\u003E_\u003C/span\u003E \u003Cspan class=\"ow\"\u003Ein\u003C/span\u003E \u003Cspan class=\"nb\"\u003Erange\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Eworld_size\u003C/span\u003E\u003Cspan class=\"p\"\u003E)]\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Edist\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eall_gather\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Egathered\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Etensor\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\n\u003Cspan class=\"c1\"\u003E# Broadcast: send from rank 0 to all others\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Edist\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ebroadcast\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Eweights\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Esrc\u003C/span\u003E\u003Cspan class=\"o\"\u003E=\u003C/span\u003E\u003Cspan class=\"mi\"\u003E0\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EThis works great for training. But for RL weight sync, we need something different...\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "e2ddceb8064dfb2235ad898d34eee19a", "console": [], "id": "bkHC", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "80fe0a96a7dd614040bf54159f19bd79", "console": [], "id": "lEQa", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"2-the-problem-collectives-are-blocking\"\u003E2. The Problem: Collectives Are Blocking\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003ECollectives work great for training - everyone computes gradients, then synchronizes.\nBut async RL has a different access pattern.\u003C/span\u003E\n\u003Ch3 id=\"high-variance-in-generation-times\"\u003EHigh Variance in Generation Times\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EGenerators have wildly different completion times:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003ESome prompts \u2192 10 tokens (fast)\u003C/li\u003E\n\u003Cli\u003EOther prompts \u2192 1000 tokens (slow)\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cspan class=\"paragraph\"\u003EWith collectives, fast generators wait for slow ones:\u003C/span\u003E\n\u003Cdiv class=\"language-scdoc codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003EGenerator 0: \u251c\u2500\u2500 gen (fast) \u2500\u2500\u2524  \u26a0\ufe0f WAITING...\nGenerator 1: \u251c\u2500\u2500\u2500\u2500\u2500\u2500 gen (slow) \u2500\u2500\u2500\u2500\u2500\u2500\u2524\nGenerator 2: \u251c\u2500\u2500 gen (fast) \u2500\u2500\u2524  \u26a0\ufe0f WAITING...\n                                      \u2193\n                          all_gather(weights)  # Everyone waits!\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Ch3 id=\"what-about-sendrecv\"\u003EWhat About send/recv?\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EPyTorch distributed does have point-to-point primitives:\u003C/span\u003E\n\u003Cdiv class=\"language-python codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"c1\"\u003E# Sender side\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Edist\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Esend\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Etensor\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Edst\u003C/span\u003E\u003Cspan class=\"o\"\u003E=\u003C/span\u003E\u003Cspan class=\"n\"\u003Ereceiver_rank\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\n\u003Cspan class=\"c1\"\u003E# Receiver side\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Edist\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Erecv\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Etensor\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Esrc\u003C/span\u003E\u003Cspan class=\"o\"\u003E=\u003C/span\u003E\u003Cspan class=\"n\"\u003Esender_rank\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EBut this is \u003Cstrong\u003Etwo-sided\u003C/strong\u003E - both sender and receiver must coordinate:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003EReceiver must call \u003Ccode\u003Erecv()\u003C/code\u003E before sender's \u003Ccode\u003Esend()\u003C/code\u003E completes\u003C/li\u003E\n\u003Cli\u003ETrainer would need to wait until generators are ready to receive\u003C/li\u003E\n\u003Cli\u003EStill blocking on coordination!\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch3 id=\"the-one-sided-solution-rdma\"\u003EThe One-Sided Solution: RDMA\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EWhat if the sender could write directly to the receiver's memory without coordination?\u003C/span\u003E\n\u003Cdiv class=\"language-teratermmacro codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"nv\"\u003ETwo\u003C/span\u003E\u003Cspan class=\"o\"\u003E-\u003C/span\u003E\u003Cspan class=\"nv\"\u003Esided\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"ss\"\u003E(\u003C/span\u003E\u003Cspan class=\"k\"\u003Esend\u003C/span\u003E\u003Cspan class=\"o\"\u003E/\u003C/span\u003E\u003Cspan class=\"nv\"\u003Erecv\u003C/span\u003E\u003Cspan class=\"ss\"\u003E)\u003C/span\u003E:\n\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"nv\"\u003ESender\u003C/span\u003E:\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"s2\"\u003E\u0026quot;I have data\u0026quot;\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"nv\"\u003EReceiver\u003C/span\u003E:\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"s2\"\u003E\u0026quot;I\u0026#39;m ready\u0026quot;\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"nv\"\u003ESender\u003C/span\u003E:\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nv\"\u003Esends\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nv\"\u003Edata\u003C/span\u003E\u003Cspan class=\"w\"\u003E     \u003C/span\u003E\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"nv\"\u003EReceiver\u003C/span\u003E:\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nv\"\u003Ereceives\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nv\"\u003Edata\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E                         \u003C/span\u003E\u003Cspan class=\"mi\"\u003E2\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nv\"\u003Emessages\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nv\"\u003Erequired\u003C/span\u003E\n\n\u003Cspan class=\"nv\"\u003EOne\u003C/span\u003E\u003Cspan class=\"o\"\u003E-\u003C/span\u003E\u003Cspan class=\"nv\"\u003Esided\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"ss\"\u003E(\u003C/span\u003E\u003Cspan class=\"nv\"\u003ERDMA\u003C/span\u003E\u003Cspan class=\"ss\"\u003E)\u003C/span\u003E:\n\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"nv\"\u003ESender\u003C/span\u003E:\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nv\"\u003Ewrites\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nv\"\u003Edirectly\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nv\"\u003Eto\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nv\"\u003Ereceiver\u003C/span\u003E\u003Cspan class=\"err\"\u003E\u0026#39;s memory\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E                         No coordination needed!\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EThis is what RDMA enables: \u003Cstrong\u003Eone-sided memory operations\u003C/strong\u003E.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "31ba79419d9ad47dff056f46fc102fbb", "console": [], "id": "PKri", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"3-how-rdma-works-ibverbs\"\u003E3. How RDMA Works (ibverbs)\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003ERDMA (Remote Direct Memory Access) lets one machine read/write another machine's memory\ndirectly, bypassing the kernel and CPU on both sides.\u003C/span\u003E\n\u003Ch3 id=\"the-ibverbs-stack\"\u003EThe ibverbs Stack\u003C/h3\u003E\n\u003Cdiv class=\"language-text codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Application (PyTorch, Monarch, etc.)                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  libibverbs  (userspace RDMA API)                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Provider driver (mlx5, efa, rxe, etc.)                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Hardware (InfiniBand NIC, RoCE NIC, etc.)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EWe'll focus on \u003Cstrong\u003EInfiniBand\u003C/strong\u003E and \u003Cstrong\u003ERoCE\u003C/strong\u003E (RDMA over Converged Ethernet).\nOther transports like AWS EFA exist but we won't cover them here.\u003C/span\u003E\n\u003Ch3 id=\"key-rdma-operations\"\u003EKey RDMA Operations\u003C/h3\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EOperation\u003C/th\u003E\n\u003Cth\u003EDescription\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003ERDMA_WRITE\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003EWrite to remote memory (one-sided)\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003ERDMA_READ\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003ERead from remote memory (one-sided)\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003ESEND/RECV\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003ETwo-sided messaging (like TCP)\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003EThe magic is in \u003Ccode\u003ERDMA_WRITE\u003C/code\u003E and \u003Ccode\u003ERDMA_READ\u003C/code\u003E - they're \u003Cstrong\u003Eone-sided\u003C/strong\u003E:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003ERemote CPU is not involved\u003C/li\u003E\n\u003Cli\u003ERemote application doesn't need to call anything\u003C/li\u003E\n\u003Cli\u003ENIC handles everything in hardware\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch3 id=\"memory-registration\"\u003EMemory Registration\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EBefore RDMA, memory must be \u003Cstrong\u003Eregistered\u003C/strong\u003E with the NIC:\u003C/span\u003E\n\u003Cdiv class=\"language-python codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"c1\"\u003E# Conceptually (actual ibverbs API is in C)\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Emr\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Erdma_register_memory\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Ebuffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Esize\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\u003Cspan class=\"c1\"\u003E# Returns:\u003C/span\u003E\n\u003Cspan class=\"c1\"\u003E#   - lkey: local access key (for local operations)\u003C/span\u003E\n\u003Cspan class=\"c1\"\u003E#   - rkey: remote access key (share with remote peer)\u003C/span\u003E\n\u003Cspan class=\"c1\"\u003E#   - addr: physical/virtual address\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EThe \u003Ccode\u003E(addr, rkey)\u003C/code\u003E pair is a \u003Cstrong\u003Eremote-accessible pointer\u003C/strong\u003E. Share it with a peer,\nand they can read/write your memory directly.\u003C/span\u003E\n\u003Ch3 id=\"queue-pair-setup\"\u003EQueue Pair Setup\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EBefore any RDMA operations, you need to establish a \u003Cstrong\u003EQueue Pair (QP)\u003C/strong\u003E between\nsender and receiver. This is a one-time connection setup:\u003C/span\u003E\n\u003Cdiv class=\"language-scdoc codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Sender    \u2502                           \u2502  Receiver   \u2502\n\u2502             \u2502                           \u2502             \u2502\n\u2502  Create QP  \u2502 \u2500\u2500\u2500 exchange QP info \u2500\u2500\u2500\u25ba \u2502  Create QP  \u2502\n\u2502  (qp_num,   \u2502 \u25c4\u2500\u2500 (qp_num, lid, gid) \u2500\u2500 \u2502             \u2502\n\u2502   lid, gid) \u2502                           \u2502             \u2502\n\u2502             \u2502                           \u2502             \u2502\n\u2502  Move QP to \u2502                           \u2502  Move QP to \u2502\n\u2502  RTR \u2192 RTS  \u2502                           \u2502  RTR \u2192 RTS  \u2502\n\u2502             \u2502                           \u2502             \u2502\n\u2502  Now ready  \u2502 \u2550\u2550\u2550 RDMA operations \u2550\u2550\u2550\u2550\u25ba \u2502  Now ready  \u2502\n\u2502  for RDMA!  \u2502                           \u2502  for RDMA!  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EThis is where \u003Cstrong\u003EMonarch actors\u003C/strong\u003E shine. Because you can spawn arbitrary actors,\nyou can create \u003Cstrong\u003ERDMA Manager actors\u003C/strong\u003E that:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003EInitialize QPs on their respective hosts\u003C/li\u003E\n\u003Cli\u003EExchange QP info via actor messages\u003C/li\u003E\n\u003Cli\u003EManage the connection lifecycle\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cdiv class=\"language-python codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"c1\"\u003E# Monarch pattern: RDMA managers as actors\u003C/span\u003E\n\u003Cspan class=\"k\"\u003Eclass\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nc\"\u003ERDMAManager\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003EActor\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"fm\"\u003E__init__\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n        \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eqp\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Ecreate_queue_pair\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\n        \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eqp_info\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Eget_qp_info\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eqp\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E  \u003Cspan class=\"c1\"\u003E# (qp_num, lid, gid)\u003C/span\u003E\n\n    \u003Cspan class=\"nd\"\u003E@endpoint\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003Eget_qp_info\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E \u003Cspan class=\"o\"\u003E-\u0026gt;\u003C/span\u003E \u003Cspan class=\"n\"\u003EQpInfo\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E\n        \u003Cspan class=\"k\"\u003Ereturn\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eqp_info\u003C/span\u003E\n\n    \u003Cspan class=\"nd\"\u003E@endpoint\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003Econnect\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Eremote_qp_info\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E \u003Cspan class=\"n\"\u003EQpInfo\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n        \u003Cspan class=\"c1\"\u003E# Transition QP: INIT \u2192 RTR \u2192 RTS\u003C/span\u003E\n        \u003Cspan class=\"n\"\u003Econnect_qp\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eqp\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Eremote_qp_info\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\n\u003Cspan class=\"c1\"\u003E# Setup: exchange QP info via actor messages, then RDMA is ready\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Etrainer_info\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Etrainer_rdma\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eget_qp_info\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecall_one\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eget\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Egenerator_rdma\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Econnect\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecall_one\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Etrainer_info\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eget\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EThe actor abstraction makes RDMA connection management natural and composable.\u003C/span\u003E\n\u003Ch3 id=\"monarch-using-monarch-rdmacontroller\"\u003EMonarch Using Monarch: RdmaController\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EHere's the cool part: \u003Cstrong\u003EMonarch uses itself\u003C/strong\u003E to manage RDMA infrastructure. Looking at\nthe actual Python code in \u003Ccode\u003Emonarch/_src/rdma/rdma.py\u003C/code\u003E:\u003C/span\u003E\n\u003Cdiv class=\"language-python codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"c1\"\u003E# From Monarch\u0026#39;s RDMA implementation\u003C/span\u003E\n\u003Cspan class=\"kn\"\u003Efrom\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nn\"\u003Emonarch._src.actor.proc_mesh\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"kn\"\u003Eimport\u003C/span\u003E \u003Cspan class=\"n\"\u003Eget_or_spawn_controller\u003C/span\u003E\n\n\u003Cspan class=\"k\"\u003Eclass\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nc\"\u003ERdmaController\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003EActor\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"sd\"\u003E\u0026#39;\u0026#39;\u0026#39;Singleton controller that coordinates RDMA initialization.\u0026#39;\u0026#39;\u0026#39;\u003C/span\u003E\n\n    \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"fm\"\u003E__init__\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n        \u003Cspan class=\"c1\"\u003E# Track which proc meshes have RDMA initialized\u003C/span\u003E\n        \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003E_manager_futures\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E \u003Cspan class=\"nb\"\u003Edict\u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003EProcMesh\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003EFuture\u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003ERdmaManager\u003C/span\u003E\u003Cspan class=\"p\"\u003E]]\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"p\"\u003E{}\u003C/span\u003E\n\n    \u003Cspan class=\"nd\"\u003E@endpoint\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Easync\u003C/span\u003E \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003Einit_rdma_on_mesh\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Eproc_mesh\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E \u003Cspan class=\"n\"\u003EProcMesh\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E \u003Cspan class=\"o\"\u003E-\u0026gt;\u003C/span\u003E \u003Cspan class=\"kc\"\u003ENone\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E        \u003C/span\u003E\u003Cspan class=\"sd\"\u003E\u0026#39;\u0026#39;\u0026#39;Lazily initialize RDMA on a proc mesh.\u0026#39;\u0026#39;\u0026#39;\u003C/span\u003E\n        \u003Cspan class=\"k\"\u003Eif\u003C/span\u003E \u003Cspan class=\"n\"\u003Eproc_mesh\u003C/span\u003E \u003Cspan class=\"ow\"\u003Enot\u003C/span\u003E \u003Cspan class=\"ow\"\u003Ein\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003E_manager_futures\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E\n            \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003E_manager_futures\u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Eproc_mesh\u003C/span\u003E\u003Cspan class=\"p\"\u003E]\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003EFuture\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\n                \u003Cspan class=\"n\"\u003Ecoro\u003C/span\u003E\u003Cspan class=\"o\"\u003E=\u003C/span\u003E\u003Cspan class=\"n\"\u003ERdmaManager\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecreate\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Eproc_mesh\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n            \u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n        \u003Cspan class=\"k\"\u003Eawait\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003E_manager_futures\u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Eproc_mesh\u003C/span\u003E\u003Cspan class=\"p\"\u003E]\u003C/span\u003E\n\n\u003Cspan class=\"c1\"\u003E# Cached initialization - only runs once per process\u003C/span\u003E\n\u003Cspan class=\"nd\"\u003E@functools\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecache\u003C/span\u003E\n\u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003E_ensure_init_rdma_manager\u003C/span\u003E\u003Cspan class=\"p\"\u003E():\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Easync\u003C/span\u003E \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003Etask\u003C/span\u003E\u003Cspan class=\"p\"\u003E():\u003C/span\u003E\n        \u003Cspan class=\"n\"\u003Econtroller\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"k\"\u003Eawait\u003C/span\u003E \u003Cspan class=\"n\"\u003Eget_or_spawn_controller\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"s2\"\u003E\u0026quot;rdma_controller\u0026quot;\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003ERdmaController\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n        \u003Cspan class=\"k\"\u003Eawait\u003C/span\u003E \u003Cspan class=\"n\"\u003Econtroller\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Einit_rdma_on_mesh\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecall_one\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecurrent_proc_mesh\u003C/span\u003E\u003Cspan class=\"p\"\u003E())\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Ereturn\u003C/span\u003E \u003Cspan class=\"n\"\u003Espawn_task\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Etask\u003C/span\u003E\u003Cspan class=\"p\"\u003E())\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EThis is \u003Cstrong\u003EMonarch building Monarch\u003C/strong\u003E - the RDMA subsystem uses the same patterns:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ccode\u003Eget_or_spawn_controller(\"rdma_controller\", RdmaController)\u003C/code\u003E ensures one global controller\u003C/li\u003E\n\u003Cli\u003EThe controller lazily initializes RDMA managers per proc mesh\u003C/li\u003E\n\u003Cli\u003E\u003Ccode\u003E@functools.cache\u003C/code\u003E ensures we only bootstrap once per process\u003C/li\u003E\n\u003Cli\u003EUnder the hood, the actual RDMA operations are in Rust (\u003Ccode\u003ERdmaManagerActor\u003C/code\u003E)\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cspan class=\"paragraph\"\u003EIt's actors all the way down.\u003C/span\u003E\n\u003Ch3 id=\"why-this-matters-for-weight-sync\"\u003EWhy This Matters for Weight Sync\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003ERemember: CPU memory AND GPU memory can both be registered for RDMA.\u003C/span\u003E\n\u003Cdiv class=\"language-actionscript3 codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"n\"\u003ETrainer\u003C/span\u003E\u003Cspan class=\"o\"\u003E:\u003C/span\u003E\n\n\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"mi\"\u003E1\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERegister\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eweight\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ebuffer\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"k\"\u003Ewith\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMA\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ENIC\u003C/span\u003E\n\n\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"mi\"\u003E2\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGet\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"o\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Eaddr\u003C/span\u003E\u003Cspan class=\"o\"\u003E,\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Erkey\u003C/span\u003E\u003Cspan class=\"o\"\u003E)\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E\n\n\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"mi\"\u003E3\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EShare\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"k\"\u003Ewith\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Egenerators\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"o\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Etiny\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Emessage\u003C/span\u003E\u003Cspan class=\"o\"\u003E)\u003C/span\u003E\n\n\u003Cspan class=\"n\"\u003EGenerator\u003C/span\u003E\u003Cspan class=\"o\"\u003E:\u003C/span\u003E\n\n\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"mi\"\u003E1\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EReceive\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E\n\n\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"mi\"\u003E2\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMA_READ\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Edirectly\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Efrom\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Etrainer\u003C/span\u003E\u003Cspan class=\"err\"\u003E\u0026#39;\u003C/span\u003E\u003Cspan class=\"n\"\u003Es\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ememory\u003C/span\u003E\n\n\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"mi\"\u003E3\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ENo\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ecoordination\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"k\"\u003Ewith\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Etrainer\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eneeded\u003C/span\u003E\u003Cspan class=\"o\"\u003E!\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EThe trainer doesn't even know when generators pull weights. True one-sided.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "be16282c591c150987f44e4b1d502bd2", "console": [], "id": "Xref", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"4-the-magic-pointer-pattern\"\u003E4. The Magic Pointer Pattern\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003ENow here's the key insight from our RDMA discussion: to represent remote data,\nwe only need a \u003Cstrong\u003Etiny handle\u003C/strong\u003E - the \u003Ccode\u003E(addr, rkey, size)\u003C/code\u003E tuple.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EMonarch wraps this in \u003Ccode\u003ERDMABuffer\u003C/code\u003E. Let's see how small it actually is:\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "93cd1380c225976a61d1164279118d8e", "console": [], "id": "SFPL", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "33f52127ad06527402c41752fe53ec8f", "console": [{"mimetype": "text/plain", "name": "stderr", "text": "Monarch internal logs are being written to /tmp/allencwang/monarch_log.log; execution id allencwang_Feb-02_17:59_487\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "RDMABuffer handle size vs actual tensor size:\n\nTensor Size  Actual Bytes    Handle Size     Ratio     \n-------------------------------------------------------\n1 KB                1,024 B      437 B               2x\n1 MB            1,048,576 B      440 B           2,383x\n10 MB          10,485,760 B      441 B          23,777x\n\n\u2192 Handle size is O(1) regardless of tensor size!\n", "type": "stream"}], "id": "BYtC", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "70a1235b5a21ad6ce9e6c68db0e1da21", "console": [], "id": "RGSE", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"the-magic-pointer\"\u003EThe Magic Pointer\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EThis is the core pattern: \u003Cstrong\u003Eseparate control plane from data plane\u003C/strong\u003E.\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003EControl plane\u003C/strong\u003E (actor messages): Send tiny handle (~100 bytes)\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EData plane\u003C/strong\u003E (RDMA): Bulk transfer of actual data (~10 GB)\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cspan class=\"paragraph\"\u003EThink of \u003Ccode\u003ERDMABuffer\u003C/code\u003E as a \u003Cstrong\u003Emagic pointer\u003C/strong\u003E - it's a pointer that works across machines:\u003C/span\u003E\n\u003Cdiv class=\"language-verilog codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"n\"\u003ETrainer\u003C/span\u003E\u003Cspan class=\"w\"\u003E                              \u003C/span\u003E\u003Cspan class=\"n\"\u003EGenerator\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\u003Cspan class=\"w\"\u003E                     \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eweights\u003C/span\u003E\u003Cspan class=\"w\"\u003E     \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E                     \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Elocal\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ecopy\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"mh\"\u003E10\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGB\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"w\"\u003E     \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E                     \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Eempty\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"w\"\u003E     \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\u003Cspan class=\"w\"\u003E                     \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E       \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E                                   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E       \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"mf\"\u003E1.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ECreate\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMABuffer\u003C/span\u003E\u003Cspan class=\"w\"\u003E             \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E       \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E     \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Eregister\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ememory\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eget\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E       \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E                                   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E       \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mf\"\u003E2.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ESend\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"o\"\u003E~\u003C/span\u003E\u003Cspan class=\"mh\"\u003E100\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ebytes\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Evia\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eactor\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E       \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E                                   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E       \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mf\"\u003E3.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMA\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eread\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"o\"\u003E~\u003C/span\u003E\u003Cspan class=\"mh\"\u003E10\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGB\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Evia\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ehardware\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E       \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E        \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Eno\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Etrainer\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Einvolvement\u003C/span\u003E\u003Cspan class=\"o\"\u003E!\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EThe trainer doesn't even know when generators pull weights. True one-sided.\u003C/span\u003E\n\u003Ch3 id=\"rdmabuffer-in-action\"\u003ERDMABuffer in Action\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EFrom \u003Ccode\u003Emonarch.rdma\u003C/code\u003E:\u003C/span\u003E\n\u003Cdiv class=\"language-python codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"kn\"\u003Efrom\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nn\"\u003Emonarch.rdma\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"kn\"\u003Eimport\u003C/span\u003E \u003Cspan class=\"n\"\u003ERDMABuffer\u003C/span\u003E\n\n\u003Cspan class=\"c1\"\u003E# Trainer side: register weights\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Eweights\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Etorch\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Erandn\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"mi\"\u003E1024\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"mi\"\u003E1024\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Edevice\u003C/span\u003E\u003Cspan class=\"o\"\u003E=\u003C/span\u003E\u003Cspan class=\"s2\"\u003E\u0026quot;cuda\u0026quot;\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Ebuffer\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003ERDMABuffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Eweights\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eview\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Etorch\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Euint8\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eflatten\u003C/span\u003E\u003Cspan class=\"p\"\u003E())\u003C/span\u003E\n\n\u003Cspan class=\"c1\"\u003E# Return buffer as part of an endpoint response\u003C/span\u003E\n\u003Cspan class=\"c1\"\u003E# This is a TINY message - just the handle!\u003C/span\u003E\n\u003Cspan class=\"nd\"\u003E@endpoint\u003C/span\u003E\n\u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003Eget_weight_handle\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E \u003Cspan class=\"o\"\u003E-\u0026gt;\u003C/span\u003E \u003Cspan class=\"n\"\u003ERDMABuffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Ereturn\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ebuffer\u003C/span\u003E\n\n\u003Cspan class=\"c1\"\u003E# Generator side: receive handle, pull directly into GPU\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Etrainer\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eget_weight_handle\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecall_one\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eget\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E  \u003Cspan class=\"c1\"\u003E# Tiny message\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Egpu_weights\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Emodel\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eweights\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eview\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Etorch\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Euint8\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eflatten\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eread_into\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Egpu_weights\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eget\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E                   \u003Cspan class=\"c1\"\u003E# Bulk RDMA \u2192 GPU\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003ESee the \u003Ca href=\"https://meta-pytorch.org/monarch/generated/examples/grpo_actor.html\" rel=\"noopener noreferrer\" target=\"_blank\"\u003EGRPO Actor example\u003C/a\u003E\nfor a minimal implementation showing RDMA data flow. We'll build a more complete\nversion in the following sections.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "a6ad4e4137408bf3a9b4bc76f35bd22a", "console": [], "id": "Kclp", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"the-cost-of-memory-registration\"\u003EThe Cost of Memory Registration\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003ERDMA memory registration is \u003Cstrong\u003Eexpensive\u003C/strong\u003E:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003EPins physical pages (prevents swapping)\u003C/li\u003E\n\u003Cli\u003ECreates IOMMU/DMA mappings in the NIC\u003C/li\u003E\n\u003Cli\u003ECan take milliseconds for large buffers\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EDon't register on the hot path!\u003C/strong\u003E Here are three approaches:\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "0bf1eb19c2ff23a73ee6c14e9b288a44", "console": [], "id": "emfo", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch4 id=\"approach-1-naive-bad\"\u003EApproach 1: Naive (Bad)\u003C/h4\u003E\n\u003Cspan class=\"paragraph\"\u003ERe-register every transfer - pays MR cost every time:\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "c552a873dcbe31e85ad569761b4bb7ca", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "NaiveSender: Re-registers all parameters on every call\n", "type": "stream"}], "id": "Hstk", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "62ef4c4311f1e1111bdc67ff94370563", "console": [], "id": "nWHF", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch4 id=\"approach-2-contiguous-buffer-good\"\u003EApproach 2: Contiguous Buffer (Good)\u003C/h4\u003E\n\u003Cspan class=\"paragraph\"\u003EAllocate one buffer, register once, pack params into it:\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "e9811c0fdbc3ed8017f0094270e5c842", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "ContiguousSender: Registers once, reuses same handle\n", "type": "stream"}], "id": "iLit", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "50aa8d04b492714022d299fecd65cec3", "console": [], "id": "ZHCJ", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch4 id=\"approach-3-scattered-rdmaaction-good\"\u003EApproach 3: Scattered + RDMAAction (Good)\u003C/h4\u003E\n\u003Cspan class=\"paragraph\"\u003ERegister each parameter once, build RDMAAction transfer plan once, execute repeatedly:\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "f34fae3c9b55f895f8314e3c1cb3f031", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "ScatteredSender: Registers each layer once\nScatteredReceiver: Uses RDMAAction to batch transfers\n", "type": "stream"}], "id": "ROlb", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "c929cc5041f7654386ee971423e91ad3", "console": [], "id": "qnkX", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EKey insight\u003C/strong\u003E: Register in \u003Ccode\u003E__init__\u003C/code\u003E, not in your transfer endpoint!\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EFor the scattered approach, RDMAAction is like a \u003Cstrong\u003Etransfer plan\u003C/strong\u003E - you build it once\nwith all the handles, then just call \u003Ccode\u003Esubmit()\u003C/code\u003E whenever you need to sync.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003ELet's benchmark to see the difference:\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "6c9d83ed28b68f212e2735ac50230c0b", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "=== RDMA Registration Benchmark ===\nTransferring 8000 floats (31.2 KB) x 5 steps\n\nNaive (re-register each step):\n  Step 1: 2195.89ms\n  Step 2: 10.00ms\n  Step 3: 11.55ms\n  Step 4: 11.10ms\n  Step 5: 12.54ms\n  Average: 448.22ms\n\nContiguous (register once):\n  Step 1: 3.51ms\n  Step 2: 3.45ms\n  Step 3: 3.62ms\n  Step 4: 4.08ms\n  Step 5: 3.72ms\n  Average: 3.67ms\n\nScattered + RDMAAction (register once, batch):\n  Step 1: 11.11ms\n  Step 2: 8.97ms\n  Step 3: 9.98ms\n  Step 4: 9.62ms\n  Step 5: 12.80ms\n  Average: 10.49ms\n\n=== Summary ===\nNaive: Pays MR cost EVERY step (slow)\nSmart: Pays MR cost once, subsequent steps are fast\n", "type": "stream"}], "id": "TqIu", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "b95b07f45da7a3e33ec1b3389ffa3e1d", "console": [], "id": "Vxnm", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"two-weight-sync-patterns\"\u003ETwo Weight Sync Patterns\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EWith RDMABuffer as our building block, there are two main approaches:\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EPattern\u003C/th\u003E\n\u003Cth\u003EHow it works\u003C/th\u003E\n\u003Cth\u003ETrade-offs\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Cstrong\u003ECPU Staging\u003C/strong\u003E\u003C/td\u003E\n\u003Ctd\u003EGPU \u2192 CPU buffer \u2192 RDMA \u2192 CPU \u2192 GPU\u003C/td\u003E\n\u003Ctd\u003EOne MR, simple, but copies\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Cstrong\u003EDirect GPU\u003C/strong\u003E\u003C/td\u003E\n\u003Ctd\u003EGPU \u2192 RDMA \u2192 GPU (GPUDirect)\u003C/td\u003E\n\u003Ctd\u003ENo copies, but one MR per param\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EPattern 1: CPU Staging (Contiguous Buffer)\u003C/strong\u003E\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EPack all parameters into one contiguous CPU buffer, register once:\u003C/span\u003E\n\u003Cdiv class=\"language-python codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"k\"\u003Eclass\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nc\"\u003ETrainer\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003EActor\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"fm\"\u003E__init__\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n        \u003Cspan class=\"c1\"\u003E# Calculate total size for all parameters\u003C/span\u003E\n        \u003Cspan class=\"n\"\u003Etotal_bytes\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"nb\"\u003Esum\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Ep\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Enumel\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E \u003Cspan class=\"o\"\u003E*\u003C/span\u003E \u003Cspan class=\"n\"\u003Ep\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eelement_size\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E \u003Cspan class=\"k\"\u003Efor\u003C/span\u003E \u003Cspan class=\"n\"\u003Ep\u003C/span\u003E \u003Cspan class=\"ow\"\u003Ein\u003C/span\u003E \u003Cspan class=\"n\"\u003Emodel\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eparameters\u003C/span\u003E\u003Cspan class=\"p\"\u003E())\u003C/span\u003E\n\n        \u003Cspan class=\"c1\"\u003E# Allocate ONE contiguous buffer, register ONCE\u003C/span\u003E\n        \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Estaging_buffer\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Etorch\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eempty\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Etotal_bytes\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Edtype\u003C/span\u003E\u003Cspan class=\"o\"\u003E=\u003C/span\u003E\u003Cspan class=\"n\"\u003Etorch\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Euint8\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n        \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003ERDMABuffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Estaging_buffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\n        \u003Cspan class=\"c1\"\u003E# Track where each param lives in the buffer\u003C/span\u003E\n        \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eparam_offsets\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Ecompute_offsets\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Emodel\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\n    \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003Epack_weights\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E        \u003C/span\u003E\u003Cspan class=\"sd\"\u003E\u0026#39;\u0026#39;\u0026#39;Copy all params into contiguous buffer.\u0026#39;\u0026#39;\u0026#39;\u003C/span\u003E\n        \u003Cspan class=\"k\"\u003Efor\u003C/span\u003E \u003Cspan class=\"n\"\u003Ename\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Eparam\u003C/span\u003E \u003Cspan class=\"ow\"\u003Ein\u003C/span\u003E \u003Cspan class=\"n\"\u003Emodel\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Enamed_parameters\u003C/span\u003E\u003Cspan class=\"p\"\u003E():\u003C/span\u003E\n            \u003Cspan class=\"n\"\u003Eoffset\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eparam_offsets\u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Ename\u003C/span\u003E\u003Cspan class=\"p\"\u003E]\u003C/span\u003E\n            \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Estaging_buffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Eoffset\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E\u003Cspan class=\"n\"\u003Eoffset\u003C/span\u003E\u003Cspan class=\"o\"\u003E+\u003C/span\u003E\u003Cspan class=\"n\"\u003Esize\u003C/span\u003E\u003Cspan class=\"p\"\u003E]\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecopy_\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Eparam\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eview\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Etorch\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Euint8\u003C/span\u003E\u003Cspan class=\"p\"\u003E))\u003C/span\u003E\n\n    \u003Cspan class=\"nd\"\u003E@endpoint\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003Eget_weight_handle\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E \u003Cspan class=\"o\"\u003E-\u0026gt;\u003C/span\u003E \u003Cspan class=\"n\"\u003ERDMABuffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E\n        \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Epack_weights\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\n        \u003Cspan class=\"k\"\u003Ereturn\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E  \u003Cspan class=\"c1\"\u003E# Same handle, new data\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EPattern 2: Direct GPU MRs\u003C/strong\u003E\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003ERegister each GPU parameter directly, no CPU copies:\u003C/span\u003E\n\u003Cdiv class=\"language-python codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"k\"\u003Eclass\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nc\"\u003ETrainer\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003EActor\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"fm\"\u003E__init__\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n        \u003Cspan class=\"c1\"\u003E# Register each param ONCE at startup\u003C/span\u003E\n        \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ehandles\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"p\"\u003E{}\u003C/span\u003E\n        \u003Cspan class=\"k\"\u003Efor\u003C/span\u003E \u003Cspan class=\"n\"\u003Ename\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Eparam\u003C/span\u003E \u003Cspan class=\"ow\"\u003Ein\u003C/span\u003E \u003Cspan class=\"n\"\u003Emodel\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Enamed_parameters\u003C/span\u003E\u003Cspan class=\"p\"\u003E():\u003C/span\u003E\n            \u003Cspan class=\"n\"\u003Ebyte_view\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Eparam\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Edata\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eview\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Etorch\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Euint8\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eflatten\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\n            \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ehandles\u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Ename\u003C/span\u003E\u003Cspan class=\"p\"\u003E]\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003ERDMABuffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Ebyte_view\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\n    \u003Cspan class=\"nd\"\u003E@endpoint\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003Eget_param_handles\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E \u003Cspan class=\"o\"\u003E-\u0026gt;\u003C/span\u003E \u003Cspan class=\"nb\"\u003Edict\u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"nb\"\u003Estr\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003ERDMABuffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E]:\u003C/span\u003E\n        \u003Cspan class=\"c1\"\u003E# Handles are reused - data updates in place\u003C/span\u003E\n        \u003Cspan class=\"k\"\u003Ereturn\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ehandles\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EBoth patterns amortize MR registration cost across training iterations.\nLet's look at CPU staging in more detail (it's more common in async RL).\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "a8e57a271297ac88d2492cc4df33416e", "console": [], "id": "DnEU", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"5-cpu-staging-pattern\"\u003E5. CPU Staging Pattern\u003C/h2\u003E\n\u003Ch3 id=\"gpu-native-rdma-works\"\u003EGPU-Native RDMA Works!\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EFirst, let's be clear: \u003Cstrong\u003EGPU-native RDMA works\u003C/strong\u003E and is fast:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003EGPUDirect RDMA: NIC reads directly from GPU memory\u003C/li\u003E\n\u003Cli\u003ENo CPU copy needed (when hardware supports it)\u003C/li\u003E\n\u003Cli\u003EGreat for synchronous transfers\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch3 id=\"why-cpu-staging-for-async-rl\"\u003EWhy CPU Staging for Async RL?\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EThe issue isn't bandwidth - it's \u003Cstrong\u003Etiming\u003C/strong\u003E:\u003C/span\u003E\n\u003Cdiv class=\"language-text codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003EDirect GPU\u2192GPU RDMA:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Generator GPU is mid-inference                       \u2502\n\u2502 \u251c\u2500\u2500 layer 1 \u2500\u2500\u2524 [RDMA arrives, needs sync!]         \u2502\n\u2502               \u2193                                      \u2502\n\u2502         cudaDeviceSynchronize()  \u2190 Blocks inference! \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EWith CPU staging, nothing on the critical path blocks:\u003C/span\u003E\n\u003Cdiv class=\"language-verilog codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"n\"\u003ETrainer\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGPU\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2500\u2500\u25ba\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ECPU\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Estaging\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ebuffer\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMA\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eregistered\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E                      \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E                      \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003ESits\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ehere\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eready\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eanytime\u003C/span\u003E\u003Cspan class=\"p\"\u003E]\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E                      \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E                      \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u25bc\u003C/span\u003E\n\u003Cspan class=\"n\"\u003EGenerator\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Egrabs\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ewhen\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eready\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2500\u2500\u25ba\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGenerator\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGPU\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EThe CPU buffer is a \u003Cstrong\u003Etemporal decoupling point\u003C/strong\u003E.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "dd06d4354b46dece7fc2c1062c403486", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "Trainer: Weights copied to CPU staging buffer (RDMA registered)\n  GPU memory: cuda:0\n  CPU staging: pinned=True\nGenerator: Weights loaded directly to GPU (via RDMA)\n  Weights match: True\n", "type": "stream"}], "id": "ulZA", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "1f92550528443774fa943b82fb79d765", "console": [], "id": "ecfG", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"6-circular-weight-buffers\"\u003E6. Circular Weight Buffers\u003C/h2\u003E\n\u003Ch3 id=\"the-versioning-problem\"\u003EThe Versioning Problem\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EIn async RL, trainer updates weights continuously. Generators need to:\u003C/span\u003E\n\u003Col\u003E\n\u003Cli\u003E\u003Cstrong\u003EGrab the latest\u003C/strong\u003E weights (not stale ones)\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003ENot block\u003C/strong\u003E waiting for updates\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EAvoid memory churn\u003C/strong\u003E (re-registering RDMA buffers is expensive)\u003C/li\u003E\n\u003C/ol\u003E\n\u003Ch3 id=\"solution-circular-buffer\"\u003ESolution: Circular Buffer\u003C/h3\u003E\n\u003Cdiv class=\"language-tsql codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"n\"\u003ETrainer\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nl\"\u003Ewrites\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E\u003Cspan class=\"w\"\u003E     \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev0\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2192\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev1\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2192\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev2\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2192\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev3\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2192\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev4\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2192\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev5\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2192\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"p\"\u003E...\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E                     \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2193\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2193\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2193\u003C/span\u003E\n\u003Cspan class=\"n\"\u003EBuffer\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nl\"\u003Eslots\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E\u003Cspan class=\"w\"\u003E      \u003C/span\u003E\u003Cspan class=\"o\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Eslot0\u003C/span\u003E\u003Cspan class=\"o\"\u003E][\u003C/span\u003E\u003Cspan class=\"n\"\u003Eslot1\u003C/span\u003E\u003Cspan class=\"o\"\u003E][\u003C/span\u003E\u003Cspan class=\"n\"\u003Eslot2\u003C/span\u003E\u003Cspan class=\"o\"\u003E]\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecircular\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ereused\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E                     \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev3\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev4\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev5\u003C/span\u003E\n\n\u003Cspan class=\"n\"\u003EGenerator\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"k\"\u003Ereads\u003C/span\u003E\u003Cspan class=\"err\"\u003E:\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"ss\"\u003E\u0026quot;Give me latest\u0026quot;\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2192\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev5\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EBenefits:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003EPre-registered RDMA buffers\u003C/strong\u003E - no memory registration on hot path\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003ELock-free reads\u003C/strong\u003E - generators always get a consistent snapshot\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EBounded memory\u003C/strong\u003E - only N versions in flight\u003C/li\u003E\n\u003C/ul\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "f46a13dd70424ef26feb00afffdfb71a", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "Published version 0\nPublished version 1\nPublished version 2\nPublished version 3\nPublished version 4\n\nGenerator got version 4, weights mean: 0.02\nVersion 1 available: False\n", "type": "stream"}], "id": "Pvdt", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "cf2298bef2f9d4413e1e9abdee5a7da6", "console": [], "id": "ZBYS", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"7-weight-re-sharding\"\u003E7. Weight Re-sharding\u003C/h2\u003E\n\u003Ch3 id=\"the-sharding-mismatch-problem\"\u003EThe Sharding Mismatch Problem\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003ETrainer and Generator often have \u003Cstrong\u003Edifferent tensor layouts\u003C/strong\u003E:\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003ERole\u003C/th\u003E\n\u003Cth\u003EParallelism\u003C/th\u003E\n\u003Cth\u003ESharding\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003ETrainer\u003C/td\u003E\n\u003Ctd\u003EFSDP (8 GPUs)\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003EShard(0)\u003C/code\u003E - rows split across 8 GPUs\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EGenerator\u003C/td\u003E\n\u003Ctd\u003ETP (2 GPUs)\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003EShard(1)\u003C/code\u003E - columns split across 2 GPUs\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003EDirect weight transfer doesn't work - we need \u003Cstrong\u003Ere-sharding\u003C/strong\u003E.\u003C/span\u003E\n\u003Cdiv class=\"language-text codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003ETrainer (row-sharded):          Generator (column-sharded):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 GPU 0: rows 0-127\u2502            \u2502 GPU 0   \u2502 GPU 1   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524     \u2192      \u2502 cols    \u2502 cols    \u2502\n\u2502 GPU 1: rows 128+ \u2502            \u2502 0-511   \u2502 512+    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "cf55cb42141c833974d8ecb1b6877238", "console": [], "id": "aLJB", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"approach-1-gather-then-slice\"\u003EApproach 1: Gather Then Slice\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003ESimple but inefficient:\u003C/span\u003E\n\u003Cdiv class=\"language-text codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E1. Each receiver gathers ALL sender shards \u2192 full tensor\n\n2. Each receiver slices out its portion\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EPros\u003C/strong\u003E: Simple, works with any sharding\n\u003Cstrong\u003ECons\u003C/strong\u003E: 2x redundant data transfer (each receiver gets everything)\u003C/span\u003E\n\u003Ch3 id=\"approach-2-routeddirect-transfer\"\u003EApproach 2: Routed/Direct Transfer\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EOptimal but complex:\u003C/span\u003E\n\u003Cdiv class=\"language-verilog codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"mf\"\u003E1.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EPre\u003C/span\u003E\u003Cspan class=\"o\"\u003E-\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecompute\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ewhich\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Esender\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Echunks\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eoverlap\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ewith\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ewhich\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ereceiver\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eregions\u003C/span\u003E\n\n\u003Cspan class=\"mf\"\u003E2.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ESend\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eonly\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ethe\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eexact\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Echunks\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eneeded\u003C/span\u003E\n\n\u003Cspan class=\"mf\"\u003E3.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EReceivers\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eplace\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Echunks\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Edirectly\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ein\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ecorrect\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Epositions\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EPros\u003C/strong\u003E: Minimal bandwidth (no redundancy)\n\u003Cstrong\u003ECons\u003C/strong\u003E: Complex overlap computation\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "01a93cde7e5ea82b6a4aa8a7a8c4acf8", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "Trainer shards (row-wise, 4 GPUs):\n  Rank 0: offset=(0, 0), shape=(256, 1024)\n  Rank 1: offset=(256, 0), shape=(256, 1024)\n  Rank 2: offset=(512, 0), shape=(256, 1024)\n  Rank 3: offset=(768, 0), shape=(256, 1024)\n\nGenerator shards (column-wise, 2 GPUs):\n  Rank 0: offset=(0, 0), shape=(1024, 512)\n  Rank 1: offset=(0, 512), shape=(1024, 512)\n", "type": "stream"}], "id": "nHfw", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "dd9ef142fcda82825b003d755d4b8bc5", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "Transfer plan: 8 chunks needed\n\nSender 0 \u2192 Receiver 0\n  Read from sender offset (0, 0), shape (256, 512)\n  Write to receiver offset (0, 0)\n\nSender 0 \u2192 Receiver 1\n  Read from sender offset (0, 512), shape (256, 512)\n  Write to receiver offset (0, 0)\n\nSender 1 \u2192 Receiver 0\n  Read from sender offset (0, 0), shape (256, 512)\n  Write to receiver offset (256, 0)\n\nSender 1 \u2192 Receiver 1\n  Read from sender offset (0, 512), shape (256, 512)\n  Write to receiver offset (256, 0)\n\nSender 2 \u2192 Receiver 0\n  Read from sender offset (0, 0), shape (256, 512)\n  Write to receiver offset (512, 0)\n\nSender 2 \u2192 Receiver 1\n  Read from sender offset (0, 512), shape (256, 512)\n  Write to receiver offset (512, 0)\n\nSender 3 \u2192 Receiver 0\n  Read from sender offset (0, 0), shape (256, 512)\n  Write to receiver offset (768, 0)\n\nSender 3 \u2192 Receiver 1\n  Read from sender offset (0, 512), shape (256, 512)\n  Write to receiver offset (768, 0)\n\n", "type": "stream"}], "id": "xXTn", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "451e5f375cd885f5faa4be78edff6345", "console": [], "id": "AjVT", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"the-dtensor-dream-and-reality\"\u003EThe DTensor Dream (and Reality)\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EIf trainer and generator both used \u003Cstrong\u003EDTensor\u003C/strong\u003E with the same sharding spec, re-sharding\nwould be automatic - the framework handles overlap computation and transfers.\u003C/span\u003E\n\u003Cdiv class=\"language-python codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"c1\"\u003E# Ideal world: DTensor handles it\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Etrainer_weights\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E \u003Cspan class=\"n\"\u003EDTensor\u003C/span\u003E  \u003Cspan class=\"c1\"\u003E# Shard(0) across 8 GPUs\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Egenerator_weights\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E \u003Cspan class=\"n\"\u003EDTensor\u003C/span\u003E  \u003Cspan class=\"c1\"\u003E# Shard(1) across 2 GPUs\u003C/span\u003E\n\n\u003Cspan class=\"c1\"\u003E# Re-sharding is just redistribution\u003C/span\u003E\n\u003Cspan class=\"n\"\u003Egenerator_weights\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Etrainer_weights\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eredistribute\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Egenerator_placement\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EIn practice, it's harder\u003C/strong\u003E:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003EvLLM does its own sharding and weight fusing (not DTensor-compatible)\u003C/li\u003E\n\u003Cli\u003ETraining frameworks (FSDP, etc.) have different abstractions\u003C/li\u003E\n\u003Cli\u003EYou often need custom overlap computation like we showed above\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cspan class=\"paragraph\"\u003EThe routed approach (compute overlaps, send only needed chunks) is 2x faster than\nnaive gather-then-slice, but requires this manual coordination.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EFor cross-node RDMA transfers\u003C/strong\u003E, the key insight remains: pre-compute the transfer\nplan once, then each sync just executes the planned RDMA operations with RDMAAction.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "806222a0a76efaa0bba91baefc514224", "console": [], "id": "pHFh", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"8-putting-it-all-together\"\u003E8. Putting It All Together\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EThe full async RL weight sync pattern:\u003C/span\u003E\n\u003Cdiv class=\"language-verilog codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E                         \u003C/span\u003E\u003Cspan class=\"n\"\u003ETRAINER\u003C/span\u003E\u003Cspan class=\"w\"\u003E                                  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"mf\"\u003E1.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ETrain\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Estep\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ecompletes\u003C/span\u003E\u003Cspan class=\"w\"\u003E                                        \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"mf\"\u003E2.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ECopy\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eweights\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eto\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ECPU\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Estaging\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ebuffer\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Enon\u003C/span\u003E\u003Cspan class=\"o\"\u003E-\u003C/span\u003E\u003Cspan class=\"n\"\u003Eblocking\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ED2H\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"w\"\u003E       \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"mf\"\u003E3.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EPublish\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eto\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ecircular\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ebuffer\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ewith\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eversion\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Etag\u003C/span\u003E\u003Cspan class=\"w\"\u003E                 \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"mf\"\u003E4.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EContinue\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Etraining\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Eno\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eblocking\u003C/span\u003E\u003Cspan class=\"o\"\u003E!\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"w\"\u003E                            \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E                                \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E                                \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u25bc\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E              \u003C/span\u003E\u003Cspan class=\"n\"\u003ECIRCULAR\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EBUFFER\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003ECPU\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMA\u003C/span\u003E\u003Cspan class=\"o\"\u003E-\u003C/span\u003E\u003Cspan class=\"n\"\u003Eregistered\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"w\"\u003E             \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Eslot\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mh\"\u003E0\u003C/span\u003E\u003Cspan class=\"o\"\u003E:\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev3\u003C/span\u003E\u003Cspan class=\"p\"\u003E]\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Eslot\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mh\"\u003E1\u003C/span\u003E\u003Cspan class=\"o\"\u003E:\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev4\u003C/span\u003E\u003Cspan class=\"p\"\u003E]\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Eslot\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mh\"\u003E2\u003C/span\u003E\u003Cspan class=\"o\"\u003E:\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev5\u003C/span\u003E\u003Cspan class=\"p\"\u003E]\u003C/span\u003E\u003Cspan class=\"w\"\u003E                        \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E                                 \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2191\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Elatest\u003C/span\u003E\u003Cspan class=\"w\"\u003E                        \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E                                \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E          \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E          \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u25bc\u003C/span\u003E\u003Cspan class=\"w\"\u003E                     \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u25bc\u003C/span\u003E\u003Cspan class=\"w\"\u003E                     \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u25bc\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"n\"\u003EGENERATOR\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mh\"\u003E0\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"n\"\u003EGENERATOR\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mh\"\u003E1\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"n\"\u003EGENERATOR\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mh\"\u003E2\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E                 \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E                 \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E                 \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EAfter\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Egen\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nl\"\u003Edone:\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EAfter\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Egen\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nl\"\u003Edone:\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EAfter\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Egen\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nl\"\u003Edone:\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mf\"\u003E1.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGet\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Elatest\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mf\"\u003E1.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGet\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Elatest\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mf\"\u003E1.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGet\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Elatest\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"n\"\u003Eversion\u003C/span\u003E\u003Cspan class=\"w\"\u003E      \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"n\"\u003Eversion\u003C/span\u003E\u003Cspan class=\"w\"\u003E      \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"n\"\u003Eversion\u003C/span\u003E\u003Cspan class=\"w\"\u003E      \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mf\"\u003E2.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMA\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eread\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mf\"\u003E2.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMA\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eread\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mf\"\u003E2.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMA\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eread\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2192\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGPU\u003C/span\u003E\u003Cspan class=\"w\"\u003E        \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2192\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGPU\u003C/span\u003E\u003Cspan class=\"w\"\u003E        \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2192\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGPU\u003C/span\u003E\u003Cspan class=\"w\"\u003E        \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mf\"\u003E3.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERe\u003C/span\u003E\u003Cspan class=\"o\"\u003E-\u003C/span\u003E\u003Cspan class=\"n\"\u003Eshard\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"k\"\u003Eif\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mf\"\u003E3.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERe\u003C/span\u003E\u003Cspan class=\"o\"\u003E-\u003C/span\u003E\u003Cspan class=\"n\"\u003Eshard\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"k\"\u003Eif\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mf\"\u003E3.\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERe\u003C/span\u003E\u003Cspan class=\"o\"\u003E-\u003C/span\u003E\u003Cspan class=\"n\"\u003Eshard\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"k\"\u003Eif\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"n\"\u003Eneeded\u003C/span\u003E\u003Cspan class=\"w\"\u003E       \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"n\"\u003Eneeded\u003C/span\u003E\u003Cspan class=\"w\"\u003E       \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"n\"\u003Eneeded\u003C/span\u003E\u003Cspan class=\"w\"\u003E       \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EKey properties:\u003C/strong\u003E\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003ETrainer never blocks waiting for generators\u003C/li\u003E\n\u003Cli\u003EGenerators pull directly to GPU when \u003Cem\u003Ethey're\u003C/em\u003E ready\u003C/li\u003E\n\u003Cli\u003ERe-sharding happens locally on each generator\u003C/li\u003E\n\u003Cli\u003ECircular buffer bounds memory, reuses RDMA registrations\u003C/li\u003E\n\u003C/ul\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "26dba11147530b7414a96869242e3902", "console": [], "id": "NCOB", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"code-pattern\"\u003ECode Pattern\u003C/h3\u003E\n\u003Cdiv class=\"language-python codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"c1\"\u003E# Trainer side\u003C/span\u003E\n\u003Cspan class=\"k\"\u003Eclass\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nc\"\u003ETrainer\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003EActor\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"fm\"\u003E__init__\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n        \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eweight_buffer\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003ECircularWeightBuffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\n            \u003Cspan class=\"n\"\u003Etemplate\u003C/span\u003E\u003Cspan class=\"o\"\u003E=\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Emodel\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Estate_dict_tensor\u003C/span\u003E\u003Cspan class=\"p\"\u003E(),\u003C/span\u003E\n            \u003Cspan class=\"n\"\u003En_slots\u003C/span\u003E\u003Cspan class=\"o\"\u003E=\u003C/span\u003E\u003Cspan class=\"mi\"\u003E3\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E\n        \u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\n    \u003Cspan class=\"nd\"\u003E@endpoint\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003Eget_weight_handle\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E \u003Cspan class=\"o\"\u003E-\u0026gt;\u003C/span\u003E \u003Cspan class=\"n\"\u003ETuple\u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMABuffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"nb\"\u003Eint\u003C/span\u003E\u003Cspan class=\"p\"\u003E]:\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E        \u003C/span\u003E\u003Cspan class=\"sd\"\u003E\u0026#39;\u0026#39;\u0026#39;Return handle to latest weights and version.\u0026#39;\u0026#39;\u0026#39;\u003C/span\u003E\n        \u003Cspan class=\"n\"\u003Eslot_idx\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eweight_buffer\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Elatest_version\u003C/span\u003E \u003Cspan class=\"o\"\u003E-\u003C/span\u003E \u003Cspan class=\"mi\"\u003E1\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E \u003Cspan class=\"o\"\u003E%\u003C/span\u003E \u003Cspan class=\"mi\"\u003E3\u003C/span\u003E\n        \u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eweight_buffer\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Erdma_handles\u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Eslot_idx\u003C/span\u003E\u003Cspan class=\"p\"\u003E]\u003C/span\u003E\n        \u003Cspan class=\"n\"\u003Eversion\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eweight_buffer\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Elatest_version\u003C/span\u003E \u003Cspan class=\"o\"\u003E-\u003C/span\u003E \u003Cspan class=\"mi\"\u003E1\u003C/span\u003E\n        \u003Cspan class=\"k\"\u003Ereturn\u003C/span\u003E \u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Eversion\u003C/span\u003E\n\n    \u003Cspan class=\"k\"\u003Easync\u003C/span\u003E \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003Etrain_loop\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n        \u003Cspan class=\"k\"\u003Ewhile\u003C/span\u003E \u003Cspan class=\"kc\"\u003ETrue\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E\n            \u003Cspan class=\"n\"\u003Eloss\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Etrain_step\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\n            \u003Cspan class=\"k\"\u003Eif\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Estep\u003C/span\u003E \u003Cspan class=\"o\"\u003E%\u003C/span\u003E \u003Cspan class=\"n\"\u003Esync_interval\u003C/span\u003E \u003Cspan class=\"o\"\u003E==\u003C/span\u003E \u003Cspan class=\"mi\"\u003E0\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E\n                \u003Cspan class=\"c1\"\u003E# Non-blocking publish\u003C/span\u003E\n                \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eweight_buffer\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Epublish\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Emodel\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eweights\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\n\u003Cspan class=\"c1\"\u003E# Generator side\u003C/span\u003E\n\u003Cspan class=\"k\"\u003Eclass\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nc\"\u003EGenerator\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003EActor\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n    \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"fm\"\u003E__init__\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Etrainer_ref\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n        \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Etrainer\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Etrainer_ref\u003C/span\u003E\n        \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecurrent_version\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"o\"\u003E-\u003C/span\u003E\u003Cspan class=\"mi\"\u003E1\u003C/span\u003E\n\n    \u003Cspan class=\"k\"\u003Easync\u003C/span\u003E \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003Emaybe_sync_weights\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n        \u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E\u003Cspan class=\"p\"\u003E,\u003C/span\u003E \u003Cspan class=\"n\"\u003Eversion\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"k\"\u003Eawait\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Etrainer\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eget_weight_handle\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecall_one\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eget\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\n        \u003Cspan class=\"k\"\u003Eif\u003C/span\u003E \u003Cspan class=\"n\"\u003Eversion\u003C/span\u003E \u003Cspan class=\"o\"\u003E\u0026gt;\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecurrent_version\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E\n            \u003Cspan class=\"c1\"\u003E# Pull via RDMA directly into GPU memory\u003C/span\u003E\n            \u003Cspan class=\"n\"\u003Egpu_weights\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Emodel\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eweights\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eview\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Etorch\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Euint8\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eflatten\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\n            \u003Cspan class=\"k\"\u003Eawait\u003C/span\u003E \u003Cspan class=\"n\"\u003Ehandle\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Eread_into\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Egpu_weights\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n            \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecurrent_version\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"n\"\u003Eversion\u003C/span\u003E\n\n    \u003Cspan class=\"k\"\u003Easync\u003C/span\u003E \u003Cspan class=\"k\"\u003Edef\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"nf\"\u003Egenerate_loop\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"p\"\u003E):\u003C/span\u003E\n        \u003Cspan class=\"k\"\u003Ewhile\u003C/span\u003E \u003Cspan class=\"kc\"\u003ETrue\u003C/span\u003E\u003Cspan class=\"p\"\u003E:\u003C/span\u003E\n            \u003Cspan class=\"k\"\u003Eawait\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Emaybe_sync_weights\u003C/span\u003E\u003Cspan class=\"p\"\u003E()\u003C/span\u003E\n            \u003Cspan class=\"n\"\u003Eoutput\u003C/span\u003E \u003Cspan class=\"o\"\u003E=\u003C/span\u003E \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Egenerate\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Eprompt\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n            \u003Cspan class=\"bp\"\u003Eself\u003C/span\u003E\u003Cspan class=\"o\"\u003E.\u003C/span\u003E\u003Cspan class=\"n\"\u003Esubmit_to_buffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Eoutput\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "a3abb4c511005ddfec0279d1c84e6846", "console": [], "id": "aqbW", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"summary\"\u003ESummary\u003C/h2\u003E\n\u003Ch3 id=\"key-takeaways\"\u003EKey Takeaways\u003C/h3\u003E\n\u003Col\u003E\n\u003Cli\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EBandwidth hierarchy matters\u003C/strong\u003E: NVLink (450 GB/s) \u0026gt;\u0026gt; InfiniBand (50-100 GB/s) \u0026gt;\u0026gt; PCIe\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003EKnow your hardware, optimize for the right interconnect\u003C/li\u003E\n\u003C/ul\u003E\n\u003C/li\u003E\n\u003Cli\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ECollectives vs P2P\u003C/strong\u003E: Collectives are synchronized; RL needs async P2P\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003EHigh variance in generation times makes blocking expensive\u003C/li\u003E\n\u003C/ul\u003E\n\u003C/li\u003E\n\u003Cli\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EMagic pointer pattern\u003C/strong\u003E: Tiny handle over control plane, bulk data over data plane\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003E~100 bytes to describe 10 GB transfer\u003C/li\u003E\n\u003C/ul\u003E\n\u003C/li\u003E\n\u003Cli\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ECPU staging\u003C/strong\u003E: Temporal decoupling for async RL\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003EGPU-native RDMA works for sync cases\u003C/li\u003E\n\u003Cli\u003ECPU staging ensures nothing blocks on the critical path\u003C/li\u003E\n\u003C/ul\u003E\n\u003C/li\u003E\n\u003Cli\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ECircular buffers\u003C/strong\u003E: Version weights without memory churn\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003EPre-register RDMA buffers, reuse slots\u003C/li\u003E\n\u003Cli\u003EGenerators grab latest, never stale\u003C/li\u003E\n\u003C/ul\u003E\n\u003C/li\u003E\n\u003Cli\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EWeight re-sharding\u003C/strong\u003E: Different layouts need overlap computation\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003ERouted approach is 2x faster than gather\u003C/li\u003E\n\u003Cli\u003EPre-compute transfer plan, minimize redundant data\u003C/li\u003E\n\u003C/ul\u003E\n\u003C/li\u003E\n\u003C/ol\u003E\n\u003Ch3 id=\"next-steps\"\u003ENext Steps\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003ESee \u003Cstrong\u003E07_async_rl_e2e.py\u003C/strong\u003E for a complete async RL system that uses these patterns.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}], "metadata": {"marimo_version": "0.19.7"}, "version": "1"},
            "runtimeConfig": null,
        };
    </script>
  
<marimo-code hidden="">
    import%20marimo%0A%0A__generated_with%20%3D%20%220.19.7%22%0Aapp%20%3D%20marimo.App(width%3D%22medium%22)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20import%20marimo%20as%20mo%0A%20%20%20%20return%20(mo%2C)%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%20RDMA%20%26%20Weight%20Synchronization%0A%0A%20%20%20%20This%20notebook%20explores%20efficient%20weight%20synchronization%20for%20async%20RL%20systems%3A%0A%0A%20%20%20%201.%20**The%20Bandwidth%20Hierarchy**%20-%20NVLink%2C%20InfiniBand%2C%20PCIe%0A%20%20%20%202.%20**The%20Problem%3A%20Collectives%20Are%20Blocking**%20-%20Why%20RL%20needs%20something%20different%0A%20%20%20%203.%20**How%20RDMA%20Works**%20-%20ibverbs%2C%20one-sided%20operations%0A%20%20%20%204.%20**The%20Magic%20Pointer%20Pattern**%20-%20Control%20plane%20vs%20data%20plane%20separation%0A%20%20%20%205.%20**CPU%20Staging**%20-%20Decoupling%20trainer%20and%20generator%20timing%0A%20%20%20%206.%20**Circular%20Weight%20Buffers**%20-%20Versioning%20without%20memory%20churn%0A%20%20%20%207.%20**Weight%20Re-sharding**%20-%20Handling%20different%20tensor%20layouts%0A%20%20%20%208.%20**Putting%20It%20All%20Together**%20-%20The%20complete%20pattern%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%201.%20The%20Bandwidth%20Hierarchy%0A%0A%20%20%20%20Modern%20HPC%20clusters%20have%20multiple%20interconnects%20with%20vastly%20different%20bandwidths%3A%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20NODE%20A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20NVSwitch%20%2F%20NVLink%20Fabric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s%20NVLink%20%E2%94%82%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%AC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3D%3D%3D%3D%3D%3D%20%2064%20GB%2Fs%20PCIe%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20------%2048%20GB%2Fs%20------%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%B4%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%20%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%94%82%20%20CPU%200%20%20%E2%94%82%20%20%20%20%20CPU%20interconnect%20%20%20%E2%94%82%20%20CPU%201%20%20%E2%94%82%20%3D%3D%3D%3D%3D%3D%2064%20GB%2Fs%20%E2%95%90%E2%95%90%E2%94%82%20NIC%200%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%E2%94%82%20NIC%201%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%AC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%AC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%20PCIe%20%20%20%20%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%AC%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%20%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%AC%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%2064%20GB%2Fs%20PCIe%20%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%AA%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%AA%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%BC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%BC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3D%3D%3D%3D%3D%3D%20%2050%20GB%2Fs%20%20%20%3D%3D%3D%3D%3D%3D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20IB%20NDR400%20%20%20%20%20%20%20%20%20IB%20NDR400%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%B4%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%B4%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20InfiniBand%20Switch%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%AC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%AC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3D%3D%3D%3D%3D%3D%20%2050%20GB%2Fs%20%20%20%3D%3D%3D%3D%3D%3D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20IB%20NDR400%20%20%20%20%20%20%20%20%20IB%20NDR400%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%BC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%BC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%2064%20GB%2Fs%20PCIe%20%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%AA%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%95%AA%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%B4%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%B4%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%20PCIe%20%20%20%20%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%B4%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%20%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%B4%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%94%82%20%20CPU%200%20%20%E2%94%82%20%20%20%20%20CPU%20interconnect%20%20%20%E2%94%82%20%20CPU%201%20%20%E2%94%82%20%3D%3D%3D%3D%3D%3D%2064%20GB%2Fs%20%E2%95%90%E2%95%90%E2%94%82%20NIC%200%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%E2%94%82%20NIC%201%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20------%2048%20GB%2Fs%20------%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%20%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3D%3D%3D%3D%3D%3D%20%2064%20GB%2Fs%20PCIe%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%B4%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%94%82%20%20%20%20%20%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%20%20900%20GB%2Fs%20NVLink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witch%20%2F%20NVLink%20Fabric%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20NODE%20B%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%0A%20%20%20%20Bandwidth%20encoding%20(line%20intensity)%3A%0A%20%20%20%20%20%20%23%23%23%23%23%23%23%23%20%20NVLink%2FNVSwitch%20%20%20900%20GB%2Fs%20%20%20(GPU%20%E2%86%94%20GPU%2C%20same%20node)%0A%20%20%20%20%20%20%3D%3D%3D%3D%3D%3D%3D%3D%20%20PCIe%20Gen5%20%2F%20IB%20%20%20%20%2050-64%20GB%2Fs%20(CPU%E2%86%94GPU%2C%20CPU%E2%86%94NIC%2C%20cross-node)%0A%20%20%20%20%20%20--------%20%20CPU%20interconnect%20%20%2048%20GB%2Fs%20%20%20(CPU%20%E2%86%94%20CPU%2C%20same%20node)%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20RDMA%20can%20transfer%20between%20any%20registered%20memory%20(CPU%20or%20GPU)%20via%20the%20NICs.%0A%0A%20%20%20%20%7C%20Interconnect%20%7C%20Bandwidth%20%7C%20Latency%20%7C%20Use%20Case%20%7C%0A%20%20%20%20%7C--------------%7C-----------%7C---------%7C----------%7C%0A%20%20%20%20%7C%20**NVLink%2FNVSwitch**%20%7C%20900%20GB%2Fs%20%7C%20~1%20%CE%BCs%20%7C%20Same-node%20GPU%E2%86%94GPU%20%7C%0A%20%20%20%20%7C%20**InfiniBand%20NDR400**%20%7C%2050%20GB%2Fs%20%7C%20~1-2%20%CE%BCs%20%7C%20Cross-node%20RDMA%20%7C%0A%20%20%20%20%7C%20**PCIe%20Gen5%20x16**%20%7C%2064%20GB%2Fs%20%7C%20~1-2%20%CE%BCs%20%7C%20CPU%E2%86%94GPU%2C%20CPU%E2%86%94NIC%20%7C%0A%20%20%20%20%7C%20**CPU%20interconnect**%20%7C%2048%20GB%2Fs%20%7C%20~100%20ns%20%7C%20CPU%E2%86%94CPU%20(same%20node)%20%7C%0A%0A%20%20%20%20**Key%20observations%3A**%0A%0A%20%20%20%201.%20**NVLink%20dominates**%20-%20900%20GB%2Fs%20is%20~18x%20faster%20than%20cross-node%20RDMA.%20Same-node%20GPU%E2%86%94GPU%0A%20%20%20%20%20%20%20communication%20is%20nearly%20free%20compared%20to%20crossing%20the%20network.%0A%0A%20%20%20%202.%20**RDMA%20%3E%3E%20Ethernet**%20-%20InfiniBand%2FRoCE%20at%2050%20GB%2Fs%20is%20~4x%20faster%20than%20100GbE%20(12.5%20GB%2Fs)%2C%0A%20%20%20%20%20%20%20plus%20kernel%20bypass%20and%20lower%20latency.%20Worth%20the%20complexity%20for%20HPC%20workloads.%0A%0A%20%20%20%203.%20**PCIe%20is%20faster%20than%20you'd%20think**%20-%20At%2064%20GB%2Fs%2C%20CPU%E2%86%94GPU%20transfers%20aren't%20the%20bottleneck%0A%20%20%20%20%20%20%20people%20often%20assume.%20The%20real%20cost%20is%20synchronization%2C%20not%20bandwidth.%0A%0A%20%20%20%20**Rule%20of%20thumb**%3A%20Place%20the%20most%20bandwidth-intensive%2C%20frequent%20operations%20on%20NVLink%0A%20%20%20%20(gradients%2C%20activations).%20Use%20RDMA%20for%20cross-node%20communication%20(weight%20sync%2C%20sharding).%0A%20%20%20%20PCIe%20is%20fine%20for%20occasional%20CPU%E2%86%94GPU%20transfers.%0A%0A%20%20%20%20We'll%20focus%20primarily%20on%20**NVLink**%20and%20**RDMA**%20for%20this%20notebook.%20Most%20people%20use%20these%0A%20%20%20%20via%20**collectives**%2C%20exposed%20through%20PyTorch%20distributed%3A%0A%0A%20%20%20%20%60%60%60python%0A%20%20%20%20import%20torch.distributed%20as%20dist%0A%0A%20%20%20%20%23%20Initialize%20process%20group%20-%20NCCL%20uses%20NVLink%20(same-node)%20and%20RDMA%20(cross-node)%0A%20%20%20%20dist.init_process_group(backend%3D%22nccl%22)%0A%0A%20%20%20%20%23%20All-reduce%3A%20average%20gradients%20across%20all%20GPUs%0A%20%20%20%20dist.all_reduce(gradients%2C%20op%3Ddist.ReduceOp.SUM)%0A%20%20%20%20gradients%20%2F%3D%20world_size%0A%0A%20%20%20%20%23%20All-gather%3A%20collect%20tensors%20from%20all%20ranks%0A%20%20%20%20gathered%20%3D%20%5Btorch.empty_like(tensor)%20for%20_%20in%20range(world_size)%5D%0A%20%20%20%20dist.all_gather(gathered%2C%20tensor)%0A%0A%20%20%20%20%23%20Broadcast%3A%20send%20from%20rank%200%20to%20all%20others%0A%20%20%20%20dist.broadcast(weights%2C%20src%3D0)%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20This%20works%20great%20for%20training.%20But%20for%20RL%20weight%20sync%2C%20we%20need%20something%20different...%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20import%20torch%0A%20%20%20%20return%20(torch%2C)%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%202.%20The%20Problem%3A%20Collectives%20Are%20Blocking%0A%0A%20%20%20%20Collectives%20work%20great%20for%20training%20-%20everyone%20computes%20gradients%2C%20then%20synchronizes.%0A%20%20%20%20But%20async%20RL%20has%20a%20different%20access%20pattern.%0A%0A%20%20%20%20%23%23%23%20High%20Variance%20in%20Generation%20Times%0A%0A%20%20%20%20Generators%20have%20wildly%20different%20completion%20times%3A%0A%20%20%20%20-%20Some%20prompts%20%E2%86%92%2010%20tokens%20(fast)%0A%20%20%20%20-%20Other%20prompts%20%E2%86%92%201000%20tokens%20(slow)%0A%0A%20%20%20%20With%20collectives%2C%20fast%20generators%20wait%20for%20slow%20ones%3A%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20Generator%200%3A%20%E2%94%9C%E2%94%80%E2%94%80%20gen%20(fast)%20%E2%94%80%E2%94%80%E2%94%A4%20%20%E2%9A%A0%EF%B8%8F%20WAITING...%0A%20%20%20%20Generator%201%3A%20%E2%94%9C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%20gen%20(slow)%20%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%A4%0A%20%20%20%20Generator%202%3A%20%E2%94%9C%E2%94%80%E2%94%80%20gen%20(fast)%20%E2%94%80%E2%94%80%E2%94%A4%20%20%E2%9A%A0%EF%B8%8F%20WAITING...%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%86%93%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20all_gather(weights)%20%20%23%20Everyone%20waits!%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20%23%23%23%20What%20About%20send%2Frecv%3F%0A%0A%20%20%20%20PyTorch%20distributed%20does%20have%20point-to-point%20primitives%3A%0A%0A%20%20%20%20%60%60%60python%0A%20%20%20%20%23%20Sender%20side%0A%20%20%20%20dist.send(tensor%2C%20dst%3Dreceiver_rank)%0A%0A%20%20%20%20%23%20Receiver%20side%0A%20%20%20%20dist.recv(tensor%2C%20src%3Dsender_rank)%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20But%20this%20is%20**two-sided**%20-%20both%20sender%20and%20receiver%20must%20coordinate%3A%0A%20%20%20%20-%20Receiver%20must%20call%20%60recv()%60%20before%20sender's%20%60send()%60%20completes%0A%20%20%20%20-%20Trainer%20would%20need%20to%20wait%20until%20generators%20are%20ready%20to%20receive%0A%20%20%20%20-%20Still%20blocking%20on%20coordination!%0A%0A%20%20%20%20%23%23%23%20The%20One-Sided%20Solution%3A%20RDMA%0A%0A%20%20%20%20What%20if%20the%20sender%20could%20write%20directly%20to%20the%20receiver's%20memory%20without%20coordination%3F%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20Two-sided%20(send%2Frecv)%3A%0A%20%20%20%20%20%20Sender%3A%20%22I%20have%20data%22%20%20%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%96%BA%20%20Receiver%3A%20%22I'm%20ready%22%0A%20%20%20%20%20%20Sender%3A%20sends%20data%20%20%20%20%20%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%96%BA%20%20Receiver%3A%20receives%20data%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%202%20messages%20required%0A%0A%20%20%20%20One-sided%20(RDMA)%3A%0A%20%20%20%20%20%20Sender%3A%20writes%20directly%20to%20receiver's%20memory%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20No%20coordination%20needed!%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20This%20is%20what%20RDMA%20enables%3A%20**one-sided%20memory%20operations**.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%203.%20How%20RDMA%20Works%20(ibverbs)%0A%0A%20%20%20%20RDMA%20(Remote%20Direct%20Memory%20Access)%20lets%20one%20machine%20read%2Fwrite%20another%20machine's%20memory%0A%20%20%20%20directly%2C%20bypassing%20the%20kernel%20and%20CPU%20on%20both%20sides.%0A%0A%20%20%20%20%23%23%23%20The%20ibverbs%20Stack%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20%20Application%20(PyTorch%2C%20Monarch%2C%20etc.)%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%9C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%A4%0A%20%20%20%20%E2%94%82%20%20libibverbs%20%20(userspace%20RDMA%20API)%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%9C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%A4%0A%20%20%20%20%E2%94%82%20%20Provider%20driver%20(mlx5%2C%20efa%2C%20rxe%2C%20etc.)%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%9C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%A4%0A%20%20%20%20%E2%94%82%20%20Hardware%20(InfiniBand%20NIC%2C%20RoCE%20NIC%2C%20etc.)%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20We'll%20focus%20on%20**InfiniBand**%20and%20**RoCE**%20(RDMA%20over%20Converged%20Ethernet).%0A%20%20%20%20Other%20transports%20like%20AWS%20EFA%20exist%20but%20we%20won't%20cover%20them%20here.%0A%0A%20%20%20%20%23%23%23%20Key%20RDMA%20Operations%0A%0A%20%20%20%20%7C%20Operation%20%7C%20Description%20%7C%0A%20%20%20%20%7C-----------%7C-------------%7C%0A%20%20%20%20%7C%20%60RDMA_WRITE%60%20%7C%20Write%20to%20remote%20memory%20(one-sided)%20%7C%0A%20%20%20%20%7C%20%60RDMA_READ%60%20%7C%20Read%20from%20remote%20memory%20(one-sided)%20%7C%0A%20%20%20%20%7C%20%60SEND%2FRECV%60%20%7C%20Two-sided%20messaging%20(like%20TCP)%20%7C%0A%0A%20%20%20%20The%20magic%20is%20in%20%60RDMA_WRITE%60%20and%20%60RDMA_READ%60%20-%20they're%20**one-sided**%3A%0A%20%20%20%20-%20Remote%20CPU%20is%20not%20involved%0A%20%20%20%20-%20Remote%20application%20doesn't%20need%20to%20call%20anything%0A%20%20%20%20-%20NIC%20handles%20everything%20in%20hardware%0A%0A%20%20%20%20%23%23%23%20Memory%20Registration%0A%0A%20%20%20%20Before%20RDMA%2C%20memory%20must%20be%20**registered**%20with%20the%20NIC%3A%0A%0A%20%20%20%20%60%60%60python%0A%20%20%20%20%23%20Conceptually%20(actual%20ibverbs%20API%20is%20in%20C)%0A%20%20%20%20mr%20%3D%20rdma_register_memory(buffer%2C%20size)%0A%20%20%20%20%23%20Returns%3A%0A%20%20%20%20%23%20%20%20-%20lkey%3A%20local%20access%20key%20(for%20local%20operations)%0A%20%20%20%20%23%20%20%20-%20rkey%3A%20remote%20access%20key%20(share%20with%20remote%20peer)%0A%20%20%20%20%23%20%20%20-%20addr%3A%20physical%2Fvirtual%20address%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20The%20%60(addr%2C%20rkey)%60%20pair%20is%20a%20**remote-accessible%20pointer**.%20Share%20it%20with%20a%20peer%2C%0A%20%20%20%20and%20they%20can%20read%2Fwrite%20your%20memory%20directly.%0A%0A%20%20%20%20%23%23%23%20Queue%20Pair%20Setup%0A%0A%20%20%20%20Before%20any%20RDMA%20operations%2C%20you%20need%20to%20establish%20a%20**Queue%20Pair%20(QP)**%20between%0A%20%20%20%20sender%20and%20receiver.%20This%20is%20a%20one-time%20connection%20setup%3A%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20%20%20Sender%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20Receiver%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20Create%20QP%20%20%E2%94%82%20%E2%94%80%E2%94%80%E2%94%80%20exchange%20QP%20info%20%E2%94%80%E2%94%80%E2%94%80%E2%96%BA%20%E2%94%82%20%20Create%20QP%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20(qp_num%2C%20%20%20%E2%94%82%20%E2%97%84%E2%94%80%E2%94%80%20(qp_num%2C%20lid%2C%20gid)%20%E2%94%80%E2%94%80%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20lid%2C%20gid)%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20Move%20QP%20to%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20Move%20QP%20to%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20RTR%20%E2%86%92%20RTS%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20RTR%20%E2%86%92%20RTS%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20Now%20ready%20%20%E2%94%82%20%E2%95%90%E2%95%90%E2%95%90%20RDMA%20operations%20%E2%95%90%E2%95%90%E2%95%90%E2%95%90%E2%96%BA%20%E2%94%82%20%20Now%20ready%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20for%20RDMA!%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20for%20RDMA!%20%20%E2%94%82%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20This%20is%20where%20**Monarch%20actors**%20shine.%20Because%20you%20can%20spawn%20arbitrary%20actors%2C%0A%20%20%20%20you%20can%20create%20**RDMA%20Manager%20actors**%20that%3A%0A%20%20%20%20-%20Initialize%20QPs%20on%20their%20respective%20hosts%0A%20%20%20%20-%20Exchange%20QP%20info%20via%20actor%20messages%0A%20%20%20%20-%20Manage%20the%20connection%20lifecycle%0A%0A%20%20%20%20%60%60%60python%0A%20%20%20%20%23%20Monarch%20pattern%3A%20RDMA%20managers%20as%20actors%0A%20%20%20%20class%20RDMAManager(Actor)%3A%0A%20%20%20%20%20%20%20%20def%20__init__(self)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.qp%20%3D%20create_queue_pair()%0A%20%20%20%20%20%20%20%20%20%20%20%20self.qp_info%20%3D%20get_qp_info(self.qp)%20%20%23%20(qp_num%2C%20lid%2C%20gid)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20get_qp_info(self)%20-%3E%20QpInfo%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20self.qp_info%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20connect(self%2C%20remote_qp_info%3A%20QpInfo)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Transition%20QP%3A%20INIT%20%E2%86%92%20RTR%20%E2%86%92%20RTS%0A%20%20%20%20%20%20%20%20%20%20%20%20connect_qp(self.qp%2C%20remote_qp_info)%0A%0A%20%20%20%20%23%20Setup%3A%20exchange%20QP%20info%20via%20actor%20messages%2C%20then%20RDMA%20is%20ready%0A%20%20%20%20trainer_info%20%3D%20trainer_rdma.get_qp_info.call_one().get()%0A%20%20%20%20generator_rdma.connect.call_one(trainer_info).get()%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20The%20actor%20abstraction%20makes%20RDMA%20connection%20management%20natural%20and%20composable.%0A%0A%20%20%20%20%23%23%23%20Monarch%20Using%20Monarch%3A%20RdmaController%0A%0A%20%20%20%20Here's%20the%20cool%20part%3A%20**Monarch%20uses%20itself**%20to%20manage%20RDMA%20infrastructure.%20Looking%20at%0A%20%20%20%20the%20actual%20Python%20code%20in%20%60monarch%2F_src%2Frdma%2Frdma.py%60%3A%0A%0A%20%20%20%20%60%60%60python%0A%20%20%20%20%23%20From%20Monarch's%20RDMA%20implementation%0A%20%20%20%20from%20monarch._src.actor.proc_mesh%20import%20get_or_spawn_controller%0A%0A%20%20%20%20class%20RdmaController(Actor)%3A%0A%20%20%20%20%20%20%20%20'''Singleton%20controller%20that%20coordinates%20RDMA%20initialization.'''%0A%0A%20%20%20%20%20%20%20%20def%20__init__(self)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Track%20which%20proc%20meshes%20have%20RDMA%20initialized%0A%20%20%20%20%20%20%20%20%20%20%20%20self._manager_futures%3A%20dict%5BProcMesh%2C%20Future%5BRdmaManager%5D%5D%20%3D%20%7B%7D%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20async%20def%20init_rdma_on_mesh(self%2C%20proc_mesh%3A%20ProcMesh)%20-%3E%20None%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20'''Lazily%20initialize%20RDMA%20on%20a%20proc%20mesh.'''%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20proc_mesh%20not%20in%20self._manager_futures%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self._manager_futures%5Bproc_mesh%5D%20%3D%20Future(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20coro%3DRdmaManager.create(proc_mesh)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20await%20self._manager_futures%5Bproc_mesh%5D%0A%0A%20%20%20%20%23%20Cached%20initialization%20-%20only%20runs%20once%20per%20process%0A%20%20%20%20%40functools.cache%0A%20%20%20%20def%20_ensure_init_rdma_manager()%3A%0A%20%20%20%20%20%20%20%20async%20def%20task()%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20controller%20%3D%20await%20get_or_spawn_controller(%22rdma_controller%22%2C%20RdmaController)%0A%20%20%20%20%20%20%20%20%20%20%20%20await%20controller.init_rdma_on_mesh.call_one(current_proc_mesh())%0A%20%20%20%20%20%20%20%20return%20spawn_task(task())%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20This%20is%20**Monarch%20building%20Monarch**%20-%20the%20RDMA%20subsystem%20uses%20the%20same%20patterns%3A%0A%0A%20%20%20%20-%20%60get_or_spawn_controller(%22rdma_controller%22%2C%20RdmaController)%60%20ensures%20one%20global%20controller%0A%20%20%20%20-%20The%20controller%20lazily%20initializes%20RDMA%20managers%20per%20proc%20mesh%0A%20%20%20%20-%20%60%40functools.cache%60%20ensures%20we%20only%20bootstrap%20once%20per%20process%0A%20%20%20%20-%20Under%20the%20hood%2C%20the%20actual%20RDMA%20operations%20are%20in%20Rust%20(%60RdmaManagerActor%60)%0A%0A%20%20%20%20It's%20actors%20all%20the%20way%20down.%0A%0A%20%20%20%20%23%23%23%20Why%20This%20Matters%20for%20Weight%20Sync%0A%0A%20%20%20%20Remember%3A%20CPU%20memory%20AND%20GPU%20memory%20can%20both%20be%20registered%20for%20RDMA.%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20Trainer%3A%0A%20%20%20%20%20%201.%20Register%20weight%20buffer%20with%20RDMA%20NIC%0A%20%20%20%20%20%202.%20Get%20(addr%2C%20rkey)%20handle%0A%20%20%20%20%20%203.%20Share%20handle%20with%20generators%20(tiny%20message)%0A%0A%20%20%20%20Generator%3A%0A%20%20%20%20%20%201.%20Receive%20handle%0A%20%20%20%20%20%202.%20RDMA_READ%20directly%20from%20trainer's%20memory%0A%20%20%20%20%20%203.%20No%20coordination%20with%20trainer%20needed!%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20The%20trainer%20doesn't%20even%20know%20when%20generators%20pull%20weights.%20True%20one-sided.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%204.%20The%20Magic%20Pointer%20Pattern%0A%0A%20%20%20%20Now%20here's%20the%20key%20insight%20from%20our%20RDMA%20discussion%3A%20to%20represent%20remote%20data%2C%0A%20%20%20%20we%20only%20need%20a%20**tiny%20handle**%20-%20the%20%60(addr%2C%20rkey%2C%20size)%60%20tuple.%0A%0A%20%20%20%20Monarch%20wraps%20this%20in%20%60RDMABuffer%60.%20Let's%20see%20how%20small%20it%20actually%20is%3A%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20%23%20Central%20imports%20for%20all%20RDMA%20examples%20in%20this%20notebook%0A%20%20%20%20import%20time%0A%20%20%20%20from%20monarch.actor%20import%20Actor%2C%20endpoint%2C%20this_host%2C%20current_rank%0A%20%20%20%20from%20monarch.rdma%20import%20RDMABuffer%2C%20RDMAAction%2C%20is_rdma_available%0A%20%20%20%20return%20(%0A%20%20%20%20%20%20%20%20Actor%2C%0A%20%20%20%20%20%20%20%20RDMAAction%2C%0A%20%20%20%20%20%20%20%20RDMABuffer%2C%0A%20%20%20%20%20%20%20%20current_rank%2C%0A%20%20%20%20%20%20%20%20endpoint%2C%0A%20%20%20%20%20%20%20%20is_rdma_available%2C%0A%20%20%20%20%20%20%20%20this_host%2C%0A%20%20%20%20%20%20%20%20time%2C%0A%20%20%20%20)%0A%0A%0A%40app.cell%0Adef%20_(Actor%2C%20RDMABuffer%2C%20endpoint%2C%20is_rdma_available%2C%20this_host%2C%20torch)%3A%0A%20%20%20%20%23%20Measure%20actual%20size%20of%20RDMABuffer%20handles%0A%20%20%20%20import%20pickle%0A%0A%20%20%20%20def%20show_fallback()%3A%0A%20%20%20%20%20%20%20%20%22%22%22Fallback%3A%20show%20expected%20sizes%20based%20on%20RDMABuffer%20structure.%22%22%22%0A%20%20%20%20%20%20%20%20print(%22(RDMA%20not%20available%20-%20showing%20expected%20handle%20sizes)%5Cn%22)%0A%20%20%20%20%20%20%20%20print(%22RDMABuffer%20contains%3A%20addr%20(8B)%20%2B%20rkey%20(4B)%20%2B%20size%20(8B)%20%2B%20owner%20(~100B)%22)%0A%20%20%20%20%20%20%20%20print(%22Total%20serialized%20size%3A%20~150-200%20bytes%20regardless%20of%20tensor%20size%5Cn%22)%0A%0A%20%20%20%20%20%20%20%20sizes%20%3D%20%5B(%221%20KB%22%2C%201024)%2C%20(%221%20MB%22%2C%201024**2)%2C%20(%221%20GB%22%2C%201024**3)%5D%0A%20%20%20%20%20%20%20%20handle_bytes%20%3D%20150%20%20%23%20approximate%0A%0A%20%20%20%20%20%20%20%20for%20name%2C%20tensor_bytes%20in%20sizes%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20ratio%20%3D%20tensor_bytes%20%2F%20handle_bytes%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%7Bname%3A%3C8%7D%20tensor%20%E2%86%92%20~150%20byte%20handle%20%E2%86%92%20%7Bratio%3A%2C.0f%7Dx%20compression%22)%0A%0A%20%20%20%20%20%20%20%20print(%22%5Cn%E2%86%92%20Handle%20size%20is%20O(1)%20regardless%20of%20tensor%20size!%22)%0A%0A%20%20%20%20try%3A%0A%20%20%20%20%20%20%20%20if%20not%20is_rdma_available()%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20show_fallback()%0A%20%20%20%20%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20class%20BufferSizeDemo(Actor)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Actor%20that%20creates%20RDMABuffers%20and%20measures%20their%20size.%22%22%22%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20def%20measure_buffer_sizes(self)%20-%3E%20list%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20import%20pickle%20as%20_pickle%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20results%20%3D%20%5B%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20sizes%20%3D%20%5B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(%221%20KB%22%2C%20256)%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(%221%20MB%22%2C%20256%20*%201024)%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(%2210%20MB%22%2C%20256%20*%201024%20*%2010)%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5D%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20for%20name%2C%20numel%20in%20sizes%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20tensor%20%3D%20torch.randn(numel)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20tensor_bytes%20%3D%20tensor.numel()%20*%20tensor.element_size()%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20byte_tensor%20%3D%20tensor.view(torch.uint8).flatten()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20buffer%20%3D%20RDMABuffer(byte_tensor)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20handle_bytes%20%3D%20len(_pickle.dumps(buffer))%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20results.append((name%2C%20tensor_bytes%2C%20handle_bytes))%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20results%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20host%20%3D%20this_host()%0A%20%20%20%20%20%20%20%20%20%20%20%20proc%20%3D%20host.spawn_procs(%7B%22procs%22%3A%201%7D)%0A%20%20%20%20%20%20%20%20%20%20%20%20demo%20%3D%20proc.spawn(%22buffer_demo%22%2C%20BufferSizeDemo)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20results%20%3D%20demo.measure_buffer_sizes.call_one().get()%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20print(%22RDMABuffer%20handle%20size%20vs%20actual%20tensor%20size%3A%5Cn%22)%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%7B'Tensor%20Size'%3A%3C12%7D%20%7B'Actual%20Bytes'%3A%3C15%7D%20%7B'Handle%20Size'%3A%3C15%7D%20%7B'Ratio'%3A%3C10%7D%22)%0A%20%20%20%20%20%20%20%20%20%20%20%20print(%22-%22%20*%2055)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20name%2C%20tensor_bytes%2C%20handle_bytes%20in%20results%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20ratio%20%3D%20tensor_bytes%20%2F%20handle_bytes%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%7Bname%3A%3C12%7D%20%7Btensor_bytes%3A%3E12%2C%7D%20B%20%20%20%7Bhandle_bytes%3A%3E6%7D%20B%20%20%20%20%20%20%20%20%7Bratio%3A%3E8%2C.0f%7Dx%22)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20print(%22%5Cn%E2%86%92%20Handle%20size%20is%20O(1)%20regardless%20of%20tensor%20size!%22)%0A%0A%20%20%20%20except%20Exception%20as%20e%3A%0A%20%20%20%20%20%20%20%20print(f%22(RDMA%20setup%20failed%3A%20%7Be%7D)%5Cn%22)%0A%20%20%20%20%20%20%20%20show_fallback()%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%20The%20Magic%20Pointer%0A%0A%20%20%20%20This%20is%20the%20core%20pattern%3A%20**separate%20control%20plane%20from%20data%20plane**.%0A%0A%20%20%20%20-%20**Control%20plane**%20(actor%20messages)%3A%20Send%20tiny%20handle%20(~100%20bytes)%0A%20%20%20%20-%20**Data%20plane**%20(RDMA)%3A%20Bulk%20transfer%20of%20actual%20data%20(~10%20GB)%0A%0A%20%20%20%20Think%20of%20%60RDMABuffer%60%20as%20a%20**magic%20pointer**%20-%20it's%20a%20pointer%20that%20works%20across%20machines%3A%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20Trainer%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20Generator%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20weights%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20local%20copy%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20(10%20GB)%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20(empty)%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%AC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%AC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%201.%20Create%20RDMABuffer%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20(register%20memory%2C%20get%20handle)%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%E2%94%9C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%202.%20Send%20handle%20%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%96%BA%E2%94%82%20%20(~100%20bytes%20via%20actor)%0A%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%E2%97%84%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%203.%20RDMA%20read%20%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%A4%20%20(~10%20GB%20via%20hardware)%0A%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20(no%20trainer%20involvement!)%20%20%E2%94%82%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20The%20trainer%20doesn't%20even%20know%20when%20generators%20pull%20weights.%20True%20one-sided.%0A%0A%20%20%20%20%23%23%23%20RDMABuffer%20in%20Action%0A%0A%20%20%20%20From%20%60monarch.rdma%60%3A%0A%0A%20%20%20%20%60%60%60python%0A%20%20%20%20from%20monarch.rdma%20import%20RDMABuffer%0A%0A%20%20%20%20%23%20Trainer%20side%3A%20register%20weights%0A%20%20%20%20weights%20%3D%20torch.randn(1024%2C%201024%2C%20device%3D%22cuda%22)%0A%20%20%20%20buffer%20%3D%20RDMABuffer(weights.view(torch.uint8).flatten())%0A%0A%20%20%20%20%23%20Return%20buffer%20as%20part%20of%20an%20endpoint%20response%0A%20%20%20%20%23%20This%20is%20a%20TINY%20message%20-%20just%20the%20handle!%0A%20%20%20%20%40endpoint%0A%20%20%20%20def%20get_weight_handle(self)%20-%3E%20RDMABuffer%3A%0A%20%20%20%20%20%20%20%20return%20self.buffer%0A%0A%20%20%20%20%23%20Generator%20side%3A%20receive%20handle%2C%20pull%20directly%20into%20GPU%0A%20%20%20%20handle%20%3D%20trainer.get_weight_handle.call_one().get()%20%20%23%20Tiny%20message%0A%20%20%20%20gpu_weights%20%3D%20model.weights.view(torch.uint8).flatten()%0A%20%20%20%20handle.read_into(gpu_weights).get()%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Bulk%20RDMA%20%E2%86%92%20GPU%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20See%20the%20%5BGRPO%20Actor%20example%5D(https%3A%2F%2Fmeta-pytorch.org%2Fmonarch%2Fgenerated%2Fexamples%2Fgrpo_actor.html)%0A%20%20%20%20for%20a%20minimal%20implementation%20showing%20RDMA%20data%20flow.%20We'll%20build%20a%20more%20complete%0A%20%20%20%20version%20in%20the%20following%20sections.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%20The%20Cost%20of%20Memory%20Registration%0A%0A%20%20%20%20RDMA%20memory%20registration%20is%20**expensive**%3A%0A%20%20%20%20-%20Pins%20physical%20pages%20(prevents%20swapping)%0A%20%20%20%20-%20Creates%20IOMMU%2FDMA%20mappings%20in%20the%20NIC%0A%20%20%20%20-%20Can%20take%20milliseconds%20for%20large%20buffers%0A%0A%20%20%20%20**Don't%20register%20on%20the%20hot%20path!**%20Here%20are%20three%20approaches%3A%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%23%20Approach%201%3A%20Naive%20(Bad)%0A%0A%20%20%20%20Re-register%20every%20transfer%20-%20pays%20MR%20cost%20every%20time%3A%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(Actor%2C%20RDMABuffer%2C%20current_rank%2C%20endpoint%2C%20time%2C%20torch)%3A%0A%20%20%20%20class%20NaiveSender(Actor)%3A%0A%20%20%20%20%20%20%20%20%22%22%22Creates%20new%20RDMABuffer%20handles%20every%20transfer.%20Expensive!%22%22%22%0A%0A%20%20%20%20%20%20%20%20def%20__init__(self%2C%20layer_sizes%3A%20list)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.layer_sizes%20%3D%20layer_sizes%0A%20%20%20%20%20%20%20%20%20%20%20%20self.layers%20%3D%20%5Btorch.zeros(size%2C%20dtype%3Dtorch.float32)%20for%20size%20in%20layer_sizes%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20i%2C%20layer%20in%20enumerate(self.layers)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20layer.fill_(float(i%20%2B%201))%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20get_fresh_handles(self)%20-%3E%20list%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20BAD%3A%20New%20registration%20every%20call!%0A%20%20%20%20%20%20%20%20%20%20%20%20handles%20%3D%20%5B%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20size%2C%20layer%20in%20zip(self.layer_sizes%2C%20self.layers)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20byte_view%20%3D%20layer.view(torch.uint8).flatten()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20handles.append((size%2C%20RDMABuffer(byte_view)))%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20handles%0A%0A%20%20%20%20class%20NaiveReceiver(Actor)%3A%0A%20%20%20%20%20%20%20%20%22%22%22Receives%20from%20naive%20sender%20-%20pays%20MR%20cost%20every%20step.%22%22%22%0A%0A%20%20%20%20%20%20%20%20def%20__init__(self%2C%20layer_sizes%3A%20list)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.layer_sizes%20%3D%20layer_sizes%0A%20%20%20%20%20%20%20%20%20%20%20%20self.layers%20%3D%20%5Btorch.zeros(size%2C%20dtype%3Dtorch.float32)%20for%20size%20in%20layer_sizes%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20self.rank%20%3D%20current_rank().rank%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20receive_step(self%2C%20sender%3A%20NaiveSender)%20-%3E%20dict%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20start%20%3D%20time.perf_counter()%0A%20%20%20%20%20%20%20%20%20%20%20%20handles%20%3D%20sender.get_fresh_handles.call_one().get()%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20i%2C%20(size%2C%20handle)%20in%20enumerate(handles)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20byte_view%20%3D%20self.layers%5Bi%5D.view(torch.uint8).flatten()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20handle.read_into(byte_view).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20elapsed_ms%20%3D%20(time.perf_counter()%20-%20start)%20*%201000%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20%7B%22elapsed_ms%22%3A%20elapsed_ms%7D%0A%0A%20%20%20%20print(%22NaiveSender%3A%20Re-registers%20all%20parameters%20on%20every%20call%22)%0A%20%20%20%20return%20NaiveReceiver%2C%20NaiveSender%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%23%20Approach%202%3A%20Contiguous%20Buffer%20(Good)%0A%0A%20%20%20%20Allocate%20one%20buffer%2C%20register%20once%2C%20pack%20params%20into%20it%3A%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(Actor%2C%20RDMABuffer%2C%20current_rank%2C%20endpoint%2C%20time%2C%20torch)%3A%0A%20%20%20%20class%20ContiguousSender(Actor)%3A%0A%20%20%20%20%20%20%20%20%22%22%22One%20buffer%2C%20one%20MR%2C%20registered%20at%20startup.%22%22%22%0A%0A%20%20%20%20%20%20%20%20def%20__init__(self%2C%20layer_sizes%3A%20list)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.layer_sizes%20%3D%20layer_sizes%0A%20%20%20%20%20%20%20%20%20%20%20%20total_size%20%3D%20sum(layer_sizes)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20One%20contiguous%20buffer%0A%20%20%20%20%20%20%20%20%20%20%20%20self.buffer%20%3D%20torch.zeros(total_size%2C%20dtype%3Dtorch.float32)%0A%20%20%20%20%20%20%20%20%20%20%20%20offset%20%3D%200%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20i%2C%20size%20in%20enumerate(layer_sizes)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.buffer%5Boffset%20%3A%20offset%20%2B%20size%5D.fill_(float(i%20%2B%201))%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20offset%20%2B%3D%20size%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Register%20ONCE%20at%20startup%0A%20%20%20%20%20%20%20%20%20%20%20%20byte_view%20%3D%20self.buffer.view(torch.uint8).flatten()%0A%20%20%20%20%20%20%20%20%20%20%20%20self.handle%20%3D%20RDMABuffer(byte_view)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20get_handle(self)%20-%3E%20tuple%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20(len(self.buffer)%2C%20self.handle)%20%20%23%20Same%20handle%20every%20time!%0A%0A%20%20%20%20class%20ContiguousReceiver(Actor)%3A%0A%20%20%20%20%20%20%20%20%22%22%22Receives%20from%20contiguous%20sender%20-%20fast%20after%20first%20step.%22%22%22%0A%0A%20%20%20%20%20%20%20%20def%20__init__(self%2C%20total_size%3A%20int)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.buffer%20%3D%20torch.zeros(total_size%2C%20dtype%3Dtorch.float32)%0A%20%20%20%20%20%20%20%20%20%20%20%20self.rank%20%3D%20current_rank().rank%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20receive_step(self%2C%20sender%3A%20ContiguousSender)%20-%3E%20dict%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20start%20%3D%20time.perf_counter()%0A%20%20%20%20%20%20%20%20%20%20%20%20size%2C%20handle%20%3D%20sender.get_handle.call_one().get()%0A%20%20%20%20%20%20%20%20%20%20%20%20byte_view%20%3D%20self.buffer.view(torch.uint8).flatten()%0A%20%20%20%20%20%20%20%20%20%20%20%20handle.read_into(byte_view).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20elapsed_ms%20%3D%20(time.perf_counter()%20-%20start)%20*%201000%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20%7B%22elapsed_ms%22%3A%20elapsed_ms%7D%0A%0A%20%20%20%20print(%22ContiguousSender%3A%20Registers%20once%2C%20reuses%20same%20handle%22)%0A%20%20%20%20return%20ContiguousReceiver%2C%20ContiguousSender%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%23%20Approach%203%3A%20Scattered%20%2B%20RDMAAction%20(Good)%0A%0A%20%20%20%20Register%20each%20parameter%20once%2C%20build%20RDMAAction%20transfer%20plan%20once%2C%20execute%20repeatedly%3A%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(Actor%2C%20RDMAAction%2C%20RDMABuffer%2C%20current_rank%2C%20endpoint%2C%20time%2C%20torch)%3A%0A%20%20%20%20class%20ScatteredSender(Actor)%3A%0A%20%20%20%20%20%20%20%20%22%22%22Multiple%20buffers%2C%20each%20registered%20once%20at%20startup.%22%22%22%0A%0A%20%20%20%20%20%20%20%20def%20__init__(self%2C%20layer_sizes%3A%20list)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.layer_sizes%20%3D%20layer_sizes%0A%20%20%20%20%20%20%20%20%20%20%20%20self.layers%20%3D%20%5B%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20self.handles%20%3D%20%5B%5D%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20i%2C%20size%20in%20enumerate(layer_sizes)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20layer%20%3D%20torch.zeros(size%2C%20dtype%3Dtorch.float32)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20layer.fill_(float(i%20%2B%201))%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.layers.append(layer)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Register%20ONCE%20at%20startup%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20byte_view%20%3D%20layer.view(torch.uint8).flatten()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.handles.append(RDMABuffer(byte_view))%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20get_handles(self)%20-%3E%20list%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20%5B(size%2C%20handle)%20for%20size%2C%20handle%20in%20zip(self.layer_sizes%2C%20self.handles)%5D%0A%0A%20%20%20%20class%20ScatteredReceiver(Actor)%3A%0A%20%20%20%20%20%20%20%20%22%22%22Receives%20from%20scattered%20sender%20with%20RDMAAction%20batching.%22%22%22%0A%0A%20%20%20%20%20%20%20%20def%20__init__(self%2C%20layer_sizes%3A%20list)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.layer_sizes%20%3D%20layer_sizes%0A%20%20%20%20%20%20%20%20%20%20%20%20self.layers%20%3D%20%5Btorch.zeros(size%2C%20dtype%3Dtorch.float32)%20for%20size%20in%20layer_sizes%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20self.rank%20%3D%20current_rank().rank%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20receive_step(self%2C%20sender%3A%20ScatteredSender)%20-%3E%20dict%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20start%20%3D%20time.perf_counter()%0A%20%20%20%20%20%20%20%20%20%20%20%20handles%20%3D%20sender.get_handles.call_one().get()%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Batch%20all%20transfers%20with%20RDMAAction%0A%20%20%20%20%20%20%20%20%20%20%20%20action%20%3D%20RDMAAction()%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20i%2C%20(size%2C%20handle)%20in%20enumerate(handles)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20byte_view%20%3D%20self.layers%5Bi%5D.view(torch.uint8).flatten()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20action.read_into(handle%2C%20byte_view)%0A%20%20%20%20%20%20%20%20%20%20%20%20action.submit().get()%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20elapsed_ms%20%3D%20(time.perf_counter()%20-%20start)%20*%201000%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20%7B%22elapsed_ms%22%3A%20elapsed_ms%7D%0A%0A%20%20%20%20print(%22ScatteredSender%3A%20Registers%20each%20layer%20once%22)%0A%20%20%20%20print(%22ScatteredReceiver%3A%20Uses%20RDMAAction%20to%20batch%20transfers%22)%0A%20%20%20%20return%20ScatteredReceiver%2C%20ScatteredSender%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20**Key%20insight**%3A%20Register%20in%20%60__init__%60%2C%20not%20in%20your%20transfer%20endpoint!%0A%0A%20%20%20%20For%20the%20scattered%20approach%2C%20RDMAAction%20is%20like%20a%20**transfer%20plan**%20-%20you%20build%20it%20once%0A%20%20%20%20with%20all%20the%20handles%2C%20then%20just%20call%20%60submit()%60%20whenever%20you%20need%20to%20sync.%0A%0A%20%20%20%20Let's%20benchmark%20to%20see%20the%20difference%3A%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(%0A%20%20%20%20ContiguousReceiver%2C%0A%20%20%20%20ContiguousSender%2C%0A%20%20%20%20NaiveReceiver%2C%0A%20%20%20%20NaiveSender%2C%0A%20%20%20%20ScatteredReceiver%2C%0A%20%20%20%20ScatteredSender%2C%0A%20%20%20%20this_host%2C%0A)%3A%0A%20%20%20%20def%20run_benchmark()%3A%0A%20%20%20%20%20%20%20%20%22%22%22Compare%20the%20three%20approaches%20over%20multiple%20steps.%22%22%22%0A%20%20%20%20%20%20%20%20layer_sizes%20%3D%20%5B1000%2C%205000%2C%202000%5D%20%20%23%208000%20floats%20total%0A%20%20%20%20%20%20%20%20total_size%20%3D%20sum(layer_sizes)%0A%20%20%20%20%20%20%20%20num_steps%20%3D%205%0A%0A%20%20%20%20%20%20%20%20host%20%3D%20this_host()%0A%20%20%20%20%20%20%20%20sender_procs%20%3D%20host.spawn_procs(%7B%22procs%22%3A%201%7D)%0A%20%20%20%20%20%20%20%20receiver_procs%20%3D%20host.spawn_procs(%7B%22procs%22%3A%201%7D)%0A%0A%20%20%20%20%20%20%20%20print(%22%3D%3D%3D%20RDMA%20Registration%20Benchmark%20%3D%3D%3D%22)%0A%20%20%20%20%20%20%20%20print(f%22Transferring%20%7Btotal_size%7D%20floats%20(%7Btotal_size%20*%204%20%2F%201024%3A.1f%7D%20KB)%20x%20%7Bnum_steps%7D%20steps%5Cn%22)%0A%0A%20%20%20%20%20%20%20%20results%20%3D%20%7B%7D%0A%0A%20%20%20%20%20%20%20%20%23%20Naive%20approach%0A%20%20%20%20%20%20%20%20naive_sender%20%3D%20sender_procs.spawn(%22naive_s%22%2C%20NaiveSender%2C%20layer_sizes)%0A%20%20%20%20%20%20%20%20naive_receiver%20%3D%20receiver_procs.spawn(%22naive_r%22%2C%20NaiveReceiver%2C%20layer_sizes)%0A%20%20%20%20%20%20%20%20times%20%3D%20%5B%5D%0A%20%20%20%20%20%20%20%20for%20step%20in%20range(num_steps)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20r%20%3D%20naive_receiver.receive_step.call_one(naive_sender).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20times.append(r%5B%22elapsed_ms%22%5D)%0A%20%20%20%20%20%20%20%20results%5B%22Naive%22%5D%20%3D%20times%0A%20%20%20%20%20%20%20%20print(f%22Naive%20(re-register%20each%20step)%3A%22)%0A%20%20%20%20%20%20%20%20for%20i%2C%20t%20in%20enumerate(times)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%20%20Step%20%7Bi%2B1%7D%3A%20%7Bt%3A.2f%7Dms%22)%0A%20%20%20%20%20%20%20%20print(f%22%20%20Average%3A%20%7Bsum(times)%2Flen(times)%3A.2f%7Dms%5Cn%22)%0A%0A%20%20%20%20%20%20%20%20%23%20Contiguous%20approach%0A%20%20%20%20%20%20%20%20cont_sender%20%3D%20sender_procs.spawn(%22cont_s%22%2C%20ContiguousSender%2C%20layer_sizes)%0A%20%20%20%20%20%20%20%20cont_receiver%20%3D%20receiver_procs.spawn(%22cont_r%22%2C%20ContiguousReceiver%2C%20total_size)%0A%20%20%20%20%20%20%20%20times%20%3D%20%5B%5D%0A%20%20%20%20%20%20%20%20for%20step%20in%20range(num_steps)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20r%20%3D%20cont_receiver.receive_step.call_one(cont_sender).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20times.append(r%5B%22elapsed_ms%22%5D)%0A%20%20%20%20%20%20%20%20results%5B%22Contiguous%22%5D%20%3D%20times%0A%20%20%20%20%20%20%20%20print(f%22Contiguous%20(register%20once)%3A%22)%0A%20%20%20%20%20%20%20%20for%20i%2C%20t%20in%20enumerate(times)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%20%20Step%20%7Bi%2B1%7D%3A%20%7Bt%3A.2f%7Dms%22)%0A%20%20%20%20%20%20%20%20print(f%22%20%20Average%3A%20%7Bsum(times)%2Flen(times)%3A.2f%7Dms%5Cn%22)%0A%0A%20%20%20%20%20%20%20%20%23%20Scattered%20%2B%20RDMAAction%20approach%0A%20%20%20%20%20%20%20%20scat_sender%20%3D%20sender_procs.spawn(%22scat_s%22%2C%20ScatteredSender%2C%20layer_sizes)%0A%20%20%20%20%20%20%20%20scat_receiver%20%3D%20receiver_procs.spawn(%22scat_r%22%2C%20ScatteredReceiver%2C%20layer_sizes)%0A%20%20%20%20%20%20%20%20times%20%3D%20%5B%5D%0A%20%20%20%20%20%20%20%20for%20step%20in%20range(num_steps)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20r%20%3D%20scat_receiver.receive_step.call_one(scat_sender).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20times.append(r%5B%22elapsed_ms%22%5D)%0A%20%20%20%20%20%20%20%20results%5B%22Scattered%22%5D%20%3D%20times%0A%20%20%20%20%20%20%20%20print(f%22Scattered%20%2B%20RDMAAction%20(register%20once%2C%20batch)%3A%22)%0A%20%20%20%20%20%20%20%20for%20i%2C%20t%20in%20enumerate(times)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%20%20Step%20%7Bi%2B1%7D%3A%20%7Bt%3A.2f%7Dms%22)%0A%20%20%20%20%20%20%20%20print(f%22%20%20Average%3A%20%7Bsum(times)%2Flen(times)%3A.2f%7Dms%5Cn%22)%0A%0A%20%20%20%20%20%20%20%20print(%22%3D%3D%3D%20Summary%20%3D%3D%3D%22)%0A%20%20%20%20%20%20%20%20print(%22Naive%3A%20Pays%20MR%20cost%20EVERY%20step%20(slow)%22)%0A%20%20%20%20%20%20%20%20print(%22Smart%3A%20Pays%20MR%20cost%20once%2C%20subsequent%20steps%20are%20fast%22)%0A%0A%20%20%20%20%20%20%20%20return%20results%0A%0A%20%20%20%20benchmark_results%20%3D%20run_benchmark()%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%20Two%20Weight%20Sync%20Patterns%0A%0A%20%20%20%20With%20RDMABuffer%20as%20our%20building%20block%2C%20there%20are%20two%20main%20approaches%3A%0A%0A%20%20%20%20%7C%20Pattern%20%7C%20How%20it%20works%20%7C%20Trade-offs%20%7C%0A%20%20%20%20%7C---------%7C--------------%7C------------%7C%0A%20%20%20%20%7C%20**CPU%20Staging**%20%7C%20GPU%20%E2%86%92%20CPU%20buffer%20%E2%86%92%20RDMA%20%E2%86%92%20CPU%20%E2%86%92%20GPU%20%7C%20One%20MR%2C%20simple%2C%20but%20copies%20%7C%0A%20%20%20%20%7C%20**Direct%20GPU**%20%7C%20GPU%20%E2%86%92%20RDMA%20%E2%86%92%20GPU%20(GPUDirect)%20%7C%20No%20copies%2C%20but%20one%20MR%20per%20param%20%7C%0A%0A%20%20%20%20**Pattern%201%3A%20CPU%20Staging%20(Contiguous%20Buffer)**%0A%0A%20%20%20%20Pack%20all%20parameters%20into%20one%20contiguous%20CPU%20buffer%2C%20register%20once%3A%0A%0A%20%20%20%20%60%60%60python%0A%20%20%20%20class%20Trainer(Actor)%3A%0A%20%20%20%20%20%20%20%20def%20__init__(self)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Calculate%20total%20size%20for%20all%20parameters%0A%20%20%20%20%20%20%20%20%20%20%20%20total_bytes%20%3D%20sum(p.numel()%20*%20p.element_size()%20for%20p%20in%20model.parameters())%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Allocate%20ONE%20contiguous%20buffer%2C%20register%20ONCE%0A%20%20%20%20%20%20%20%20%20%20%20%20self.staging_buffer%20%3D%20torch.empty(total_bytes%2C%20dtype%3Dtorch.uint8)%0A%20%20%20%20%20%20%20%20%20%20%20%20self.handle%20%3D%20RDMABuffer(self.staging_buffer)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Track%20where%20each%20param%20lives%20in%20the%20buffer%0A%20%20%20%20%20%20%20%20%20%20%20%20self.param_offsets%20%3D%20compute_offsets(model)%0A%0A%20%20%20%20%20%20%20%20def%20pack_weights(self)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20'''Copy%20all%20params%20into%20contiguous%20buffer.'''%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20name%2C%20param%20in%20model.named_parameters()%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20offset%20%3D%20self.param_offsets%5Bname%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.staging_buffer%5Boffset%3Aoffset%2Bsize%5D.copy_(param.view(torch.uint8))%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20get_weight_handle(self)%20-%3E%20RDMABuffer%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.pack_weights()%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20self.handle%20%20%23%20Same%20handle%2C%20new%20data%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20**Pattern%202%3A%20Direct%20GPU%20MRs**%0A%0A%20%20%20%20Register%20each%20GPU%20parameter%20directly%2C%20no%20CPU%20copies%3A%0A%0A%20%20%20%20%60%60%60python%0A%20%20%20%20class%20Trainer(Actor)%3A%0A%20%20%20%20%20%20%20%20def%20__init__(self)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Register%20each%20param%20ONCE%20at%20startup%0A%20%20%20%20%20%20%20%20%20%20%20%20self.handles%20%3D%20%7B%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20name%2C%20param%20in%20model.named_parameters()%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20byte_view%20%3D%20param.data.view(torch.uint8).flatten()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.handles%5Bname%5D%20%3D%20RDMABuffer(byte_view)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20get_param_handles(self)%20-%3E%20dict%5Bstr%2C%20RDMABuffer%5D%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Handles%20are%20reused%20-%20data%20updates%20in%20place%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20self.handles%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20Both%20patterns%20amortize%20MR%20registration%20cost%20across%20training%20iterations.%0A%20%20%20%20Let's%20look%20at%20CPU%20staging%20in%20more%20detail%20(it's%20more%20common%20in%20async%20RL).%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%205.%20CPU%20Staging%20Pattern%0A%0A%20%20%20%20%23%23%23%20GPU-Native%20RDMA%20Works!%0A%0A%20%20%20%20First%2C%20let's%20be%20clear%3A%20**GPU-native%20RDMA%20works**%20and%20is%20fast%3A%0A%20%20%20%20-%20GPUDirect%20RDMA%3A%20NIC%20reads%20directly%20from%20GPU%20memory%0A%20%20%20%20-%20No%20CPU%20copy%20needed%20(when%20hardware%20supports%20it)%0A%20%20%20%20-%20Great%20for%20synchronous%20transfers%0A%0A%20%20%20%20%23%23%23%20Why%20CPU%20Staging%20for%20Async%20RL%3F%0A%0A%20%20%20%20The%20issue%20isn't%20bandwidth%20-%20it's%20**timing**%3A%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20Direct%20GPU%E2%86%92GPU%20RDMA%3A%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20Generator%20GPU%20is%20mid-inference%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%E2%94%9C%E2%94%80%E2%94%80%20layer%201%20%E2%94%80%E2%94%80%E2%94%A4%20%5BRDMA%20arrives%2C%20needs%20sync!%5D%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%86%93%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20cudaDeviceSynchronize()%20%20%E2%86%90%20Blocks%20inference!%20%E2%94%82%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20With%20CPU%20staging%2C%20nothing%20on%20the%20critical%20path%20blocks%3A%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20Trainer%20GPU%20%E2%94%80%E2%94%80%E2%96%BA%20CPU%20staging%20buffer%20(RDMA%20registered)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%5BSits%20here%2C%20ready%20anytime%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%96%BC%0A%20%20%20%20Generator%20grabs%20when%20ready%20%E2%94%80%E2%94%80%E2%96%BA%20Generator%20GPU%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20The%20CPU%20buffer%20is%20a%20**temporal%20decoupling%20point**.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(torch)%3A%0A%20%20%20%20def%20demonstrate_cpu_staging()%3A%0A%20%20%20%20%20%20%20%20%22%22%22Demonstrate%20the%20CPU%20staging%20pattern.%22%22%22%0A%20%20%20%20%20%20%20%20if%20not%20torch.cuda.is_available()%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20print(%22CUDA%20not%20available%20-%20showing%20conceptual%20flow%22)%0A%20%20%20%20%20%20%20%20%20%20%20%20return%0A%0A%20%20%20%20%20%20%20%20%23%20Trainer%20side%3A%20GPU%20weights%20%E2%86%92%20CPU%20staging%20buffer%20(RDMA%20registered)%0A%20%20%20%20%20%20%20%20trainer_weights%20%3D%20torch.randn(1000%2C%201000%2C%20device%3D%22cuda%3A0%22)%0A%0A%20%20%20%20%20%20%20%20%23%20Pin%20memory%20for%20efficient%20transfers%20and%20RDMA%20registration%0A%20%20%20%20%20%20%20%20cpu_staging%20%3D%20torch.empty_like(trainer_weights%2C%20device%3D%22cpu%22).pin_memory()%0A%0A%20%20%20%20%20%20%20%20%23%20D2H%3A%20Trainer%20dumps%20to%20CPU%20(async%2C%20non-blocking%20for%20trainer)%0A%20%20%20%20%20%20%20%20cpu_staging.copy_(trainer_weights%2C%20non_blocking%3DTrue)%0A%20%20%20%20%20%20%20%20torch.cuda.synchronize()%20%20%23%20Just%20for%20timing%20demo%0A%0A%20%20%20%20%20%20%20%20print(%22Trainer%3A%20Weights%20copied%20to%20CPU%20staging%20buffer%20(RDMA%20registered)%22)%0A%20%20%20%20%20%20%20%20print(f%22%20%20GPU%20memory%3A%20%7Btrainer_weights.device%7D%22)%0A%20%20%20%20%20%20%20%20print(f%22%20%20CPU%20staging%3A%20pinned%3D%7Bcpu_staging.is_pinned()%7D%22)%0A%0A%20%20%20%20%20%20%20%20%23%20Generator%20side%3A%20RDMA%20pulls%20from%20trainer's%20CPU%20%E2%86%92%20directly%20to%20generator's%20GPU%0A%20%20%20%20%20%20%20%20%23%20(In%20this%20demo%20we%20simulate%20the%20RDMA%20transfer%20with%20a%20local%20copy)%0A%20%20%20%20%20%20%20%20generator_gpu_weights%20%3D%20torch.empty_like(cpu_staging%2C%20device%3D%22cuda%3A0%22)%0A%20%20%20%20%20%20%20%20generator_gpu_weights.copy_(cpu_staging%2C%20non_blocking%3DTrue)%20%20%23%20Simulates%20RDMA%20%E2%86%92%20GPU%0A%20%20%20%20%20%20%20%20torch.cuda.synchronize()%0A%0A%20%20%20%20%20%20%20%20print(%22Generator%3A%20Weights%20loaded%20directly%20to%20GPU%20(via%20RDMA)%22)%0A%20%20%20%20%20%20%20%20print(f%22%20%20Weights%20match%3A%20%7Btorch.allclose(trainer_weights%2C%20generator_gpu_weights)%7D%22)%0A%0A%20%20%20%20demonstrate_cpu_staging()%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%206.%20Circular%20Weight%20Buffers%0A%0A%20%20%20%20%23%23%23%20The%20Versioning%20Problem%0A%0A%20%20%20%20In%20async%20RL%2C%20trainer%20updates%20weights%20continuously.%20Generators%20need%20to%3A%0A%20%20%20%201.%20**Grab%20the%20latest**%20weights%20(not%20stale%20ones)%0A%20%20%20%202.%20**Not%20block**%20waiting%20for%20updates%0A%20%20%20%203.%20**Avoid%20memory%20churn**%20(re-registering%20RDMA%20buffers%20is%20expensive)%0A%0A%20%20%20%20%23%23%23%20Solution%3A%20Circular%20Buffer%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20Trainer%20writes%3A%20%20%20%20%20v0%20%E2%86%92%20v1%20%E2%86%92%20v2%20%E2%86%92%20v3%20%E2%86%92%20v4%20%E2%86%92%20v5%20%E2%86%92%20...%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%86%93%20%20%20%20%E2%86%93%20%20%20%20%E2%86%93%0A%20%20%20%20Buffer%20slots%3A%20%20%20%20%20%20%5Bslot0%5D%5Bslot1%5D%5Bslot2%5D%20%20(circular%2C%20reused)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20v3%20%20%20%20v4%20%20%20%20v5%0A%0A%20%20%20%20Generator%20reads%3A%20%22Give%20me%20latest%22%20%E2%86%92%20v5%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20Benefits%3A%0A%20%20%20%20-%20**Pre-registered%20RDMA%20buffers**%20-%20no%20memory%20registration%20on%20hot%20path%0A%20%20%20%20-%20**Lock-free%20reads**%20-%20generators%20always%20get%20a%20consistent%20snapshot%0A%20%20%20%20-%20**Bounded%20memory**%20-%20only%20N%20versions%20in%20flight%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(torch)%3A%0A%20%20%20%20from%20threading%20import%20Lock%20as%20_Lock%0A%20%20%20%20from%20typing%20import%20Tuple%20as%20_Tuple%2C%20Optional%20as%20_Opt%0A%0A%20%20%20%20class%20CircularWeightBuffer%3A%0A%20%20%20%20%20%20%20%20%22%22%22Circular%20buffer%20for%20versioned%20weight%20storage.%22%22%22%0A%0A%20%20%20%20%20%20%20%20def%20__init__(self%2C%20template_tensor%3A%20torch.Tensor%2C%20n_slots%3A%20int%20%3D%203)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.n_slots%20%3D%20n_slots%0A%20%20%20%20%20%20%20%20%20%20%20%20self.slots%20%3D%20%5B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20torch.empty_like(template_tensor).pin_memory()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20template_tensor.device.type%20%3D%3D%20%22cpu%22%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20else%20torch.empty_like(template_tensor%2C%20device%3D%22cpu%22).pin_memory()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20for%20_%20in%20range(n_slots)%0A%20%20%20%20%20%20%20%20%20%20%20%20%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20self.latest_version%20%3D%200%0A%20%20%20%20%20%20%20%20%20%20%20%20self._lock%20%3D%20_Lock()%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20In%20production%3A%20pre-register%20all%20slots%20with%20RDMA%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20self.rdma_handles%20%3D%20%5BRDMABuffer(slot)%20for%20slot%20in%20self.slots%5D%0A%0A%20%20%20%20%20%20%20%20def%20publish(self%2C%20weights%3A%20torch.Tensor)%20-%3E%20int%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Trainer%20publishes%20new%20weights.%20Returns%20version%20number.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20with%20self._lock%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20slot_idx%20%3D%20self.latest_version%20%25%20self.n_slots%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.slots%5Bslot_idx%5D.copy_(weights)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.latest_version%20%2B%3D%201%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20self.latest_version%20-%201%0A%0A%20%20%20%20%20%20%20%20def%20get_latest(self)%20-%3E%20_Tuple%5Btorch.Tensor%2C%20int%5D%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Generator%20gets%20latest%20weights.%20Non-blocking.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20with%20self._lock%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20self.latest_version%20%3D%3D%200%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20raise%20RuntimeError(%22No%20weights%20published%20yet%22)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20slot_idx%20%3D%20(self.latest_version%20-%201)%20%25%20self.n_slots%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20version%20%3D%20self.latest_version%20-%201%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Return%20a%20copy%20to%20avoid%20races%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20self.slots%5Bslot_idx%5D.clone()%2C%20version%0A%0A%20%20%20%20%20%20%20%20def%20get_version(self%2C%20version%3A%20int)%20-%3E%20_Opt%5Btorch.Tensor%5D%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Get%20specific%20version%20if%20still%20available.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20with%20self._lock%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20oldest_available%20%3D%20max(0%2C%20self.latest_version%20-%20self.n_slots)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20version%20%3C%20oldest_available%20or%20version%20%3E%3D%20self.latest_version%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20None%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20slot_idx%20%3D%20version%20%25%20self.n_slots%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20self.slots%5Bslot_idx%5D.clone()%0A%0A%20%20%20%20%23%20Demo%0A%20%20%20%20_template%20%3D%20torch.randn(100%2C%20100)%0A%20%20%20%20weight_buffer%20%3D%20CircularWeightBuffer(_template%2C%20n_slots%3D3)%0A%0A%20%20%20%20%23%20Trainer%20publishes%20versions%0A%20%20%20%20for%20_v%20in%20range(5)%3A%0A%20%20%20%20%20%20%20%20_new_weights%20%3D%20torch.randn(100%2C%20100)%20*%20(_v%20%2B%201)%0A%20%20%20%20%20%20%20%20published_v%20%3D%20weight_buffer.publish(_new_weights)%0A%20%20%20%20%20%20%20%20print(f%22Published%20version%20%7Bpublished_v%7D%22)%0A%0A%20%20%20%20%23%20Generator%20grabs%20latest%0A%20%20%20%20latest_weights%2C%20latest_version%20%3D%20weight_buffer.get_latest()%0A%20%20%20%20print(f%22%5CnGenerator%20got%20version%20%7Blatest_version%7D%2C%20weights%20mean%3A%20%7Blatest_weights.mean()%3A.2f%7D%22)%0A%0A%20%20%20%20%23%20Try%20to%20get%20old%20version%20(might%20be%20evicted)%0A%20%20%20%20old_weights%20%3D%20weight_buffer.get_version(1)%0A%20%20%20%20print(f%22Version%201%20available%3A%20%7Bold_weights%20is%20not%20None%7D%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%207.%20Weight%20Re-sharding%0A%0A%20%20%20%20%23%23%23%20The%20Sharding%20Mismatch%20Problem%0A%0A%20%20%20%20Trainer%20and%20Generator%20often%20have%20**different%20tensor%20layouts**%3A%0A%0A%20%20%20%20%7C%20Role%20%7C%20Parallelism%20%7C%20Sharding%20%7C%0A%20%20%20%20%7C------%7C-------------%7C----------%7C%0A%20%20%20%20%7C%20Trainer%20%7C%20FSDP%20(8%20GPUs)%20%7C%20%60Shard(0)%60%20-%20rows%20split%20across%208%20GPUs%20%7C%0A%20%20%20%20%7C%20Generator%20%7C%20TP%20(2%20GPUs)%20%7C%20%60Shard(1)%60%20-%20columns%20split%20across%202%20GPUs%20%7C%0A%0A%20%20%20%20Direct%20weight%20transfer%20doesn't%20work%20-%20we%20need%20**re-sharding**.%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20Trainer%20(row-sharded)%3A%20%20%20%20%20%20%20%20%20%20Generator%20(column-sharded)%3A%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%AC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20GPU%200%3A%20rows%200-127%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20GPU%200%20%20%20%E2%94%82%20GPU%201%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%9C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%A4%20%20%20%20%20%E2%86%92%20%20%20%20%20%20%E2%94%82%20cols%20%20%20%20%E2%94%82%20cols%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20GPU%201%3A%20rows%20128%2B%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%200-511%20%20%20%E2%94%82%20512%2B%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%B4%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%60%60%60%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%20Approach%201%3A%20Gather%20Then%20Slice%0A%0A%20%20%20%20Simple%20but%20inefficient%3A%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%201.%20Each%20receiver%20gathers%20ALL%20sender%20shards%20%E2%86%92%20full%20tensor%0A%20%20%20%202.%20Each%20receiver%20slices%20out%20its%20portion%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20**Pros**%3A%20Simple%2C%20works%20with%20any%20sharding%0A%20%20%20%20**Cons**%3A%202x%20redundant%20data%20transfer%20(each%20receiver%20gets%20everything)%0A%0A%20%20%20%20%23%23%23%20Approach%202%3A%20Routed%2FDirect%20Transfer%0A%0A%20%20%20%20Optimal%20but%20complex%3A%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%201.%20Pre-compute%20which%20sender%20chunks%20overlap%20with%20which%20receiver%20regions%0A%20%20%20%202.%20Send%20only%20the%20exact%20chunks%20needed%0A%20%20%20%203.%20Receivers%20place%20chunks%20directly%20in%20correct%20positions%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20**Pros**%3A%20Minimal%20bandwidth%20(no%20redundancy)%0A%20%20%20%20**Cons**%3A%20Complex%20overlap%20computation%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20from%20dataclasses%20import%20dataclass%0A%20%20%20%20from%20typing%20import%20List%2C%20Tuple%0A%0A%20%20%20%20%40dataclass%0A%20%20%20%20class%20ShardMetadata%3A%0A%20%20%20%20%20%20%20%20%22%22%22Metadata%20describing%20a%20tensor%20shard.%22%22%22%0A%20%20%20%20%20%20%20%20rank%3A%20int%0A%20%20%20%20%20%20%20%20global_shape%3A%20Tuple%5Bint%2C%20...%5D%0A%20%20%20%20%20%20%20%20offset%3A%20Tuple%5Bint%2C%20...%5D%20%20%23%20Start%20position%20in%20global%20tensor%0A%20%20%20%20%20%20%20%20local_shape%3A%20Tuple%5Bint%2C%20...%5D%20%20%23%20Shape%20of%20this%20shard%0A%0A%20%20%20%20def%20compute_shard_metadata(%0A%20%20%20%20%20%20%20%20global_shape%3A%20Tuple%5Bint%2C%20int%5D%2C%0A%20%20%20%20%20%20%20%20num_ranks%3A%20int%2C%0A%20%20%20%20%20%20%20%20shard_dim%3A%20int%2C%0A%20%20%20%20)%20-%3E%20List%5BShardMetadata%5D%3A%0A%20%20%20%20%20%20%20%20%22%22%22Compute%20shard%20metadata%20for%20a%20given%20sharding.%22%22%22%0A%20%20%20%20%20%20%20%20shards%20%3D%20%5B%5D%0A%20%20%20%20%20%20%20%20dim_size%20%3D%20global_shape%5Bshard_dim%5D%0A%20%20%20%20%20%20%20%20shard_size%20%3D%20dim_size%20%2F%2F%20num_ranks%0A%0A%20%20%20%20%20%20%20%20for%20rank%20in%20range(num_ranks)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20offset%20%3D%20%5B0%2C%200%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20local_shape%20%3D%20list(global_shape)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20offset%5Bshard_dim%5D%20%3D%20rank%20*%20shard_size%0A%20%20%20%20%20%20%20%20%20%20%20%20local_shape%5Bshard_dim%5D%20%3D%20shard_size%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20shards.append(ShardMetadata(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20rank%3Drank%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20global_shape%3Dglobal_shape%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20offset%3Dtuple(offset)%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20local_shape%3Dtuple(local_shape)%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20))%0A%0A%20%20%20%20%20%20%20%20return%20shards%0A%0A%20%20%20%20%23%20Demo%3A%20Trainer%20has%20row-sharding%2C%20Generator%20has%20column-sharding%0A%20%20%20%20_global_shape%20%3D%20(1024%2C%201024)%0A%0A%20%20%20%20trainer_shards%20%3D%20compute_shard_metadata(_global_shape%2C%20num_ranks%3D4%2C%20shard_dim%3D0)%0A%20%20%20%20generator_shards%20%3D%20compute_shard_metadata(_global_shape%2C%20num_ranks%3D2%2C%20shard_dim%3D1)%0A%0A%20%20%20%20print(%22Trainer%20shards%20(row-wise%2C%204%20GPUs)%3A%22)%0A%20%20%20%20for%20s%20in%20trainer_shards%3A%0A%20%20%20%20%20%20%20%20print(f%22%20%20Rank%20%7Bs.rank%7D%3A%20offset%3D%7Bs.offset%7D%2C%20shape%3D%7Bs.local_shape%7D%22)%0A%0A%20%20%20%20print(%22%5CnGenerator%20shards%20(column-wise%2C%202%20GPUs)%3A%22)%0A%20%20%20%20for%20s%20in%20generator_shards%3A%0A%20%20%20%20%20%20%20%20print(f%22%20%20Rank%20%7Bs.rank%7D%3A%20offset%3D%7Bs.offset%7D%2C%20shape%3D%7Bs.local_shape%7D%22)%0A%20%20%20%20return%20(%0A%20%20%20%20%20%20%20%20List%2C%0A%20%20%20%20%20%20%20%20ShardMetadata%2C%0A%20%20%20%20%20%20%20%20Tuple%2C%0A%20%20%20%20%20%20%20%20dataclass%2C%0A%20%20%20%20%20%20%20%20generator_shards%2C%0A%20%20%20%20%20%20%20%20trainer_shards%2C%0A%20%20%20%20)%0A%0A%0A%40app.cell%0Adef%20_(List%2C%20ShardMetadata%2C%20Tuple%2C%20dataclass%2C%20generator_shards%2C%20trainer_shards)%3A%0A%20%20%20%20%40dataclass%0A%20%20%20%20class%20TransferChunk%3A%0A%20%20%20%20%20%20%20%20%22%22%22A%20chunk%20to%20transfer%20from%20sender%20to%20receiver.%22%22%22%0A%20%20%20%20%20%20%20%20sender_rank%3A%20int%0A%20%20%20%20%20%20%20%20receiver_rank%3A%20int%0A%20%20%20%20%20%20%20%20sender_offset%3A%20Tuple%5Bint%2C%20int%5D%20%20%23%20Where%20to%20read%20from%20sender%0A%20%20%20%20%20%20%20%20receiver_offset%3A%20Tuple%5Bint%2C%20int%5D%20%20%23%20Where%20to%20write%20in%20receiver%0A%20%20%20%20%20%20%20%20shape%3A%20Tuple%5Bint%2C%20int%5D%20%20%23%20Shape%20of%20the%20chunk%0A%0A%20%20%20%20def%20compute_overlap(%0A%20%20%20%20%20%20%20%20sender%3A%20ShardMetadata%2C%0A%20%20%20%20%20%20%20%20receiver%3A%20ShardMetadata%2C%0A%20%20%20%20)%20-%3E%20TransferChunk%20%7C%20None%3A%0A%20%20%20%20%20%20%20%20%22%22%22Compute%20overlap%20between%20sender%20and%20receiver%20shards.%22%22%22%0A%20%20%20%20%20%20%20%20%23%20Find%20intersection%20in%20global%20coordinates%0A%20%20%20%20%20%20%20%20s_start%20%3D%20sender.offset%0A%20%20%20%20%20%20%20%20s_end%20%3D%20(s_start%5B0%5D%20%2B%20sender.local_shape%5B0%5D%2C%20s_start%5B1%5D%20%2B%20sender.local_shape%5B1%5D)%0A%0A%20%20%20%20%20%20%20%20r_start%20%3D%20receiver.offset%0A%20%20%20%20%20%20%20%20r_end%20%3D%20(r_start%5B0%5D%20%2B%20receiver.local_shape%5B0%5D%2C%20r_start%5B1%5D%20%2B%20receiver.local_shape%5B1%5D)%0A%0A%20%20%20%20%20%20%20%20%23%20Compute%20intersection%0A%20%20%20%20%20%20%20%20inter_start%20%3D%20(max(s_start%5B0%5D%2C%20r_start%5B0%5D)%2C%20max(s_start%5B1%5D%2C%20r_start%5B1%5D))%0A%20%20%20%20%20%20%20%20inter_end%20%3D%20(min(s_end%5B0%5D%2C%20r_end%5B0%5D)%2C%20min(s_end%5B1%5D%2C%20r_end%5B1%5D))%0A%0A%20%20%20%20%20%20%20%20%23%20Check%20if%20there's%20actual%20overlap%0A%20%20%20%20%20%20%20%20if%20inter_start%5B0%5D%20%3E%3D%20inter_end%5B0%5D%20or%20inter_start%5B1%5D%20%3E%3D%20inter_end%5B1%5D%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20None%0A%0A%20%20%20%20%20%20%20%20shape%20%3D%20(inter_end%5B0%5D%20-%20inter_start%5B0%5D%2C%20inter_end%5B1%5D%20-%20inter_start%5B1%5D)%0A%0A%20%20%20%20%20%20%20%20%23%20Convert%20to%20local%20coordinates%0A%20%20%20%20%20%20%20%20sender_local%20%3D%20(inter_start%5B0%5D%20-%20s_start%5B0%5D%2C%20inter_start%5B1%5D%20-%20s_start%5B1%5D)%0A%20%20%20%20%20%20%20%20receiver_local%20%3D%20(inter_start%5B0%5D%20-%20r_start%5B0%5D%2C%20inter_start%5B1%5D%20-%20r_start%5B1%5D)%0A%0A%20%20%20%20%20%20%20%20return%20TransferChunk(%0A%20%20%20%20%20%20%20%20%20%20%20%20sender_rank%3Dsender.rank%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20receiver_rank%3Dreceiver.rank%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20sender_offset%3Dsender_local%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20receiver_offset%3Dreceiver_local%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20shape%3Dshape%2C%0A%20%20%20%20%20%20%20%20)%0A%0A%20%20%20%20def%20compute_transfer_plan(%0A%20%20%20%20%20%20%20%20sender_shards%3A%20List%5BShardMetadata%5D%2C%0A%20%20%20%20%20%20%20%20receiver_shards%3A%20List%5BShardMetadata%5D%2C%0A%20%20%20%20)%20-%3E%20List%5BTransferChunk%5D%3A%0A%20%20%20%20%20%20%20%20%22%22%22Compute%20all%20transfers%20needed%20for%20re-sharding.%22%22%22%0A%20%20%20%20%20%20%20%20transfers%20%3D%20%5B%5D%0A%20%20%20%20%20%20%20%20for%20sender%20in%20sender_shards%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20receiver%20in%20receiver_shards%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20chunk%20%3D%20compute_overlap(sender%2C%20receiver)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20chunk%20is%20not%20None%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20transfers.append(chunk)%0A%20%20%20%20%20%20%20%20return%20transfers%0A%0A%20%20%20%20%23%20Compute%20transfer%20plan%0A%20%20%20%20transfer_plan%20%3D%20compute_transfer_plan(trainer_shards%2C%20generator_shards)%0A%0A%20%20%20%20print(f%22Transfer%20plan%3A%20%7Blen(transfer_plan)%7D%20chunks%20needed%5Cn%22)%0A%20%20%20%20for%20chunk%20in%20transfer_plan%3A%0A%20%20%20%20%20%20%20%20print(f%22Sender%20%7Bchunk.sender_rank%7D%20%E2%86%92%20Receiver%20%7Bchunk.receiver_rank%7D%22)%0A%20%20%20%20%20%20%20%20print(f%22%20%20Read%20from%20sender%20offset%20%7Bchunk.sender_offset%7D%2C%20shape%20%7Bchunk.shape%7D%22)%0A%20%20%20%20%20%20%20%20print(f%22%20%20Write%20to%20receiver%20offset%20%7Bchunk.receiver_offset%7D%22)%0A%20%20%20%20%20%20%20%20print()%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%20The%20DTensor%20Dream%20(and%20Reality)%0A%0A%20%20%20%20If%20trainer%20and%20generator%20both%20used%20**DTensor**%20with%20the%20same%20sharding%20spec%2C%20re-sharding%0A%20%20%20%20would%20be%20automatic%20-%20the%20framework%20handles%20overlap%20computation%20and%20transfers.%0A%0A%20%20%20%20%60%60%60python%0A%20%20%20%20%23%20Ideal%20world%3A%20DTensor%20handles%20it%0A%20%20%20%20trainer_weights%3A%20DTensor%20%20%23%20Shard(0)%20across%208%20GPUs%0A%20%20%20%20generator_weights%3A%20DTensor%20%20%23%20Shard(1)%20across%202%20GPUs%0A%0A%20%20%20%20%23%20Re-sharding%20is%20just%20redistribution%0A%20%20%20%20generator_weights%20%3D%20trainer_weights.redistribute(generator_placement)%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20**In%20practice%2C%20it's%20harder**%3A%0A%20%20%20%20-%20vLLM%20does%20its%20own%20sharding%20and%20weight%20fusing%20(not%20DTensor-compatible)%0A%20%20%20%20-%20Training%20frameworks%20(FSDP%2C%20etc.)%20have%20different%20abstractions%0A%20%20%20%20-%20You%20often%20need%20custom%20overlap%20computation%20like%20we%20showed%20above%0A%0A%20%20%20%20The%20routed%20approach%20(compute%20overlaps%2C%20send%20only%20needed%20chunks)%20is%202x%20faster%20than%0A%20%20%20%20naive%20gather-then-slice%2C%20but%20requires%20this%20manual%20coordination.%0A%0A%20%20%20%20**For%20cross-node%20RDMA%20transfers**%2C%20the%20key%20insight%20remains%3A%20pre-compute%20the%20transfer%0A%20%20%20%20plan%20once%2C%20then%20each%20sync%20just%20executes%20the%20planned%20RDMA%20operations%20with%20RDMAAction.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%208.%20Putting%20It%20All%20Together%0A%0A%20%20%20%20The%20full%20async%20RL%20weight%20sync%20pattern%3A%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20TRAINER%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%201.%20Train%20step%20completes%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%202.%20Copy%20weights%20to%20CPU%20staging%20buffer%20(non-blocking%20D2H)%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%203.%20Publish%20to%20circular%20buffer%20with%20version%20tag%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%204.%20Continue%20training%20(no%20blocking!)%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%96%BC%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20CIRCULAR%20BUFFER%20(CPU%2C%20RDMA-registered)%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%5Bslot%200%3A%20v3%5D%20%5Bslot%201%3A%20v4%5D%20%5Bslot%202%3A%20v5%5D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%86%91%20latest%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%BC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%96%BC%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%96%BC%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%96%BC%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20%20%20GENERATOR%200%20%20%20%E2%94%82%20%20%20%E2%94%82%20%20%20GENERATOR%201%20%20%20%E2%94%82%20%20%20%E2%94%82%20%20%20GENERATOR%202%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20After%20gen%20done%3A%20%E2%94%82%20%20%20%E2%94%82%20After%20gen%20done%3A%20%E2%94%82%20%20%20%E2%94%82%20After%20gen%20done%3A%20%E2%94%82%0A%20%20%20%20%E2%94%82%201.%20Get%20latest%20%20%20%E2%94%82%20%20%20%E2%94%82%201.%20Get%20latest%20%20%20%E2%94%82%20%20%20%E2%94%82%201.%20Get%20latest%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20version%20%20%20%20%20%20%E2%94%82%20%20%20%E2%94%82%20%20%20%20version%20%20%20%20%20%20%E2%94%82%20%20%20%E2%94%82%20%20%20%20version%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%202.%20RDMA%20read%20%20%20%20%E2%94%82%20%20%20%E2%94%82%202.%20RDMA%20read%20%20%20%20%E2%94%82%20%20%20%E2%94%82%202.%20RDMA%20read%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%E2%86%92%20GPU%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%E2%94%82%20%20%20%20%E2%86%92%20GPU%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%E2%94%82%20%20%20%20%E2%86%92%20GPU%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%203.%20Re-shard%20if%20%20%E2%94%82%20%20%20%E2%94%82%203.%20Re-shard%20if%20%20%E2%94%82%20%20%20%E2%94%82%203.%20Re-shard%20if%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20needed%20%20%20%20%20%20%20%E2%94%82%20%20%20%E2%94%82%20%20%20%20needed%20%20%20%20%20%20%20%E2%94%82%20%20%20%E2%94%82%20%20%20%20needed%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20**Key%20properties%3A**%0A%20%20%20%20-%20Trainer%20never%20blocks%20waiting%20for%20generators%0A%20%20%20%20-%20Generators%20pull%20directly%20to%20GPU%20when%20*they're*%20ready%0A%20%20%20%20-%20Re-sharding%20happens%20locally%20on%20each%20generator%0A%20%20%20%20-%20Circular%20buffer%20bounds%20memory%2C%20reuses%20RDMA%20registrations%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%20Code%20Pattern%0A%0A%20%20%20%20%60%60%60python%0A%20%20%20%20%23%20Trainer%20side%0A%20%20%20%20class%20Trainer(Actor)%3A%0A%20%20%20%20%20%20%20%20def%20__init__(self)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.weight_buffer%20%3D%20CircularWeightBuffer(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20template%3Dself.model.state_dict_tensor()%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20n_slots%3D3%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20get_weight_handle(self)%20-%3E%20Tuple%5BRDMABuffer%2C%20int%5D%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20'''Return%20handle%20to%20latest%20weights%20and%20version.'''%0A%20%20%20%20%20%20%20%20%20%20%20%20slot_idx%20%3D%20(self.weight_buffer.latest_version%20-%201)%20%25%203%0A%20%20%20%20%20%20%20%20%20%20%20%20handle%20%3D%20self.weight_buffer.rdma_handles%5Bslot_idx%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20version%20%3D%20self.weight_buffer.latest_version%20-%201%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20handle%2C%20version%0A%0A%20%20%20%20%20%20%20%20async%20def%20train_loop(self)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20while%20True%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20loss%20%3D%20self.train_step()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20self.step%20%25%20sync_interval%20%3D%3D%200%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Non-blocking%20publish%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.weight_buffer.publish(self.model.weights)%0A%0A%20%20%20%20%23%20Generator%20side%0A%20%20%20%20class%20Generator(Actor)%3A%0A%20%20%20%20%20%20%20%20def%20__init__(self%2C%20trainer_ref)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.trainer%20%3D%20trainer_ref%0A%20%20%20%20%20%20%20%20%20%20%20%20self.current_version%20%3D%20-1%0A%0A%20%20%20%20%20%20%20%20async%20def%20maybe_sync_weights(self)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20handle%2C%20version%20%3D%20await%20self.trainer.get_weight_handle.call_one().get()%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20version%20%3E%20self.current_version%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Pull%20via%20RDMA%20directly%20into%20GPU%20memory%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20gpu_weights%20%3D%20self.model.weights.view(torch.uint8).flatten()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20await%20handle.read_into(gpu_weights)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.current_version%20%3D%20version%0A%0A%20%20%20%20%20%20%20%20async%20def%20generate_loop(self)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20while%20True%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20await%20self.maybe_sync_weights()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20output%20%3D%20self.generate(prompt)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.submit_to_buffer(output)%0A%20%20%20%20%60%60%60%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Summary%0A%0A%20%20%20%20%23%23%23%20Key%20Takeaways%0A%0A%20%20%20%201.%20**Bandwidth%20hierarchy%20matters**%3A%20NVLink%20(450%20GB%2Fs)%20%3E%3E%20InfiniBand%20(50-100%20GB%2Fs)%20%3E%3E%20PCIe%0A%20%20%20%20%20%20%20-%20Know%20your%20hardware%2C%20optimize%20for%20the%20right%20interconnect%0A%0A%20%20%20%202.%20**Collectives%20vs%20P2P**%3A%20Collectives%20are%20synchronized%3B%20RL%20needs%20async%20P2P%0A%20%20%20%20%20%20%20-%20High%20variance%20in%20generation%20times%20makes%20blocking%20expensive%0A%0A%20%20%20%203.%20**Magic%20pointer%20pattern**%3A%20Tiny%20handle%20over%20control%20plane%2C%20bulk%20data%20over%20data%20plane%0A%20%20%20%20%20%20%20-%20~100%20bytes%20to%20describe%2010%20GB%20transfer%0A%0A%20%20%20%204.%20**CPU%20staging**%3A%20Temporal%20decoupling%20for%20async%20RL%0A%20%20%20%20%20%20%20-%20GPU-native%20RDMA%20works%20for%20sync%20cases%0A%20%20%20%20%20%20%20-%20CPU%20staging%20ensures%20nothing%20blocks%20on%20the%20critical%20path%0A%0A%20%20%20%205.%20**Circular%20buffers**%3A%20Version%20weights%20without%20memory%20churn%0A%20%20%20%20%20%20%20-%20Pre-register%20RDMA%20buffers%2C%20reuse%20slots%0A%20%20%20%20%20%20%20-%20Generators%20grab%20latest%2C%20never%20stale%0A%0A%20%20%20%206.%20**Weight%20re-sharding**%3A%20Different%20layouts%20need%20overlap%20computation%0A%20%20%20%20%20%20%20-%20Routed%20approach%20is%202x%20faster%20than%20gather%0A%20%20%20%20%20%20%20-%20Pre-compute%20transfer%20plan%2C%20minimize%20redundant%20data%0A%0A%20%20%20%20%23%23%23%20Next%20Steps%0A%0A%20%20%20%20See%20**07_async_rl_e2e.py**%20for%20a%20complete%20async%20RL%20system%20that%20uses%20these%20patterns.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0Aif%20__name__%20%3D%3D%20%22__main__%22%3A%0A%20%20%20%20app.run()%0A
</marimo-code>

<marimo-code-hash hidden="">a85c13fccef7c08db2d9ee37f7fb2c48</marimo-code-hash>
</body>
</html>
