<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/favicon.ico" />
    <!-- Preload is necessary because we show these images when we disconnect from the server,
    but at that point we cannot load these images from the server -->
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/gradient-yHQUC_QB.png" as="image" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/noise-60BoTA8O.png" as="image" />
    <!-- Preload the fonts -->
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/Lora-VariableFont_wght-B2ootaw-.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/PTSans-Regular-CxL0S8W7.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/PTSans-Bold-D9fedIX3.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/FiraMono-Regular-BTCkDNvf.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/FiraMono-Medium-DU3aDxX5.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/FiraMono-Bold-CLVRCuM9.ttf" as="font" crossorigin="anonymous" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta name="description" content="a marimo app" />
    <link rel="apple-touch-icon" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/apple-touch-icon.png" />
    <link rel="manifest" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/manifest.json" />

    <script data-marimo="true">
      function __resizeIframe(obj) {
        const scrollbarHeight = 20; // Max between windows, mac, and linux

        function setHeight() {
          // Guard against race condition where iframe isn't ready
          if (!obj.contentWindow?.document?.documentElement) {
            return;
          }
          const element = obj.contentWindow.document.documentElement;
          // If there is no vertical scrollbar, we don't need to resize the iframe
          if (element.scrollHeight === element.clientHeight) {
            return;
          }

          // Create a new height that includes the scrollbar height if it's visible
          const hasHorizontalScrollbar = element.scrollWidth > element.clientWidth;
          const newHeight = element.scrollHeight + (hasHorizontalScrollbar ? scrollbarHeight : 0);

          // Only update the height if it's different from the current height
          if (obj.style.height !== `${newHeight}px`) {
            obj.style.height = `${newHeight}px`;
          }
        }

        // Resize the iframe to the height of the content and bottom scrollbar height
        setHeight();

        // Resize the iframe when the content changes
        const resizeObserver = new ResizeObserver((_entries) => {
          setHeight();
        });
        // Only observe if iframe content is ready
        if (obj.contentWindow?.document?.body) {
          resizeObserver.observe(obj.contentWindow.document.body);
        }
      }
    </script>
    <marimo-filename hidden>08_rl_e2e.py</marimo-filename>
    <!-- TODO(Trevor): Legacy, required by VS Code plugin. Remove when plugin is updated (see marimo/server/_templates/template.py) -->
    <marimo-version data-version="{{ version }}" hidden></marimo-version>
    <marimo-user-config data-config="{{ user_config }}" hidden></marimo-user-config>
    <marimo-server-token data-token="{{ server_token }}" hidden></marimo-server-token>
    <!-- /TODO -->
    <title>08 rl e2e</title>
    <script type="module" crossorigin crossorigin="anonymous" src="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/index-CD6Gw4UH.js"></script>
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/preload-helper-D2MJg03u.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/clsx-D8GwTfvk.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/cn-BKtXLv3a.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/chunk-LvLJmgfZ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/react-Bj1aDYRI.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/compiler-runtime-B3qBwwSJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/jsx-runtime-ZmTK25f3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/badge-DX6CQ6PA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/hotkeys-BHHWjLlp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useEventListener-Cb-RVVEn.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/button-CZ3Cs4qb.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/react-dom-CSu739Rf.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/Combination-BAEdC-rz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/menu-items-BMjcEb2j.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dist-DwV58Fb1.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/createLucideIcon-BCdY6lG5.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/check-Dr3SxUsb.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/x-ZP5cObgf.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/select-BVdzZKAh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/tooltip-CMQz28hC.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/use-toast-BDYuj3zG.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/_Uint8Array-BGESiCQL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/_baseIsEqual-B9N9Mw_N.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useEvent-BhXAndur.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/invariant-CAG_dYON.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/_baseFor-Duhs3RiJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/merge-BBX6ug-N.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/zod-H_cgTO0M.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/utils-YqBXNpsM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/Deferred-DxQeE5uh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/uuid-DXdzqzcr.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/DeferredRequestRegistry-CMf25YiV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/constants-B6Cb__3x.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/session-BOFn9QrD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/config-Q0O7_stz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/requests-B4FYHTZl.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/isSymbol-BGkTcW3U.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/toString-DlRqgfqz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/_hasUnicode-CWqKLxBC.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/assertNever-CBU83Y6o.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/_arrayReduce-TT0iOGKY.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useLifecycle-ClI_npeg.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useNonce-CS26E0hA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useTheme-DQozhcp1.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/once-Bul8mtFs.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/capabilities-MM7JYRxj.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/createReducer-B3rBsy4P.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/paths-BzSgteR-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dist-DBwNzi3C.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dist-ChS0Dc_R.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dist-CtsanegT.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dist-Dcqqg9UU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dist-sMh6mJ2d.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dist-Btv5Rh1v.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dist-bBwmhqty.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dist-CoCQUAeM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dist-Gqv0jSNr.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/stex-jWatZkll.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/toDate-DETS9bBd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/cjs-CH5Rj0g8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/_baseProperty-NKyJO2oh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/now-6sUe0ZdD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/debounce-B3mjKxHe.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/toInteger-CDcO32Gx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/database-zap-k4ePIFAU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/main-U5Goe76G.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/cells-DPp5cDaO.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/spinner-DA8-7wQv.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/chevron-right--18M_6o9.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dropdown-menu-ldcmQvIV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/kbd-Cm6Ba9qg.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/renderShortcut-BckyRbYt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/multi-map-DxdLNTBd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/alert-BOoN6gJ1.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/card-OlSjYhmd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/alert-dialog-BW4srmS0.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dialog-eb-NieZw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dist-CDXJRSCj.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/label-E64zk6_7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useDebounce-7iEVSqwM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/textarea-CRI7xDBj.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/numbers-D7O23mOZ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/SSRProvider-BIDQNg9Q.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/context-BfYAMNLF.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useNumberFormatter-Db6Vjve5.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/usePress-C__vuri5.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/input-DUrq2DiR.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/links-7AQBmdyV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/popover-CH1FzjxU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/switch-dWLWbbtg.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/table-DScsXgJW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/mode-Bn7pdJvO.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useAsyncData-BMGLSTg8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/errors-TZBmrJmc.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/error-banner-B9ts0mNl.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/copy-DHrHayPa.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/memoize-BCOZVFBt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/get-6uJrSKbw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/capitalize-CmNnkG9y.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/copy-D-8y6iMN.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/plus-B7DF33lD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/refresh-cw-Dx8TEWFP.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/trash-2-DDsWrxuJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/triangle-alert-CebQ7XwA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/ai-model-dropdown-Dk2SdB3C.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/defaultLocale-JieDVWC_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/precisionRound-CU2C3Vxx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/defaultLocale-BLne0bXb.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/vega-loader.browser-DXARUlxo.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/tooltip-DxKBXCGp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/ErrorBoundary-B9Ifj8Jf.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useInstallPackage-D4fX0Ee_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/ImperativeModal-BNN1HA7x.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/cell-link-B9b7J8QK.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/datasource-CtyqtITR.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/state-D4T75eZb.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/MarimoErrorOutput-Lf9P8Fhl.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/copy-icon-v8ME_JKB.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/html-to-image-CIQqSu-S.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/focus-C1YokgL7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/LazyAnyLanguageCodeMirror-DgZ8iknE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/chunk-5FQGJX7Z-DPlx2kjA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/katex-CDLTCvjQ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/markdown-renderer-DJy8ww5d.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/command-2ElA5IkO.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/download-os8QlW6l.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useRunCells-D2HBb4DB.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/purify.es-DZrAQFIu.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/RenderHTML-D-of_-s7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useIframeCapabilities-B_pQb20b.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/formats-CobRswjh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/en-US-CCVfmA-q.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/isValid-DDt9wNjK.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/dates-CrvjILe3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/maps-D2_Mq1pZ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/extends-BiFDv3jB.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/emotion-is-prop-valid.esm-C59xfSYt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useDateFormatter-CqhdUl2n.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/range-D2UKkEg-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/table-CfDbAm78.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/JsonOutput-PE5ko4gi.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useDeleteCell-DdRX94yC.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/icons-CCHmxi8d.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/process-output-ByfLnk6j.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/blob-D-eV0cU3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/objectWithoutPropertiesLoose-DfWeGRFv.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/esm-Bmu2DhPy.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/file-Ch78NKWp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/play-GLWQQs7F.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/add-cell-with-ai-e_HMl7UU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/isEmpty-CgX_-6Mt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/bot-message-square-B2ThzDUZ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/chat-display--jAB7huF.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/chart-no-axes-column-qvVRjhv1.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/square-function-B6mgCeFJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/spec-Ch0xnJY4.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/column-preview-CXjSXUhP.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/toggle-zVW4FXNz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/globals-BgACvYmr.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/share-ipf2hrOh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/_baseSet-5Rdwpmr3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/react-resizable-panels.browser.esm-Da3ksQXL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/utilities.esm-dm9SQStE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/floating-outline-BtdqbkUq.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useAddCell-CmuX2hOk.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/eye-off-AK_9uodG.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/readonly-python-code-WjTf6Pdd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/file-video-camera-C3wGzBnE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/types-BRfQN3HL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/refresh-ccw-DN_xCV6A.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/form-BidPUZUn.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/field-CySaBlkz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useBoolean-Ck_unDZw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/useDeepCompareMemoize-5OUgerQ3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/types-C1UhS3qM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/prop-types-DaaA-ptl.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/es-BYgU_srD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/hasIn-CycJImp8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/_baseFlatten-CUZNxU8H.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/flatten-D-7VEN0q.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/pick-B_6Qi5aM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/code-xml-CgN_Yig7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/download-Dg7clfkc.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/square-CuJ72M8f.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/settings-OBbrbhij.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/bundle.esm-i_UbZC0w.js">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/cells-jmgGt1lS.css">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/markdown-renderer-DdDKmWlR.css">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/JsonOutput-B7vuddcd.css">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.19.9/dist/assets/index-CeUwN_0i.css">
  
<script data-marimo="true">
    window.__MARIMO_STATIC__ = {};
    window.__MARIMO_STATIC__.files = {};
</script>
</head>
  <body>
    <div id="root"></div>
    <!-- This is a portal for the data editor to render in -->
    <div id="portal" data-testid="glide-portal" style="position: fixed; left: 0; top: 0; z-index: 9999"></div>
    <script data-marimo="true">
      window.__MARIMO_MOUNT_CONFIG__ = {
            "filename": "08_rl_e2e.py",
            "mode": "read",
            "version": "0.19.9",
            "serverToken": "static",
            "config": {"ai": {"custom_providers": {}, "models": {"custom_models": [], "displayed_models": []}}, "completion": {"activate_on_typing": true, "copilot": false, "signature_hint_on_typing": false}, "diagnostics": {"sql_linter": true}, "display": {"cell_output": "below", "code_editor_font_size": 14, "dataframes": "rich", "default_table_max_columns": 50, "default_table_page_size": 10, "default_width": "medium", "reference_highlighting": true, "theme": "light"}, "formatting": {"line_length": 79}, "keymap": {"overrides": {}, "preset": "default"}, "language_servers": {"pylsp": {"enable_flake8": false, "enable_mypy": true, "enable_pydocstyle": false, "enable_pyflakes": false, "enable_pylint": false, "enable_ruff": true, "enabled": false}}, "mcp": {"mcpServers": {}, "presets": []}, "package_management": {"manager": "uv"}, "runtime": {"auto_instantiate": false, "auto_reload": "off", "default_csv_encoding": "utf-8", "default_sql_output": "auto", "on_cell_change": "autorun", "output_max_bytes": 8000000, "reactive_tests": true, "std_stream_max_bytes": 1000000, "watcher_on_save": "lazy"}, "save": {"autosave": "after_delay", "autosave_delay": 1000, "format_on_save": false}, "server": {"browser": "default", "follow_symlink": false}, "snippets": {"custom_paths": [], "include_default_snippets": true}},
            "configOverrides": {},
            "appConfig": {"sql_output": "auto", "width": "medium"},
            "view": {"showAppCode": true},
            "notebook": {"cells": [{"code": "import marimo as mo", "code_hash": "1d0db38904205bec4d6f6f6a1f6cec3e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Hbol", "name": "_"}, {"code": "# Environment setup for Monarch subprocesses\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"HF_HUB_OFFLINE\"] = \"1\"\nos.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n# Note: CUDA_VISIBLE_DEVICES is set per-actor in setup()\n# Note: PYTORCH_ALLOC_CONF is set at module level for RDMA\n\nimport sys\n_src_dir = os.path.abspath(os.path.join(os.path.dirname(__file__) if \"__file__\" in dir() else os.getcwd(), \"..\", \"src\"))\nif _src_dir not in sys.path:\n    sys.path.insert(0, _src_dir)\n\n# Set PYTHONPATH for Monarch subprocesses\n_existing = os.environ.get(\"PYTHONPATH\", \"\")\nos.environ[\"PYTHONPATH\"] = f\"{_src_dir}:{_existing}\" if _existing else _src_dir", "code_hash": "9d2da1a64a7c48c53bbbd693ac608cce", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "MJUe", "name": "_"}, {"code": "mo.md(r\"\"\"\n# Closing the Loop: Async RL Training\n\nIn **Notebook 05**, we introduced the Zorplex benchmark and identified three\nfailure modes: wrong format (no `[ANSWER]` tag), tool spam, and wrong answers.\nThe model often gets the right value but fails to emit it correctly.\n\n**Now we close the loop**: train the model to get better at these tasks, and\ntrack which failure modes improve during training.\n\nWe'll build on patterns from across the series. The architecture we're building: multiple generators\nfeed trajectories into a replay buffer while a trainer continuously samples and updates the policy.\n\nWe'll measure *before* and *after* accuracy -- and failure mode breakdown -- to\nsee if training actually helps.\n\"\"\")", "code_hash": "a9faced051e7222f49293f3c3a8368d0", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "vblA", "name": "_"}, {"code": "from collections import deque\nimport random\nimport torch\nimport torch.nn.functional as F\n\n# Model imports\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Zorplex imports\nfrom zorplex_rl import get_spec, Task\nfrom zorplex_rl.evaluate import generate_with_tools\n\n# RL primitives (shared dataclasses)\nfrom rl_primitives import Trajectory, TrainMetrics\n\n# RDMA imports (with fallback)\ntry:\n    from monarch.rdma import RDMABuffer, is_rdma_available\n    _rdma_available = is_rdma_available()\nexcept Exception:\n    RDMABuffer = None\n    _rdma_available = False\n\ndef rdma_available():\n    return _rdma_available", "code_hash": "e24305fa03ca6fb85a9e6c40bae7425e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "bkHC", "name": "_"}, {"code": "from monarch.actor import Actor, endpoint, current_rank", "code_hash": "5fc990d636c1d12556a4677252891c16", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "lEQa", "name": "_"}, {"code": "import dataclasses as _dc\n_traj_fields = [(f.name, f.type.__name__ if hasattr(f.type, '__name__') else str(f.type)) for f in _dc.fields(Trajectory)]\n_metrics_fields = [(f.name, f.type.__name__ if hasattr(f.type, '__name__') else str(f.type)) for f in _dc.fields(TrainMetrics)]\n\nmo.md(f\"\"\"\n## Shared Data Structures\n\nFor clarity, we are using the following data structures in this notebook:\n\n**Trajectory** -- one rollout from a generator:\n\n| Field | Type |\n|-------|------|\n{\"\".join(f\"| `{n}` | `{t}` |{chr(10)}\" for n, t in _traj_fields)}\n\n**TrainMetrics** -- returned after each training step:\n\n| Field | Type |\n|-------|------|\n{\"\".join(f\"| `{n}` | `{t}` |{chr(10)}\" for n, t in _metrics_fields)}\n\nKey fields:\n- `model_only_text` stores the model's generated tokens without injected tool\n  results, so the trainer can compute log-probabilities on exactly what the model produced.\n- `has_answer_tag` tracks whether the model emitted `[ANSWER]` -- this is the\n  format compliance signal from [NB05](./05_rl_intro.html)'s failure mode analysis.\n- `failure_mode` classifies each trajectory as `\"success\"`, `\"wrong_format\"`,\n  `\"tool_spam\"`, or `\"wrong_answer\"` so we can track which failure modes improve\n  during training.\n- `correct_rate` and `format_rate` on `TrainMetrics` let us track these signals\n  per training step.\n\"\"\")", "code_hash": "32735ea5328ce18acdcd3809bedc813b", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "PKri", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Service Infrastructure\n\nWe import a **custom Service abstraction** from `monarch_utils` that manages worker\nreplicas with health tracking and round-robin routing. This is a utility we built\nfor this notebook series -- the canonical Monarch pattern uses direct actor\nreferences and slicing, which is what the Service wraps internally.\n\n(See notebook 05 for the full implementation.)\n\"\"\")", "code_hash": "4e6c895b9547643bc1a0087250cf6b27", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Xref", "name": "_"}, {"code": "from monarch_utils.services import Service, register_service", "code_hash": "e644d8e238271c0dcb2c797fbb8e561f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "SFPL", "name": "_"}, {"code": "mo.md(r\"\"\"\n## The `setup()` Pattern\n\nActors in this notebook use a two-phase initialization:\n\n1. **`__init__`** runs during `spawn()` -- keep it lightweight (store config, set rank)\n2. **`setup()`** is an endpoint called explicitly after spawn -- do heavy work here\n   (load models, allocate GPU memory, register RDMA buffers)\n\nWhy not do everything in `__init__`? Two reasons:\n\n- **`spawn()` is asynchronous** -- it returns immediately, and `__init__` runs in\n  the remote process before the first endpoint call. But you don't control *when*,\n  and you can't confirm it completed. An explicit `setup()` call lets you sequence\n  initialization (e.g., set `CUDA_VISIBLE_DEVICES` and confirm it took effect before\n  loading a model).\n- **Coordination** -- you often need to initialize actors in a specific order (set up\n  the trainer before generators try to sync weights). Endpoint calls give you that\n  sequencing; `__init__` doesn't.\n\"\"\")", "code_hash": "d05d08aec233ea3fb2641681ad0bae7b", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "BYtC", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Actor 1: ZorplexWorker\n\nTool execution environments (docker containers, sandboxes, API endpoints) naturally\nform a fleet -- you want many instances running in parallel to keep up with\ngeneration throughput. That makes them a good fit for a **Service** (from [NB06](./06_services.html))\nwith health tracking and round-robin routing.\n\nOur ZorplexWorker actors handle Zorplex tasks:\n- `generate_task()` -- creates a new problem\n- `execute_tool()` -- handles LOOKUP calls\n- `check_answer()` -- verifies correctness\n\"\"\")", "code_hash": "7f9ef072399bd5d75ebb426bac50701e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "RGSE", "name": "_"}, {"code": "class ZorplexWorker(Actor):\n    \"\"\"Worker actor that handles Zorplex tool execution.\n\n    Managed by a Service for load balancing across replicas.\n    \"\"\"\n\n    def __init__(self, difficulty: str = \"easy\", seed: int = 42):\n        self.rank = current_rank().rank\n        self.spec = get_spec(\"compositional\", difficulty=difficulty, seed=seed + self.rank)\n        self.calls_served = 0\n        print(f\"[ZorplexWorker:{self.rank}] Initialized with difficulty={difficulty}\")\n\n    @endpoint\n    def ping(self) -\u003E bool:\n        return True\n\n    @endpoint\n    def generate_task(self) -\u003E tuple[str, int]:\n        \"\"\"Generate a new task. Returns (question, correct_answer).\"\"\"\n        task = self.spec.generate_task()\n        return task.question, task.correct_answer\n\n    @endpoint\n    def execute_tool(self, tool_name: str, argument: str) -\u003E str:\n        \"\"\"Execute a tool call.\"\"\"\n        from zorplex_rl.task_specs import ToolCall\n        tc = ToolCall(tool_name, argument)\n        result = self.spec.execute_tool(tc)\n        self.calls_served += 1\n        return str(result)\n\n    @endpoint\n    def get_system_prompt(self) -\u003E str:\n        \"\"\"Get the system prompt with tool hints.\"\"\"\n        return self.spec.get_system_prompt(with_hint=True)\n\n    @endpoint\n    def check_answer(self, model_output: str, correct_answer: int) -\u003E tuple[bool, int | None]:\n        \"\"\"Check if model output contains the correct answer.\"\"\"\n        extracted = self.spec.extract_answer(model_output, [])\n        is_correct = extracted == correct_answer\n        return is_correct, extracted\n\n    @endpoint\n    def stats(self) -\u003E dict:\n        return {\"rank\": self.rank, \"calls_served\": self.calls_served}", "code_hash": "5b736dfec5189aa58d1c4d7bd99b7601", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Kclp", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Actor 2: ReplayBuffer\n\nA simple actor that stores trajectories. Generators push trajectories in,\nthe trainer samples batches out.\n\nRecall our intro to async RL in notebook 4 -- the replay buffer is the decoupling\npoint that enables asynchronous execution. Generators push, trainer pulls, neither\nwaits for the other. A secondary benefit is decorrelation: random sampling breaks\nthe correlation between consecutive trajectories from the same generator, giving\nbetter gradient estimates (especially when mixing tasks of different difficulties).\n\"\"\")", "code_hash": "9d4b4d785de4cd9567cfb9a9e40236a1", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "emfo", "name": "_"}, {"code": "class ReplayBuffer(Actor):\n    \"\"\"Stores trajectories for async RL training.\"\"\"\n\n    def __init__(self, max_size: int = 1000):\n        self.buffer: deque[Trajectory] = deque(maxlen=max_size)\n        self.total_added = 0\n        print(f\"[ReplayBuffer] Initialized with max_size={max_size}\")\n\n    @endpoint\n    def add(self, trajectory: Trajectory) -\u003E None:\n        \"\"\"Add a trajectory to the buffer.\"\"\"\n        self.buffer.append(trajectory)\n        self.total_added += 1\n\n    @endpoint\n    def sample(self, batch_size: int) -\u003E list[Trajectory]:\n        \"\"\"Sample a batch of trajectories.\"\"\"\n        if len(self.buffer) == 0:\n            return []\n        n = min(batch_size, len(self.buffer))\n        return random.sample(list(self.buffer), n)\n\n    @endpoint\n    def size(self) -\u003E int:\n        return len(self.buffer)\n\n    @endpoint\n    def clear(self) -\u003E int:\n        \"\"\"Clear the buffer. Returns number of items removed.\"\"\"\n        count = len(self.buffer)\n        self.buffer.clear()\n        return count\n\n    @endpoint\n    def stats(self) -\u003E dict:\n        if len(self.buffer) == 0:\n            return {\"size\": 0, \"total_added\": self.total_added, \"avg_reward\": 0.0}\n        rewards = [t.reward for t in self.buffer]\n        failure_modes = {}\n        for t in self.buffer:\n            fm = t.failure_mode or \"unknown\"\n            failure_modes[fm] = failure_modes.get(fm, 0) + 1\n        return {\n            \"size\": len(self.buffer),\n            \"total_added\": self.total_added,\n            \"avg_reward\": sum(rewards) / len(rewards),\n            \"correct_rate\": sum(1 for t in self.buffer if t.is_correct) / len(self.buffer),\n            \"format_rate\": sum(1 for t in self.buffer if t.has_answer_tag) / len(self.buffer),\n            \"failure_modes\": failure_modes,\n        }", "code_hash": "a6ac4b7a7f37f842cb2a48f522901c24", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Hstk", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Actor 3: TrainerActor\n\nThe trainer loads the model, receives batches of trajectories, and computes\npolicy gradient updates. We use **REINFORCE** -- the simplest policy gradient\nmethod. Production systems typically use PPO or GRPO, which are variations that improve\nstability, but the approach looks similar from a systems perspective. In other words,\nREINFORCE lets us focus on the *system* (actors, weight sync, async coordination) rather\nthan the algorithm.\n\nThe loss for each trajectory is:\n```\nloss = -sum(log_prob(response_token_i)) * (reward - baseline)\n```\n\nThe trainer is the most complex actor, with several responsibilities:\n\n- **`setup()`** \u2014 loads the model onto GPU 0, creates the optimizer, and\n  registers RDMA circular buffer slots\n- **`train_step()`** \u2014 REINFORCE policy gradient on a batch of trajectories\n- **`get_weight_handle()`** \u2014 returns an RDMA handle to the current circular\n  buffer slot for generators to pull from\n- **`evaluate_zorplex()`** \u2014 runs deterministic evaluation for before/after comparison\n\n**GPU assignment note:** Monarch doesn't assign GPUs automatically \u2014\n`spawn_procs` creates processes, but it's up to you to set\n`CUDA_VISIBLE_DEVICES` in `setup()`. Here, the trainer hardcodes GPU 0\nand generators use GPU 1+.\n\n**Circular buffer with CPU staging** (from [NB07](./07_rdma_weight_sync.html)): After each training step,\nweights are copied GPU -\u003E CPU into a circular buffer slot. Generators read\nfrom CPU via RDMA, then copy to their own GPU. This decouples training from\nweight distribution.\n\n```\nTrainer GPU --D2H--\u003E CPU slot[v % 3] --RDMA--\u003E Generator CPU staging --H2D--\u003E Generator GPU\n```\n\nEach slot is a single **contiguous** CPU buffer \u2014 all parameters packed\nend-to-end. This means one RDMA read transfers the entire model. An\nalternative is keeping parameters scattered and batching reads with\n`RDMAAction`. We go into the different patterns and trade-offs in [NB07b](./07b_weight_sync_deep_dive.html).\n\"\"\")", "code_hash": "84a1e4e45c5ac15cbaaa182388830fb4", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "nWHF", "name": "_"}, {"code": "class TrainerActor(Actor):\n    \"\"\"Trains the model on trajectories.\n\n    Uses setup() for heavy initialization (model loading, RDMA registration).\n    Implements circular buffer with CPU staging for weight distribution.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"Qwen/Qwen2.5-0.5B-Instruct\",\n        lr: float = 1e-5,\n        device: str = \"cuda\",\n        n_buffer_slots: int = 3,\n    ):\n        # Lightweight init - just store config\n        self.model_name = model_name\n        self.lr = lr\n        self.device_config = device\n        self.n_buffer_slots = n_buffer_slots\n        self.rank = current_rank().rank\n        self._ready = False\n        print(f\"[Trainer:{self.rank}] Spawned, waiting for setup()...\")\n\n    @endpoint\n    def setup(self) -\u003E dict:\n        \"\"\"Heavy initialization: load model, create optimizer, set up circular buffer.\"\"\"\n        import os\n\n        if self._ready:\n            return {\"status\": \"already_ready\"}\n\n        # Trainer always uses GPU 0\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.policy_version = 0\n        self.train_steps = 0\n\n        print(f\"[Trainer:{self.rank}] Loading model {self.model_name} on GPU 0...\")\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            torch_dtype=torch.bfloat16 if self.device == \"cuda\" else torch.float32,\n        ).to(self.device)\n\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n\n        # --- Circular buffer with CPU staging ---\n        # Each slot is a single contiguous CPU buffer that holds ALL model\n        # parameters packed end-to-end. We copy the full state_dict into one\n        # flat region, which means one RDMA read per sync (fewest round trips).\n        #\n        # Alternative: keep parameters scattered (one buffer per param) and\n        # use RDMAAction to batch multiple reads into one operation. See NB07b\n        # for a comparison of the different patterns and trade-offs.\n        total_bytes = sum(p.numel() * p.element_size() for p in self.model.parameters())\n\n        self._slots = [\n            torch.empty(total_bytes, dtype=torch.uint8)\n            for _ in range(self.n_buffer_slots)\n        ]\n\n        self._slot_handles = []\n        if rdma_available() and RDMABuffer is not None:\n            try:\n                for slot in self._slots:\n                    self._slot_handles.append(RDMABuffer(slot))\n                print(f\"[Trainer:{self.rank}] RDMA handles registered for {self.n_buffer_slots} circular buffer slots\")\n            except Exception as e:\n                print(f\"[Trainer:{self.rank}] RDMA registration failed: {e}\")\n                self._slot_handles = []\n\n        self._param_meta = {}\n        offset = 0\n        for name, p in self.model.named_parameters():\n            self._param_meta[name] = (offset, tuple(p.shape), p.dtype)\n            offset += p.numel() * p.element_size()\n\n        self._publish_weights()\n\n        self._ready = True\n        param_count = sum(p.numel() for p in self.model.parameters())\n        print(f\"[Trainer:{self.rank}] Ready! {param_count:,} params, \"\n              f\"RDMA={len(self._slot_handles) \u003E 0}, \"\n              f\"buffer_slots={self.n_buffer_slots}\")\n\n        return {\n            \"status\": \"ready\",\n            \"params\": param_count,\n            \"rdma\": len(self._slot_handles) \u003E 0,\n            \"buffer_slots\": self.n_buffer_slots,\n        }\n\n    def _publish_weights(self):\n        \"\"\"Copy GPU params to the current circular buffer slot (D2H).\"\"\"\n        slot_idx = self.policy_version % self.n_buffer_slots\n        slot = self._slots[slot_idx]\n        for name, p in self.model.named_parameters():\n            off, shape, dtype = self._param_meta[name]\n            nbytes = p.numel() * p.element_size()\n            slot[off:off + nbytes].copy_(\n                p.data.view(-1).view(torch.uint8).cpu(), non_blocking=True\n            )\n        torch.cuda.synchronize()  # Ensure D2H complete before RDMA reads\n\n    @endpoint\n    def get_weight_handle(self) -\u003E tuple:\n        \"\"\"Get RDMA handle for the latest weight slot.\n\n        Returns (handle_or_None, param_meta, version, total_bytes).\n        If RDMA unavailable, handle is None and caller should use get_state_dict().\n        \"\"\"\n        total_bytes = sum(p.numel() * p.element_size() for p in self.model.parameters())\n        if self._slot_handles:\n            slot_idx = self.policy_version % self.n_buffer_slots\n            return self._slot_handles[slot_idx], self._param_meta, self.policy_version, total_bytes\n        return None, self._param_meta, self.policy_version, total_bytes\n\n    @endpoint\n    def get_state_dict(self) -\u003E tuple[dict, int]:\n        \"\"\"Fallback: get state dict directly (when RDMA not available).\"\"\"\n        return self.model.state_dict(), self.policy_version\n\n    @endpoint\n    def get_version(self) -\u003E int:\n        return self.policy_version\n\n    @endpoint\n    def train_step(self, trajectories: list[Trajectory], baseline: float) -\u003E TrainMetrics:\n        \"\"\"Train on a batch of trajectories using REINFORCE.\n\n        Each trajectory carries pre-tokenized input_ids and a prompt_length\n        boundary from the generator, so we just slice and compute log-probs.\n        \"\"\"\n        if len(trajectories) == 0:\n            return TrainMetrics(\n                step=self.train_steps, loss=0.0, batch_size=0,\n                avg_reward=0.0, policy_version=self.policy_version,\n            )\n\n        self.model.train()\n        self.optimizer.zero_grad()\n\n        losses = []\n        valid_count = 0\n\n        for traj in trajectories:\n            if not traj.input_ids or traj.prompt_length == 0:\n                continue\n\n            # Step 1: Load pre-tokenized sequence from the generator\n            full_ids = torch.tensor(traj.input_ids, device=self.device).unsqueeze(0)\n            prompt_len = traj.prompt_length\n\n            if full_ids.shape[1] \u003C= prompt_len + 1:\n                continue\n\n            # Step 2-3: Forward pass, then slice at prompt_length for response-only log-probs\n            with torch.amp.autocast('cuda', enabled=self.device == \"cuda\"):\n                logits = self.model(full_ids).logits\n\n            # logits[i] predicts token[i+1], so start at prompt_len - 1\n            shift_logits = logits[:, prompt_len - 1:-1, :]\n            shift_labels = full_ids[:, prompt_len:]\n            log_probs = F.log_softmax(shift_logits, dim=-1)\n            token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n\n            # Step 4: REINFORCE loss = -log_prob * advantage\n            advantage = traj.reward - baseline\n            losses.append(-token_log_probs.sum() * advantage)\n            valid_count += 1\n\n        # Step 5: Optimizer step, then publish weights to circular buffer\n        if valid_count \u003E 0:\n            avg_loss = torch.stack(losses).sum() / valid_count\n            avg_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n            self.optimizer.step()\n        else:\n            avg_loss = torch.tensor(0.0)\n\n        # Bump version, then publish weights to the new slot.\n        # Safe because Monarch actors process endpoints sequentially.\n        self.policy_version += 1\n        self._publish_weights()\n        self.train_steps += 1\n\n        avg_reward = sum(t.reward for t in trajectories) / len(trajectories)\n        correct_rate = sum(1 for t in trajectories if t.is_correct) / len(trajectories)\n        format_rate = sum(1 for t in trajectories if t.has_answer_tag) / len(trajectories)\n\n        return TrainMetrics(\n            step=self.train_steps,\n            loss=avg_loss.item() if torch.is_tensor(avg_loss) else avg_loss,\n            batch_size=len(trajectories),\n            avg_reward=avg_reward,\n            policy_version=self.policy_version,\n            correct_rate=correct_rate,\n            format_rate=format_rate,\n        )\n\n    @endpoint\n    def evaluate_zorplex(self, num_samples: int = 10, seed: int = 42) -\u003E dict:\n        \"\"\"Evaluate current model on compositional Zorplex tasks.\"\"\"\n        import re as _re\n        self.model.eval()\n        torch.manual_seed(seed)  # Deterministic evaluation\n        spec = get_spec(\"compositional\", seed=seed)\n        correct = 0\n        total_turns = 0\n        total_tools = 0\n        format_ok = 0\n        failure_modes = {\"success\": 0, \"wrong_format\": 0, \"tool_spam\": 0, \"wrong_answer\": 0}\n        for _ in range(num_samples):\n            task = spec.generate_task()\n            result = generate_with_tools(\n                self.model, self.tokenizer, spec, task,\n                self.device, max_turns=5,\n                temperature=0.0, do_sample=False,\n            )\n            correct += int(result.is_correct)\n            total_turns += len(result.turns)\n            total_tools += result.total_tool_calls\n            has_tag = bool(_re.search(r'\\[ANSWER\\]', result.final_text))\n            format_ok += int(has_tag)\n            if result.is_correct:\n                failure_modes[\"success\"] += 1\n            elif not has_tag:\n                failure_modes[\"wrong_format\"] += 1\n            elif result.total_tool_calls \u003E 3:\n                failure_modes[\"tool_spam\"] += 1\n            else:\n                failure_modes[\"wrong_answer\"] += 1\n        return {\n            \"accuracy\": correct / num_samples,\n            \"correct\": correct,\n            \"total\": num_samples,\n            \"avg_turns\": total_turns / num_samples,\n            \"avg_tools\": total_tools / num_samples,\n            \"format_rate\": format_ok / num_samples,\n            \"failure_modes\": failure_modes,\n        }", "code_hash": "9e7e0a5b383dec5068cbb6276f1bebb4", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "iLit", "name": "_"}, {"code": "mo.md(r\"\"\"\n### How `train_step` Works\n\nEach trajectory arrives with pre-tokenized `input_ids` and a `prompt_length`\nboundary (computed by the generator at generation time). The trainer:\n\n1. **Loads the token sequence** directly from `traj.input_ids` -- no re-tokenization.\n2. **Slices at `prompt_length`** to separate prompt from response tokens.\n3. **Computes log-probs** on response tokens only (`logits[i]` predicts `token[i+1]`,\n   so we start at `prompt_length - 1`).\n4. **Computes loss**: `loss = -sum(log_probs) * advantage` where\n   `advantage = reward - baseline`. Positive advantage reinforces the response.\n5. **Steps the optimizer** once for the whole batch, then publishes new weights\n   to the circular buffer.\n\nLook for the `# Step N:` comments in `train_step` above -- they correspond to\nthese steps.\n\"\"\")", "code_hash": "ef3666a5e9fb8f300fd0f4a7ea826e47", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ZHCJ", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Actor 4: GeneratorWorker\n\nEach generator loads its own copy of the model, generates its own tasks from\nits seeded spec, and runs inference independently. The key endpoint is\n`generate_trajectory()` -- it generates a task, runs multi-turn inference\nwith tool execution, and returns a complete `Trajectory` with pre-tokenized\n`input_ids` and `prompt_length` for the trainer.\n\n**Reward shaping.** Instead of a binary 0/1 reward, we decompose rewards from\nthe failure modes identified in [NB05](./05_rl_intro.html):\n\n| Component | Value | Why |\n|-----------|-------|-----|\n| Correct answer | +1.0 | The main signal |\n| Format compliance (`[ANSWER]` tag) | +0.2 | Learnable even when wrong |\n| Tool spam penalty | -0.1 per call beyond 2 | Discourages degenerate loops |\n\nThis means a correct, well-formatted response earns up to 1.2, while a\nformat-only success (wrong answer but used `[ANSWER]`) earns 0.2. The\ngradient signal is richer than binary: the model gets *partial credit* for\ngood formatting even before it learns the right answers.\n\nWeight sync uses the pattern from [NB07](./07_rdma_weight_sync.html): the trainer publishes weights to CPU\nslots (circular buffer), and generators pull via RDMA into a CPU staging\nbuffer, then scatter into GPU parameters (H2D copy). Ideally we'd load\ndirectly from the trainer's CPU buffer into the model's `state_dict` to\navoid the extra copy, but we hit `RDMABuffer` bugs doing that \u2014 will fix.\nFallback path (`sync_weights` using `state_dict`) stays for when RDMA is unavailable.\n\"\"\")", "code_hash": "6075ebc58a19ce15723e86fcd43fabbb", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ROlb", "name": "_"}, {"code": "class GeneratorWorker(Actor):\n    \"\"\"Individual generator worker.\n\n    Uses setup() for heavy initialization (model loading).\n    Weight sync uses CPU staging buffer for explicit RDMA -\u003E H2D flow.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"Qwen/Qwen2.5-0.5B-Instruct\",\n        difficulty: str = \"easy\",\n        device: str = \"cuda\",\n    ):\n        # Lightweight init - just store config\n        self.model_name = model_name\n        self.difficulty = difficulty\n        self.device_config = device\n        self.rank = current_rank().rank\n        self._ready = False\n        print(f\"[GeneratorWorker:{self.rank}] Spawned, waiting for setup()...\")\n\n    @endpoint\n    def setup(self) -\u003E dict:\n        \"\"\"Heavy initialization: load model, create weight buffer.\"\"\"\n        import os\n\n        if self._ready:\n            return {\"status\": \"already_ready\"}\n\n        # Generators use GPU 1 + rank (trainer uses GPU 0)\n        gpu_id = 1 + self.rank\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.policy_version = 0\n        self.generations = 0\n\n        print(f\"[GeneratorWorker:{self.rank}] Loading model {self.model_name} on GPU {gpu_id}...\")\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            torch_dtype=torch.bfloat16 if self.device == \"cuda\" else torch.float32,\n        ).to(self.device)\n\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self.spec = get_spec(\"compositional\", difficulty=self.difficulty, seed=42 + self.rank)\n\n        self._sync_buf = None  # CPU staging buffer for RDMA weight sync\n\n        self._ready = True\n        print(f\"[GeneratorWorker:{self.rank}] Ready on GPU {gpu_id}!\")\n\n        return {\"status\": \"ready\", \"rank\": self.rank, \"gpu\": gpu_id}\n\n    @endpoint\n    def get_version(self) -\u003E int:\n        return self.policy_version\n\n    @endpoint\n    def sync_weights_from_buffer(self, handle, param_meta: dict, version: int, total_bytes: int) -\u003E bool:\n        \"\"\"Sync weights via RDMA from trainer's circular buffer.\n\n        Flow: Trainer CPU slot --RDMA--\u003E Generator CPU staging --H2D--\u003E Generator GPU params\n\n        NOTE: Ideally we'd load weights from the trainer's CPU buffer directly\n        into the model's state_dict parameters, avoiding the intermediate copy.\n        We hit bugs with RDMABuffer targeting model tensors directly, so for\n        now we read into a separate CPU buffer and then do a H2D copy. Will fix.\n        \"\"\"\n        if version \u003C= self.policy_version:\n            return False\n\n        # Allocate CPU staging buffer on first sync (reuse thereafter).\n        # Ideally we'd load from the trainer's CPU buffer straight into the\n        # model's state_dict to skip this copy, but we hit RDMABuffer bugs\n        # doing that -- so for now, separate CPU buffer + H2D scatter.\n        if self._sync_buf is None or self._sync_buf.numel() \u003C total_bytes:\n            self._sync_buf = torch.empty(total_bytes, dtype=torch.uint8)\n\n        # RDMA read: trainer CPU slot -\u003E generator CPU staging buffer\n        byte_view = self._sync_buf[:total_bytes].flatten()\n        handle.read_into(byte_view).get()\n\n        # Scatter from CPU staging into GPU model params (H2D copy per parameter)\n        for name, p in self.model.named_parameters():\n            off, shape, dtype = param_meta[name]\n            nbytes = p.numel() * p.element_size()\n            src = self._sync_buf[off:off + nbytes].view(dtype).view(shape)\n            p.data.copy_(src)\n        self.policy_version = version\n        return True\n\n    @endpoint\n    def sync_weights(self, state_dict: dict, version: int) -\u003E bool:\n        \"\"\"Sync weights directly (fallback when RDMA unavailable).\"\"\"\n        if version \u003C= self.policy_version:\n            return False\n        self.model.load_state_dict(state_dict)\n        self.policy_version = version\n        return True\n\n    @endpoint\n    def generate(self, question: str, answer: int, max_turns: int = 5) -\u003E Trajectory:\n        \"\"\"Generate a trajectory for a given task (used for examples/debugging).\"\"\"\n        self.model.eval()\n        task = Task(question=question, correct_answer=answer, metadata={})\n        return self._run_generation(task, max_turns)\n\n    @endpoint\n    def generate_trajectory(self, max_turns: int = 5) -\u003E Trajectory:\n        \"\"\"Generate a trajectory using a self-generated task.\n\n        Each generator has its own seeded spec, so broadcasting this endpoint\n        to all generators produces diverse trajectories from different tasks.\n        \"\"\"\n        self.model.eval()\n        task = self.spec.generate_task()\n        return self._run_generation(task, max_turns)\n\n    def _run_generation(self, task: Task, max_turns: int) -\u003E Trajectory:\n        \"\"\"Shared generation logic: run inference, compute tokens, return Trajectory.\"\"\"\n        import re as _re\n\n        result = generate_with_tools(\n            self.model, self.tokenizer, self.spec, task, self.device,\n            max_turns=max_turns, max_tokens_per_turn=150,\n        )\n\n        self.generations += 1\n\n        # Build model-only text (generated tokens without injected tool results)\n        model_only_text = \"\".join(t.generated_text for t in result.turns)\n\n        # Detect [ANSWER] tag and classify failure mode\n        has_answer_tag = bool(_re.search(r'\\[ANSWER\\]', result.final_text))\n        if result.is_correct:\n            failure_mode = \"success\"\n        elif not has_answer_tag:\n            failure_mode = \"wrong_format\"\n        elif result.total_tool_calls \u003E 3:\n            failure_mode = \"tool_spam\"\n        else:\n            failure_mode = \"wrong_answer\"\n\n        # Pre-tokenize for the trainer: prompt + model_only_text\n        messages = [\n            {\"role\": \"system\", \"content\": self.spec.get_system_prompt(with_hint=True)},\n            {\"role\": \"user\", \"content\": task.question},\n        ]\n        prompt_text = self.tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        prompt_ids = self.tokenizer(\n            prompt_text, return_tensors=\"pt\", add_special_tokens=False\n        )[\"input_ids\"]\n        prompt_length = prompt_ids.shape[1]\n\n        full_ids = self.tokenizer(\n            prompt_text + model_only_text,\n            return_tensors=\"pt\",\n            add_special_tokens=False,\n            truncation=True,\n            max_length=1024,\n        )[\"input_ids\"]\n\n        # Reward shaping (see NB05 \"From Failure Modes to RL Rewards\"):\n        #   +1.0 for correct answer\n        #   +0.2 for format compliance ([ANSWER] tag)\n        #   -0.1 per tool call beyond 2 (discourages tool spam)\n        reward = 0.0\n        if result.is_correct:\n            reward += 1.0\n        if has_answer_tag:\n            reward += 0.2\n        excess_tools = max(0, result.total_tool_calls - 2)\n        reward -= 0.1 * excess_tools\n\n        return Trajectory(\n            task_question=task.question,\n            task_answer=task.correct_answer,\n            response_text=result.final_text,\n            reward=reward,\n            is_correct=result.is_correct,\n            num_turns=len(result.turns),\n            num_tool_calls=result.total_tool_calls,\n            generator_id=self.rank,\n            policy_version=self.policy_version,\n            model_only_text=model_only_text,\n            has_answer_tag=has_answer_tag,\n            failure_mode=failure_mode,\n            input_ids=full_ids[0].tolist(),\n            prompt_length=prompt_length,\n        )", "code_hash": "b43243d596b9b4e3119d2a4f7dd44a2d", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "qnkX", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Architecture Overview\n\nNow we have all our actors defined. Here's how they connect -- this is the\n**single-controller paradigm** from [NB01](./01_history_and_vision.html): the notebook process orchestrates\neverything, but actors do the heavy lifting on their own GPUs.\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  GeneratorMesh      \u2502         \u2502  ZorplexService     \u2502\n\u2502  (ActorMesh)        \u2502         \u2502  (Service)          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  tool   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Generator 0   \u2502\u2500\u2500\u253c\u2500calls\u2500\u2500\u25ba\u2502  \u2502 ZorplexWorker \u2502  \u2502\n\u2502  \u2502 Generator 1   \u2502\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502  \u2502 ZorplexWorker \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\u25c4\u2500results\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         \u2502           \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502 trajectories\n          v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    ReplayBuffer     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502 sample batch\n           v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Trainer        \u2502\n\u2502  (circular buffer)  \u2502\u2500\u2500\u003E RDMA weight sync\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      to GeneratorMesh\n```\n\nEach generator calls zorplex tool endpoints during multi-turn inference\n(e.g., `lookup_value`, `compute`). The Service routes these calls round-robin\nacross ZorplexWorkers.\n\n**ActorMesh vs Service.** Generators are a plain **ActorMesh** -- we address\nthem directly via `.call()` (broadcast to all) or `.slice()` (individual\naccess). This is natural for sync RL (broadcast generate, then train) and\nfor async RL (each thread slices its own generator). ZorplexWorkers are\nwrapped in a **Service** ([NB06](./06_services.html) pattern) because they're stateless: any\nworker can handle any request, so round-robin routing and health tracking\nare useful. In production async RL, you might wrap generators in a Service\ntoo -- that gives you auto-scaling and health tracking -- but here the\nActorMesh is simpler and lets us demonstrate both addressing patterns.\n\"\"\")", "code_hash": "7608b837715e9d9adfb6faf3936c9879", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "TqIu", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Sync vs Async RL\n\n**Sync RL** (traditional):\n```\n|--generate--|--train--|--generate--|--train--|--generate--|--train--|\n```\nOnly ONE thing happens at a time. GPU sits idle during generation,\ngenerator sits idle during training.\n\n**Async RL** (what we're building):\n```\nGen0:  |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\nGen1:  |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\nTrain:      |\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593|\n```\nEverything runs concurrently. More data collected, better GPU utilization.\n\nWe'll run BOTH modes with the **same actors** and compare wall time, throughput,\nand utilization.\n\"\"\")", "code_hash": "cac9da9ca1bd3826773909f78152bebe", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Vxnm", "name": "_"}, {"code": "num_steps_slider = mo.ui.slider(10, 100, value=20, label=\"Training steps\")\nnum_generators_slider = mo.ui.slider(1, 4, value=2, label=\"Generators\")\n\nmo.md(f\"\"\"\n## Configuration\n\nAdjust parameters for the training run. **Marimo is reactive**: changing a slider\nre-runs all downstream cells that depend on it. This means actors will be\nre-spawned and both training loops will re-execute with the new values.\n\n{num_steps_slider}\n\n{num_generators_slider}\n\n**Batch size** is set to match the number of generators -- each training step\ntrains on exactly one round of generation. This keeps the comparison fair:\nsync and async train on the same amount of data per step.\n\n**Suggestions:** Start with defaults (20 steps, 2 generators) to see the\nfull pipeline. Then try increasing generators to 3-4 to see the async throughput\nadvantage grow. Increasing training steps gives the model more updates but adds\nwall time. Note that re-spawning actors (loading models onto GPUs) is the most\nexpensive part of the setup -- the training loops themselves are relatively fast.\n\n**Try this:** Set generators to 1 and watch the async timeline -- with only one\ngenerator, async degrades to near-sync performance because there's no parallel\ngeneration to overlap with training.\n\"\"\")", "code_hash": "85c314495b9115cfe1c1d9c730e7a2b2", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "DnEU", "name": "_"}, {"code": "import threading\nimport time\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass TimingEvent:\n    \"\"\"A single timed event for timeline visualization.\"\"\"\n    actor_id: str\n    event_type: str  # \"generate\", \"train\", \"sync\"\n    start_time: float\n    duration: float\n\n@dataclass\nclass TimingStats:\n    \"\"\"Timing statistics for a training run.\"\"\"\n    mode: str\n    num_generators: int\n    num_steps: int\n    total_generations: int\n    wall_time: float\n    gen_times: list = field(default_factory=list)\n    train_times: list = field(default_factory=list)\n    events: list = field(default_factory=list)  # List of TimingEvent\n    rdma_syncs: int = 0\n    direct_syncs: int = 0\n    staleness: list = field(default_factory=list)  # policy_version gaps per train batch\n\n    @property\n    def gens_per_second(self) -\u003E float:\n        return self.total_generations / self.wall_time if self.wall_time \u003E 0 else 0\n\n    @property\n    def steps_per_second(self) -\u003E float:\n        return self.num_steps / self.wall_time if self.wall_time \u003E 0 else 0", "code_hash": "609c448f633ccd864db86c56b25225fc", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ulZA", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Spawning and Initializing Actors\n\nThis is the **single-controller paradigm** in action. The notebook process\norchestrates a careful initialization sequence:\n\n1. Spawn ZorplexWorkers via a **Service** ([NB06](./06_services.html) pattern -- health tracking, round-robin)\n2. Spawn GeneratorWorkers as a plain **ActorMesh** and call `setup()` on all via\n   `.call()` broadcast (loads model onto each GPU)\n3. Spawn ReplayBuffer (CPU-only, ready immediately)\n4. Spawn Trainer, then call `setup()` (loads model onto GPU 0, registers RDMA buffers)\n\"\"\")", "code_hash": "bf4586ecfdc747dcb0276e3471105b8f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ecfG", "name": "_"}, {"code": "from monarch.actor import this_host\n\nNUM_STEPS = num_steps_slider.value\nNUM_GENERATORS = num_generators_slider.value\nNUM_ZORPLEX = 2\nBATCH_SIZE = NUM_GENERATORS  # Train on exactly one round of generation per step\n\ndef setup_actors():\n    \"\"\"Spawn and initialize all actors. Returns them for reuse.\"\"\"\n    host = this_host()\n\n    # 1. ZorplexWorkers -- wrapped in a Service (NB06 pattern) for\n    #    health tracking and round-robin routing\n    zorplex_worker_procs = host.spawn_procs(per_host={\"procs\": NUM_ZORPLEX})\n    zorplex_svc_procs = host.spawn_procs(per_host={\"procs\": 1})\n    zorplex_svc = zorplex_svc_procs.spawn(\"zorplex_svc\", Service,\n        service_name=\"zorplex\", worker_class=ZorplexWorker,\n        procs=zorplex_worker_procs, procs_per_replica=1,\n        difficulty=\"easy\")\n\n    # 2. Generators -- plain ActorMesh (no Service wrapper).\n    #    Each generator has its own GPU and model copy; we address them\n    #    via .call() broadcast or .slice() for individual access.\n    gen_procs = host.spawn_procs(per_host={\"procs\": NUM_GENERATORS})\n    generators = gen_procs.spawn(\"generators\", GeneratorWorker)\n\n    # 3. ReplayBuffer\n    buffer_procs = host.spawn_procs(per_host={\"procs\": 1})\n    buffer = buffer_procs.spawn(\"buffer\", ReplayBuffer, max_size=500)\n\n    # 4. Trainer\n    trainer_procs = host.spawn_procs(per_host={\"procs\": 1})\n    trainer = trainer_procs.spawn(\"trainer\", TrainerActor)\n\n    # Initialize actors that need setup\n    zorplex_svc.ping.call_one().get()\n\n    print(\"[SETUP] Setting up generator workers...\")\n    generators.setup.call().get()  # broadcast setup to all generators\n\n    buffer.stats.call_one().get()\n\n    print(\"[SETUP] Setting up trainer...\")\n    trainer.setup.call_one().get()\n\n    register_service(\"zorplex\", zorplex_svc)\n\n    print(f\"[SETUP] All actors ready! {NUM_GENERATORS} generators, {NUM_ZORPLEX} zorplex workers\")\n\n    # Track ProcMeshes for cleanup\n    proc_meshes = [zorplex_worker_procs, zorplex_svc_procs, gen_procs, buffer_procs, trainer_procs]\n\n    return {\n        \"trainer\": trainer,\n        \"buffer\": buffer,\n        \"generators\": generators,\n        \"zorplex_svc\": zorplex_svc,\n        \"_proc_meshes\": proc_meshes,\n    }\n\ndef teardown_actors(actors):\n    \"\"\"Stop all ProcMeshes, releasing processes and GPU memory.\"\"\"\n    for pm in actors.get(\"_proc_meshes\", []):\n        try:\n            pm.stop(\"teardown for re-init\").get()\n        except Exception:\n            pass  # Best-effort cleanup\n    print(\"[TEARDOWN] All actors stopped.\")\n\nactors = setup_actors()", "code_hash": "0942732c887af3453a85a25400884725", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Pvdt", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Before Training: Zorplex Baseline\n\nLet's evaluate the model *before* any training to establish a baseline.\nWe run 10 compositional Zorplex tasks and record accuracy, average turns,\nand tool usage. This gives us a concrete \"before\" snapshot to compare against.\n\"\"\")", "code_hash": "476adcec390e4d09fb048f4c447bb22f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ZBYS", "name": "_"}, {"code": "print(\"Evaluating pre-training baseline...\")\npre_eval = actors[\"trainer\"].evaluate_zorplex.call_one(num_samples=10, seed=42).get()\n\nmo.md(f\"\"\"\n### Pre-Training Results\n\n**Metrics refresher** (from [NB05](./05_rl_intro.html)): *Accuracy* is how often the model gets the\ncorrect answer. *Format compliance* tracks whether it emits the `[ANSWER]` tag\nwe trained it to use. *Avg turns/tool calls* measure how many interaction\nsteps the model takes \u2014 lower is more efficient.\n\n| Metric | Value |\n|--------|-------|\n| Accuracy | {pre_eval['accuracy']:.0%} ({pre_eval['correct']}/{pre_eval['total']}) |\n| Format compliance | {pre_eval['format_rate']:.0%} |\n| Avg turns | {pre_eval['avg_turns']:.1f} |\n| Avg tool calls | {pre_eval['avg_tools']:.1f} |\n\n**Failure mode breakdown:**\n\n| Mode | Count |\n|------|-------|\n| Success | {pre_eval['failure_modes']['success']} |\n| Wrong format | {pre_eval['failure_modes']['wrong_format']} |\n| Tool spam | {pre_eval['failure_modes']['tool_spam']} |\n| Wrong answer | {pre_eval['failure_modes']['wrong_answer']} |\n\nThis is our starting point. Let's see if training improves it at all...\n\"\"\")", "code_hash": "33240f80c23ace120bb89a36cc7a1f90", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "aLJB", "name": "_"}, {"code": "# Generate one example trajectory so the reader can see what the pipeline produces\n_gen = actors[\"generators\"].slice(procs=0)\n_example_traj = _gen.generate_trajectory.call_one().get()\n\n_status = \"Correct\" if _example_traj.is_correct else \"Wrong\"\n_reward = _example_traj.reward\n\n# Truncate long responses for display\n_resp_display = _example_traj.response_text[:500]\nif len(_example_traj.response_text) \u003E 500:\n    _resp_display += \"...\"\n\nmo.md(f\"\"\"\n### Example Trajectory\n\nHere's what a single generation looks like -- this is the data unit flowing\nthrough the pipeline:\n\n| Field | Value |\n|-------|-------|\n| Question | {_example_traj.task_question[:100]}... |\n| Correct answer | `{_example_traj.task_answer}` |\n| Result | **{_status}** (reward={_reward:.2f}) |\n| Failure mode | `{_example_traj.failure_mode}` |\n| Format (`[ANSWER]` tag) | {\"Yes\" if _example_traj.has_answer_tag else \"No\"} |\n| Turns | {_example_traj.num_turns} |\n| Tool calls | {_example_traj.num_tool_calls} |\n\n**Model response** (first 500 chars):\n```\n{_resp_display}\n```\n\nEach generator produces trajectories like this, which flow into the replay buffer\nfor the trainer to sample from.\n\"\"\")", "code_hash": "a0dcf4a953ef2722683c73f791068743", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "nHfw", "name": "_"}, {"code": "def run_sync_loop(actors) -\u003E TimingStats:\n    \"\"\"\n    SYNC MODE: Broadcast generate to all generators, then train.\n    Pattern: generate batch -\u003E train -\u003E generate batch -\u003E train ...\n\n    Uses .call() to broadcast generate_trajectory to all generators\n    simultaneously. Each generator produces a different trajectory\n    (different seed, stochastic sampling), but the call is synchronous --\n    we wait for ALL generators to finish before training.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SYNC MODE: Broadcast Generate -\u003E Train\")\n    print(\"=\" * 60)\n\n    trainer = actors[\"trainer\"]\n    generators = actors[\"generators\"]\n\n    stats = TimingStats(\n        mode=\"SYNC\",\n        num_generators=NUM_GENERATORS,\n        num_steps=NUM_STEPS,\n        total_generations=0,\n        wall_time=0,\n    )\n\n    baseline = 0.5\n    t0 = time.perf_counter()\n\n    for step in range(NUM_STEPS):\n        # Generate trajectories -- broadcast to ALL generators\n        gen_start = time.perf_counter()\n        traj_mesh = generators.generate_trajectory.call().get()\n\n        # Collect into a plain list -- no buffer in sync mode.\n        # We train on exactly what we just generated, so staleness is 0.\n        # (Async mode uses the buffer because generators and trainer are\n        # decoupled in time -- that's where the buffer matters.)\n        batch = list(traj_mesh.values())\n\n        gen_time = time.perf_counter() - gen_start\n        stats.gen_times.append(gen_time)\n        stats.total_generations += NUM_GENERATORS\n\n        # Record one event per generator (they ran in parallel)\n        for gi in range(NUM_GENERATORS):\n            stats.events.append(TimingEvent(\n                actor_id=f\"Gen{gi}\",\n                event_type=\"generate\",\n                start_time=gen_start - t0,\n                duration=gen_time,\n            ))\n\n        # Train directly on the batch we just generated (no buffer)\n        train_start = time.perf_counter()\n        if batch:\n            metrics = trainer.train_step.call_one(batch, baseline).get()\n            baseline = 0.9 * baseline + 0.1 * metrics.avg_reward\n\n            # Staleness should be 0: we generated with current policy and\n            # train immediately. This contrasts with async mode.\n            batch_staleness = [metrics.policy_version - t.policy_version for t in batch]\n            stats.staleness.extend(batch_staleness)\n\n            # Sync weights to all generators (broadcast)\n            try:\n                handle, param_meta, version, total_bytes = trainer.get_weight_handle.call_one().get()\n                if handle is not None:\n                    generators.sync_weights_from_buffer.call(handle, param_meta, version, total_bytes).get()\n                else:\n                    state_dict, ver = trainer.get_state_dict.call_one().get()\n                    generators.sync_weights.call(state_dict, ver).get()\n            except Exception:\n                pass  # Non-fatal: generators will use slightly stale weights\n\n        train_time = time.perf_counter() - train_start\n        stats.train_times.append(train_time)\n        stats.events.append(TimingEvent(\n            actor_id=\"Train\",\n            event_type=\"train\",\n            start_time=train_start - t0,\n            duration=train_time,\n        ))\n\n        correct_count = sum(1 for t in traj_mesh.values() if t.is_correct)\n        format_count = sum(1 for t in traj_mesh.values() if t.has_answer_tag)\n        print(f\"[SYNC {step + 1:2d}] {correct_count}/{NUM_GENERATORS} correct \"\n              f\"{format_count}/{NUM_GENERATORS} formatted \"\n              f\"gen={gen_time * 1000:.0f}ms train={train_time * 1000:.0f}ms\")\n\n    stats.wall_time = time.perf_counter() - t0\n    return stats", "code_hash": "6e20128924fe44882de16035d62c36fe", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "xXTn", "name": "_"}, {"code": "def run_async_loop(actors) -\u003E TimingStats:\n    \"\"\"\n    ASYNC MODE: All generators running concurrently with trainer.\n    - 1 thread per generator (each uses .slice() to address its generator)\n    - Training in main thread\n    - Each generator pulls latest weights before each trajectory\n\n    Uses try/except pattern from NB03 for fault tolerance in generation loops.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"ASYNC MODE: {NUM_GENERATORS} Generators + 1 Trainer (Concurrent)\")\n    print(\"=\" * 60)\n\n    trainer = actors[\"trainer\"]\n    buffer = actors[\"buffer\"]\n    generators = actors[\"generators\"]\n\n    stats = TimingStats(\n        mode=\"ASYNC\",\n        num_generators=NUM_GENERATORS,\n        num_steps=NUM_STEPS,\n        total_generations=0,\n        wall_time=0,\n    )\n\n    lock = threading.Lock()\n    stop_flag = threading.Event()\n    t0 = time.perf_counter()\n\n    def generation_loop(gen_idx):\n        \"\"\"Each generator gets its own thread, using .slice() for individual access.\"\"\"\n        gen = generators.slice(procs=gen_idx)\n        while not stop_flag.is_set():\n            gen_start = time.perf_counter()\n\n            try:\n                # Pull latest weights before generating.\n                # sync_weights_from_buffer short-circuits if version\n                # hasn't changed, so this is cheap when there's nothing new.\n                handle, param_meta, version, total_bytes = trainer.get_weight_handle.call_one().get()\n                if handle is not None:\n                    synced = gen.sync_weights_from_buffer.call_one(handle, param_meta, version, total_bytes).get()\n                    if synced:\n                        with lock:\n                            stats.rdma_syncs += 1\n                else:\n                    state_dict, ver = trainer.get_state_dict.call_one().get()\n                    synced = gen.sync_weights.call_one(state_dict, ver).get()\n                    if synced:\n                        with lock:\n                            stats.direct_syncs += 1\n\n                # Generate trajectory\n                traj = gen.generate_trajectory.call_one().get()\n                buffer.add.call_one(traj).get()\n\n                gen_time = time.perf_counter() - gen_start\n                with lock:\n                    stats.gen_times.append(gen_time)\n                    stats.total_generations += 1\n                    count = stats.total_generations\n                    stats.events.append(TimingEvent(\n                        actor_id=f\"Gen{gen_idx}\",\n                        event_type=\"generate\",\n                        start_time=gen_start - t0,\n                        duration=gen_time,\n                    ))\n\n                status = \"correct\" if traj.is_correct else traj.failure_mode\n                print(f\"[GEN{gen_idx} #{count:2d}] {status} gen={gen_time * 1000:.0f}ms\")\n\n            except Exception as e:\n                # try/except pattern from NB03 -- log and continue\n                print(f\"[GEN{gen_idx}] Error: {e}, retrying...\")\n                continue\n\n    # Start 1 thread per generator, each using .slice() for its worker\n    gen_threads = []\n    for idx in range(NUM_GENERATORS):\n        t = threading.Thread(target=generation_loop, args=(idx,), daemon=True)\n        t.start()\n        gen_threads.append(t)\n\n    # Training in main thread\n    train_steps_done = 0\n    baseline = 0.5\n\n    while train_steps_done \u003C NUM_STEPS:\n        # Wait for enough samples\n        while True:\n            size = buffer.size.call_one().get()\n            if size \u003E= BATCH_SIZE:\n                break\n            time.sleep(0.02)\n\n        train_start = time.perf_counter()\n        batch = buffer.sample.call_one(BATCH_SIZE).get()\n        if batch:\n            metrics = trainer.train_step.call_one(batch, baseline).get()\n            baseline = 0.9 * baseline + 0.1 * metrics.avg_reward\n\n            # Measure staleness: in async mode, trajectories may have been\n            # generated with an older policy version.\n            batch_staleness = [metrics.policy_version - t.policy_version for t in batch]\n            with lock:\n                stats.staleness.extend(batch_staleness)\n\n        train_time = time.perf_counter() - train_start\n        with lock:\n            stats.train_times.append(train_time)\n            stats.events.append(TimingEvent(\n                actor_id=\"Train\",\n                event_type=\"train\",\n                start_time=train_start - t0,\n                duration=train_time,\n            ))\n        train_steps_done += 1\n\n        print(f\"[TRAIN {train_steps_done:2d}] time={train_time * 1000:.0f}ms buffer={size}\")\n\n    stop_flag.set()\n\n    for t in gen_threads:\n        t.join(timeout=2.0)\n\n    stats.wall_time = time.perf_counter() - t0\n    return stats", "code_hash": "dcdcd02377d0ebb18af0cef7cda831d2", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "AjVT", "name": "_"}, {"code": "sync_stats = run_sync_loop(actors)\nprint(f\"\\nSync complete: {sync_stats.wall_time:.2f}s, \"\n      f\"{sync_stats.total_generations} generations, \"\n      f\"{sync_stats.gens_per_second:.2f} gens/s\")\n\n# Evaluate immediately after sync training\nprint(\"Evaluating post-sync performance...\")\nsync_post_eval = actors[\"trainer\"].evaluate_zorplex.call_one(num_samples=10, seed=42).get()\nprint(f\"Post-sync accuracy: {sync_post_eval['accuracy']:.0%}\")", "code_hash": "b77f525d8cb685ba488989c212c381ce", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "pHFh", "name": "_"}, {"code": "mo.md(r\"\"\"\n### Re-initializing for Async\n\nTo compare fairly, we tear down all actors and re-spawn from scratch so async\ntraining starts from the same untrained baseline. `ProcMesh.stop()` releases\nthe processes and frees GPU memory before we spawn fresh ones.\n\"\"\")", "code_hash": "f3fc1e2cd04479f2764fd72bfa495c9c", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "NCOB", "name": "_"}, {"code": "# Tear down sync actors to free GPU memory\nteardown_actors(actors)\n\n# Re-spawn everything so async starts from the same untrained baseline\nprint(\"Re-spawning actors for async run...\")\nasync_actors = setup_actors()\nprint(\"Actors re-initialized. Starting async loop...\")\n\nasync_stats = run_async_loop(async_actors)\nprint(f\"\\nAsync complete: {async_stats.wall_time:.2f}s, \"\n      f\"{async_stats.total_generations} generations, \"\n      f\"{async_stats.gens_per_second:.2f} gens/s\")\n\n# Evaluate immediately after async training\nprint(\"Evaluating post-async performance...\")\nasync_post_eval = async_actors[\"trainer\"].evaluate_zorplex.call_one(num_samples=10, seed=42).get()\nprint(f\"Post-async accuracy: {async_post_eval['accuracy']:.0%}\")", "code_hash": "bda1a812a94ab293703d0a2366de6aed", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "aqbW", "name": "_"}, {"code": "def _build_comparison(sync_s, async_s) -\u003E str:\n    speedup = sync_s.wall_time / async_s.wall_time if async_s.wall_time \u003E 0 else 0\n    gen_ratio = async_s.gens_per_second / sync_s.gens_per_second if sync_s.gens_per_second \u003E 0 else 0\n\n    avg_sync_gen = sum(sync_s.gen_times) / len(sync_s.gen_times) * 1000 if sync_s.gen_times else 0\n    avg_async_gen = sum(async_s.gen_times) / len(async_s.gen_times) * 1000 if async_s.gen_times else 0\n    avg_sync_train = sum(sync_s.train_times) / len(sync_s.train_times) * 1000 if sync_s.train_times else 0\n    avg_async_train = sum(async_s.train_times) / len(async_s.train_times) * 1000 if async_s.train_times else 0\n\n    async_syncs = async_s.rdma_syncs + async_s.direct_syncs\n\n    return f\"\"\"\n## Sync vs Async Comparison\n\n| Metric | SYNC | ASYNC | Ratio |\n|--------|------|-------|-------|\n| Wall time | {sync_s.wall_time:.2f}s | {async_s.wall_time:.2f}s | **{speedup:.2f}x** speedup |\n| Generations | {sync_s.total_generations} | {async_s.total_generations} | {async_s.total_generations / max(sync_s.total_generations, 1):.1f}x |\n| Gens/second | {sync_s.gens_per_second:.2f} | {async_s.gens_per_second:.2f} | **{gen_ratio:.1f}x** throughput |\n| Avg gen time | {avg_sync_gen:.0f}ms | {avg_async_gen:.0f}ms | |\n| Avg train time | {avg_sync_train:.0f}ms | {avg_async_train:.0f}ms | |\n| Weight syncs | {sync_s.total_generations} (every step) | {async_syncs} (per-generator) | |\n\n### Key Observations\n\n- **Data throughput**: Async collected **{gen_ratio:.1f}x** more trajectories per second.\n  More data means better gradient estimates.\n- **GPU utilization**: In sync mode, the trainer GPU sits idle during generation and\n  vice versa. Async keeps both busy.\n- **Generators ran in parallel**: {async_s.num_generators} generators each had their own\n  thread, producing data independently.\n- The trainer consumed from the replay buffer continuously, never waiting for a specific\n  generator to finish.\n\nIn production with more generators, the throughput advantage grows further.\n\"\"\"\n\ncomparison_md = _build_comparison(sync_stats, async_stats)\nmo.md(comparison_md)", "code_hash": "f26daa12018661cf14d95adc3623c9c4", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "TRpd", "name": "_"}, {"code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\ndef _plot_timeline(stats, ax, title):\n    \"\"\"Plot a Gantt chart of timing events.\"\"\"\n    color_map = {\n        \"generate\": \"#4CAF50\",  # green\n        \"train\": \"#E91E63\",     # pink\n        \"sync\": \"#9C27B0\",      # purple\n    }\n\n    # Collect unique actor IDs and assign y positions\n    actor_ids = []\n    for ev in stats.events:\n        if ev.actor_id not in actor_ids:\n            actor_ids.append(ev.actor_id)\n\n    # Sort: Gen0, Gen1, ..., Train, Sync\n    gen_ids = sorted([a for a in actor_ids if a.startswith(\"Gen\")])\n    other_ids = [a for a in [\"Train\", \"Sync\"] if a in actor_ids]\n    actor_ids = gen_ids + other_ids\n\n    y_map = {aid: i for i, aid in enumerate(actor_ids)}\n\n    for ev in stats.events:\n        if ev.actor_id in y_map:\n            y = y_map[ev.actor_id]\n            color = color_map.get(ev.event_type, \"#999999\")\n            ax.barh(y, ev.duration, left=ev.start_time, height=0.6,\n                    color=color, alpha=0.8, edgecolor=\"white\", linewidth=0.5)\n\n    ax.set_yticks(range(len(actor_ids)))\n    ax.set_yticklabels(actor_ids)\n    ax.set_xlabel(\"Wall time (seconds)\")\n    ax.set_title(title)\n    ax.invert_yaxis()\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=False)\n\n_plot_timeline(sync_stats, ax1, f\"SYNC ({sync_stats.wall_time:.1f}s)\")\n_plot_timeline(async_stats, ax2, f\"ASYNC ({async_stats.wall_time:.1f}s)\")\n\n# Legend\nlegend_patches = [\n    mpatches.Patch(color=\"#4CAF50\", label=\"Generate\"),\n    mpatches.Patch(color=\"#E91E63\", label=\"Train\"),\n]\nfig.legend(handles=legend_patches, loc=\"upper right\", framealpha=0.9)\n\nplt.tight_layout()\n_timeline_desc = mo.md(\"\"\"### Timeline Visualization\n\nThe Gantt charts below show what each actor was doing over time. In sync mode,\nbars are strictly sequential -- notice the gaps between generation and training bars.\nIn async mode, generators and trainer overlap -- that overlap is where the throughput\ngain comes from.\n\n**Try this:** Look at the sync chart and count the idle gaps. Each gap is wasted GPU\ntime. Then look at the async chart -- the trainer bar starts almost immediately because\ngenerators are pre-filling the buffer concurrently.\n\"\"\")\nmo.vstack([_timeline_desc, fig])", "code_hash": "6ef3a1d608e215c8065f609a97024cb1", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "TXez", "name": "_"}, {"code": "def _avg(lst):\n    return sum(lst) / len(lst) if lst else 0.0\n\n_sync_avg = _avg(sync_stats.staleness)\n_async_avg = _avg(async_stats.staleness)\n_async_max = max(async_stats.staleness) if async_stats.staleness else 0\n\nmo.md(f\"\"\"\n### Policy Staleness: The Cost of Async\n\nAsync mode gives us better hardware utilization, but there's a trade-off:\n**policy staleness**. Generators produce trajectories using an older version\nof the policy while the trainer has already moved on. This is *off-policy*\ndata -- the log-probabilities computed during training don't match the policy\nthat generated the trajectory.\n\nWe measure staleness as `trainer_version - trajectory_version` at each\ntraining step:\n\n| Metric | SYNC | ASYNC |\n|--------|------|-------|\n| Avg staleness | {_sync_avg:.1f} | {_async_avg:.1f} |\n| Max staleness | {max(sync_stats.staleness) if sync_stats.staleness else 0} | {_async_max} |\n\nSync mode shows ~0 staleness because we sync weights to generators after\nevery training step. Async mode shows \u003E0 because generators keep producing\nwith older weights while the trainer advances.\n\nWith REINFORCE, this introduces some bias. More sophisticated algorithms\n(PPO, GRPO) address this with importance sampling ratios (`pi_new / pi_old`)\nand clipping, but that's beyond our scope here. For a small model with few\nsteps, the staleness is mild -- and the throughput gain from async more than\ncompensates.\n\"\"\")", "code_hash": "062962b0a775ae3479f416525946cec1", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "dNNg", "name": "_"}, {"code": "mo.md(r\"\"\"\n## After Training: Did It Improve?\n\nWe ran sync and async training **independently** -- each started from the same\nuntrained model (we re-spawned actors between runs). This lets us compare\nboth the throughput characteristics (above) and the training outcomes.\n\nNote: We're using a small model (0.5B) with few training steps, so dramatic\nimprovement isn't guaranteed. The point is the *infrastructure* -- showing that\nthe full loop works end to end.\n\"\"\")", "code_hash": "cc61e0a627bbd4580b6ed8ba45ecb6a7", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "yCnT", "name": "_"}, {"code": "def _delta(post, pre, key):\n    return post[key] - pre[key]\n\ndef _dir(delta):\n    if delta \u003E 0:\n        return \"improved\"\n    elif delta == 0:\n        return \"unchanged\"\n    return \"decreased\"\n\n_sync_acc_d = _delta(sync_post_eval, pre_eval, \"accuracy\")\n_async_acc_d = _delta(async_post_eval, pre_eval, \"accuracy\")\n_sync_fmt_d = _delta(sync_post_eval, pre_eval, \"format_rate\")\n_async_fmt_d = _delta(async_post_eval, pre_eval, \"format_rate\")\n\n_pre_fm = pre_eval[\"failure_modes\"]\n_sync_fm = sync_post_eval[\"failure_modes\"]\n_async_fm = async_post_eval[\"failure_modes\"]\n\nmo.md(f\"\"\"\n### Training Results: Baseline vs Sync vs Async\n\nBoth runs started from the same untrained model and ran for the same number\nof training steps.\n\n| Metric | Baseline | After Sync | After Async |\n|--------|----------|------------|-------------|\n| Accuracy | {pre_eval['accuracy']:.0%} | {sync_post_eval['accuracy']:.0%} ({_sync_acc_d:+.0%}) | {async_post_eval['accuracy']:.0%} ({_async_acc_d:+.0%}) |\n| Format compliance | {pre_eval['format_rate']:.0%} | {sync_post_eval['format_rate']:.0%} ({_sync_fmt_d:+.0%}) | {async_post_eval['format_rate']:.0%} ({_async_fmt_d:+.0%}) |\n| Avg turns | {pre_eval['avg_turns']:.1f} | {sync_post_eval['avg_turns']:.1f} | {async_post_eval['avg_turns']:.1f} |\n| Avg tool calls | {pre_eval['avg_tools']:.1f} | {sync_post_eval['avg_tools']:.1f} | {async_post_eval['avg_tools']:.1f} |\n\n**Failure mode breakdown:**\n\n| Mode | Baseline | After Sync | After Async |\n|------|----------|------------|-------------|\n| Success | {_pre_fm['success']} | {_sync_fm['success']} | {_async_fm['success']} |\n| Wrong format | {_pre_fm['wrong_format']} | {_sync_fm['wrong_format']} | {_async_fm['wrong_format']} |\n| Tool spam | {_pre_fm['tool_spam']} | {_sync_fm['tool_spam']} | {_async_fm['tool_spam']} |\n| Wrong answer | {_pre_fm['wrong_answer']} | {_sync_fm['wrong_answer']} | {_async_fm['wrong_answer']} |\n\nSync accuracy {_dir(_sync_acc_d)} by {abs(_sync_acc_d):.0%}.\nAsync accuracy {_dir(_async_acc_d)} by {abs(_async_acc_d):.0%}.\n\nWith a 0.5B model and only a few training steps, large gains are unlikely.\nThe key result is that the full pipeline works: generation, training,\nweight sync, and evaluation all compose correctly through Monarch actors. The\nfailure mode breakdown shows *where* the model is improving (or not) -- watch\nfor format compliance changes in particular, since that's the easiest RL win.\n\"\"\")", "code_hash": "0d346a7547ae79efa1a5ad23da7002d3", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "wlCL", "name": "_"}, {"code": "mo.md(r\"\"\"\n## What's Happening Under the Hood\n\nWhen you run the training loop, here's what each layer does:\n\n**Actor isolation**: Each actor (trainer, generators, buffer, zorplex workers)\nruns in its own process with its own GPU assignment. `CUDA_VISIBLE_DEVICES` is\nset in `setup()`, not at spawn time -- the `procs` dimension in `spawn_procs`\nis just a dimension name, not a GPU assignment.\n\n**Weight sync data flow** (circular buffer + CPU staging from [NB07](./07_rdma_weight_sync.html)):\n```\nTrainer GPU  --D2H--\u003E  CPU slot[v % 3]  --RDMA--\u003E  Generator CPU staging  --H2D--\u003E  Generator GPU\n```\n- Trainer publishes weights to a circular buffer after each train step\n- Generators pull from the buffer via RDMA into a CPU staging buffer\n- Explicit H2D copy scatters into GPU model parameters\n- The circular buffer has 3 slots, so training never blocks on reads\n- **Future improvement**: ideally we'd load from the trainer's CPU buffer\n  directly into the model's `state_dict`, skipping the staging copy.\n  We hit `RDMABuffer` bugs doing that, so for now we use the extra buffer.\n\n**Async concurrency** (via threads):\n- 1 thread per generator, each using `.slice(procs=i)` to address its generator\n- Each generator pulls latest weights from the trainer before each trajectory\n  (`sync_weights_from_buffer` short-circuits if version hasn't changed)\n- Training in the main thread\n- `threading.Event` coordinates shutdown when training completes\n- GIL is released during I/O (actor calls) and CUDA (GPU compute), so threads\n  achieve real concurrency\n\n**Sync vs Async generation**:\n- Sync mode uses `.call()` broadcast to trigger all generators at once, then waits\n  for all to finish before training\n- Async mode uses `.slice()` per thread so each generator runs independently --\n  no generator waits for another\n\n**Fault tolerance** (from [NB03](./03_fault_tolerance.html)):\n- Generation loops wrap `generate_trajectory.call_one().get()` in `try/except`\n- On failure, the generator logs and retries instead of crashing the loop\n\"\"\")", "code_hash": "c4f4917bb8b869c54595594a0130480b", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "kqZH", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Scaling Up\n\nWhat we built here scales naturally with Monarch:\n\n| Scale | What Changes |\n|-------|--------------|\n| More generators | Increase `num_generators` slider -- spawns larger ActorMesh, `.call()` broadcast scales automatically |\n| More zorplex workers | Increase `NUM_ZORPLEX` -- parallel task generation via Service |\n| Multi-node | Use `SlurmJob` instead of `this_host()` |\n| Better algorithms | Swap REINFORCE for PPO/GRPO (add importance sampling) |\n| Production generators | Wrap generators in a Service too (health tracking, auto-scaling) |\n| More services | Add reward models, search APIs as actors |\n\n**The patterns stay the same:**\n- Actors for isolation and GPU assignment\n- Endpoints for communication (`.call_one().get()`)\n- RDMA + circular buffer for efficient weight transfer\n- Version tracking for consistency across actors\n\nThis is the foundation for which you could build production systems, using monarch at scale.\n\"\"\")", "code_hash": "14bf2752cf457144964fe4d3557c20eb", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "wAgl", "name": "_"}, {"code": "mo.md(r\"\"\"\n## Recap: The Full Journey\n\nWe've come a long way in this notebook series:\n\n| Notebook | What We Learned |\n|----------|-----------------|\n| 01 | Monarch's history and the single-controller paradigm |\n| 02 | Interactive development with `this_host()` |\n| 03 | Fault tolerance with `try/except` on actor calls |\n| 04 | Distributed tensors -- Monarch's tensor engine |\n| 05 | Zorplex benchmark -- where Qwen 0.5B struggles |\n| 06 | Services for managing worker pools with health tracking |\n| 07 | RDMA weight sync, circular buffers, CPU staging |\n| **08** | **Closing the loop: async RL training end to end** |\n\n**Key takeaways from this notebook:**\n\n- Monarch makes distributed RL feel like local Python -- actors, endpoints,\n  and slicing compose naturally into a full training system\n- Async RL collects more data per unit wall time by running generators\n  and trainer concurrently\n- The circular buffer + CPU staging pattern from [NB07](./07_rdma_weight_sync.html) decouples training\n  from weight distribution\n- Before/after evaluation closes the loop: we can measure whether training\n  actually improves the model\n\n**Where to go next:** Forge GRPO implements these same patterns at production\nscale -- multiple nodes, larger models, PPO/GRPO instead of REINFORCE, and\nproper reward modeling. The Monarch primitives you've learned here are the\nbuilding blocks for all of it.\n\n---\n\n**Previous:** [NB07b \u2014 RDMA Deep Dive](./07b_weight_sync_deep_dive.html)\n\"\"\")", "code_hash": "50e495670ea119d1c4b5cd85c8922ff9", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "rEll", "name": "_"}, {"code": "", "code_hash": null, "config": {"column": null, "disabled": false, "hide_code": false}, "id": "dGlV", "name": "_"}], "metadata": {"marimo_version": "0.19.9"}, "version": "1"},
            "session": {"cells": [{"code_hash": "1d0db38904205bec4d6f6f6a1f6cec3e", "console": [], "id": "Hbol", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "9d2da1a64a7c48c53bbbd693ac608cce", "console": [], "id": "MJUe", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "a9faced051e7222f49293f3c3a8368d0", "console": [], "id": "vblA", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch1 id=\"closing-the-loop-async-rl-training\"\u003EClosing the Loop: Async RL Training\u003C/h1\u003E\n\u003Cspan class=\"paragraph\"\u003EIn \u003Cstrong\u003ENotebook 05\u003C/strong\u003E, we introduced the Zorplex benchmark and identified three\nfailure modes: wrong format (no \u003Ccode\u003E[ANSWER]\u003C/code\u003E tag), tool spam, and wrong answers.\nThe model often gets the right value but fails to emit it correctly.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ENow we close the loop\u003C/strong\u003E: train the model to get better at these tasks, and\ntrack which failure modes improve during training.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe'll build on patterns from across the series. The architecture we're building: multiple generators\nfeed trajectories into a replay buffer while a trainer continuously samples and updates the policy.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe'll measure \u003Cem\u003Ebefore\u003C/em\u003E and \u003Cem\u003Eafter\u003C/em\u003E accuracy -- and failure mode breakdown -- to\nsee if training actually helps.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "e24305fa03ca6fb85a9e6c40bae7425e", "console": [], "id": "bkHC", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "5fc990d636c1d12556a4677252891c16", "console": [], "id": "lEQa", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "32735ea5328ce18acdcd3809bedc813b", "console": [], "id": "PKri", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"shared-data-structures\"\u003EShared Data Structures\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EFor clarity, we are using the following data structures in this notebook:\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ETrajectory\u003C/strong\u003E -- one rollout from a generator:\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EField\u003C/th\u003E\n\u003Cth\u003EType\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Etask_question\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Estr\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Etask_answer\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Eint | str\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eresponse_text\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Estr\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Ereward\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Efloat\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eis_correct\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Ebool\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Enum_turns\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Eint\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Enum_tool_calls\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Eint\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Egenerator_id\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Eint\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Epolicy_version\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Eint\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Emodel_only_text\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Estr\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Ehas_answer_tag\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Ebool\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Efailure_mode\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Estr\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Einput_ids\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Elist\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eprompt_length\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Eint\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ETrainMetrics\u003C/strong\u003E -- returned after each training step:\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EField\u003C/th\u003E\n\u003Cth\u003EType\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Estep\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Eint\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eloss\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Efloat\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Ebatch_size\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Eint\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eavg_reward\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Efloat\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Epolicy_version\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Eint\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Ecorrect_rate\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Efloat\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Eformat_rate\u003C/code\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Efloat\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003EKey fields:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ccode\u003Emodel_only_text\u003C/code\u003E stores the model's generated tokens without injected tool\n  results, so the trainer can compute log-probabilities on exactly what the model produced.\u003C/li\u003E\n\u003Cli\u003E\u003Ccode\u003Ehas_answer_tag\u003C/code\u003E tracks whether the model emitted \u003Ccode\u003E[ANSWER]\u003C/code\u003E -- this is the\n  format compliance signal from \u003Ca href=\"./05_rl_intro.html\"\u003ENB05\u003C/a\u003E's failure mode analysis.\u003C/li\u003E\n\u003Cli\u003E\u003Ccode\u003Efailure_mode\u003C/code\u003E classifies each trajectory as \u003Ccode\u003E\"success\"\u003C/code\u003E, \u003Ccode\u003E\"wrong_format\"\u003C/code\u003E,\n  \u003Ccode\u003E\"tool_spam\"\u003C/code\u003E, or \u003Ccode\u003E\"wrong_answer\"\u003C/code\u003E so we can track which failure modes improve\n  during training.\u003C/li\u003E\n\u003Cli\u003E\u003Ccode\u003Ecorrect_rate\u003C/code\u003E and \u003Ccode\u003Eformat_rate\u003C/code\u003E on \u003Ccode\u003ETrainMetrics\u003C/code\u003E let us track these signals\n  per training step.\u003C/li\u003E\n\u003C/ul\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "4e6c895b9547643bc1a0087250cf6b27", "console": [], "id": "Xref", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"service-infrastructure\"\u003EService Infrastructure\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EWe import a \u003Cstrong\u003Ecustom Service abstraction\u003C/strong\u003E from \u003Ccode\u003Emonarch_utils\u003C/code\u003E that manages worker\nreplicas with health tracking and round-robin routing. This is a utility we built\nfor this notebook series -- the canonical Monarch pattern uses direct actor\nreferences and slicing, which is what the Service wraps internally.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E(See notebook 05 for the full implementation.)\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "e644d8e238271c0dcb2c797fbb8e561f", "console": [], "id": "SFPL", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "d05d08aec233ea3fb2641681ad0bae7b", "console": [], "id": "BYtC", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"the-setup-pattern\"\u003EThe \u003Ccode\u003Esetup()\u003C/code\u003E Pattern\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EActors in this notebook use a two-phase initialization:\u003C/span\u003E\n\u003Col\u003E\n\u003Cli\u003E\u003Cstrong\u003E\u003Ccode\u003E__init__\u003C/code\u003E\u003C/strong\u003E runs during \u003Ccode\u003Espawn()\u003C/code\u003E -- keep it lightweight (store config, set rank)\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003E\u003Ccode\u003Esetup()\u003C/code\u003E\u003C/strong\u003E is an endpoint called explicitly after spawn -- do heavy work here\n   (load models, allocate GPU memory, register RDMA buffers)\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cspan class=\"paragraph\"\u003EWhy not do everything in \u003Ccode\u003E__init__\u003C/code\u003E? Two reasons:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003E\u003Ccode\u003Espawn()\u003C/code\u003E is asynchronous\u003C/strong\u003E -- it returns immediately, and \u003Ccode\u003E__init__\u003C/code\u003E runs in\n  the remote process before the first endpoint call. But you don't control \u003Cem\u003Ewhen\u003C/em\u003E,\n  and you can't confirm it completed. An explicit \u003Ccode\u003Esetup()\u003C/code\u003E call lets you sequence\n  initialization (e.g., set \u003Ccode\u003ECUDA_VISIBLE_DEVICES\u003C/code\u003E and confirm it took effect before\n  loading a model).\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003ECoordination\u003C/strong\u003E -- you often need to initialize actors in a specific order (set up\n  the trainer before generators try to sync weights). Endpoint calls give you that\n  sequencing; \u003Ccode\u003E__init__\u003C/code\u003E doesn't.\u003C/li\u003E\n\u003C/ul\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "7f9ef072399bd5d75ebb426bac50701e", "console": [], "id": "RGSE", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"actor-1-zorplexworker\"\u003EActor 1: ZorplexWorker\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003ETool execution environments (docker containers, sandboxes, API endpoints) naturally\nform a fleet -- you want many instances running in parallel to keep up with\ngeneration throughput. That makes them a good fit for a \u003Cstrong\u003EService\u003C/strong\u003E (from \u003Ca href=\"./06_services.html\"\u003ENB06\u003C/a\u003E)\nwith health tracking and round-robin routing.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EOur ZorplexWorker actors handle Zorplex tasks:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ccode\u003Egenerate_task()\u003C/code\u003E -- creates a new problem\u003C/li\u003E\n\u003Cli\u003E\u003Ccode\u003Eexecute_tool()\u003C/code\u003E -- handles LOOKUP calls\u003C/li\u003E\n\u003Cli\u003E\u003Ccode\u003Echeck_answer()\u003C/code\u003E -- verifies correctness\u003C/li\u003E\n\u003C/ul\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "5b736dfec5189aa58d1c4d7bd99b7601", "console": [], "id": "Kclp", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "9d4b4d785de4cd9567cfb9a9e40236a1", "console": [], "id": "emfo", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"actor-2-replaybuffer\"\u003EActor 2: ReplayBuffer\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EA simple actor that stores trajectories. Generators push trajectories in,\nthe trainer samples batches out.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003ERecall our intro to async RL in notebook 4 -- the replay buffer is the decoupling\npoint that enables asynchronous execution. Generators push, trainer pulls, neither\nwaits for the other. A secondary benefit is decorrelation: random sampling breaks\nthe correlation between consecutive trajectories from the same generator, giving\nbetter gradient estimates (especially when mixing tasks of different difficulties).\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "a6ac4b7a7f37f842cb2a48f522901c24", "console": [], "id": "Hstk", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "84a1e4e45c5ac15cbaaa182388830fb4", "console": [], "id": "nWHF", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"actor-3-traineractor\"\u003EActor 3: TrainerActor\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EThe trainer loads the model, receives batches of trajectories, and computes\npolicy gradient updates. We use \u003Cstrong\u003EREINFORCE\u003C/strong\u003E -- the simplest policy gradient\nmethod. Production systems typically use PPO or GRPO, which are variations that improve\nstability, but the approach looks similar from a systems perspective. In other words,\nREINFORCE lets us focus on the \u003Cem\u003Esystem\u003C/em\u003E (actors, weight sync, async coordination) rather\nthan the algorithm.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EThe loss for each trajectory is:\n\u003Cdiv class=\"language-scdoc codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003Eloss = -sum(log_prob(response_token_i)) * (reward - baseline)\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EThe trainer is the most complex actor, with several responsibilities:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003E\u003Ccode\u003Esetup()\u003C/code\u003E\u003C/strong\u003E \u2014 loads the model onto GPU 0, creates the optimizer, and\n  registers RDMA circular buffer slots\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003E\u003Ccode\u003Etrain_step()\u003C/code\u003E\u003C/strong\u003E \u2014 REINFORCE policy gradient on a batch of trajectories\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003E\u003Ccode\u003Eget_weight_handle()\u003C/code\u003E\u003C/strong\u003E \u2014 returns an RDMA handle to the current circular\n  buffer slot for generators to pull from\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003E\u003Ccode\u003Eevaluate_zorplex()\u003C/code\u003E\u003C/strong\u003E \u2014 runs deterministic evaluation for before/after comparison\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EGPU assignment note:\u003C/strong\u003E Monarch doesn't assign GPUs automatically \u2014\n\u003Ccode\u003Espawn_procs\u003C/code\u003E creates processes, but it's up to you to set\n\u003Ccode\u003ECUDA_VISIBLE_DEVICES\u003C/code\u003E in \u003Ccode\u003Esetup()\u003C/code\u003E. Here, the trainer hardcodes GPU 0\nand generators use GPU 1+.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ECircular buffer with CPU staging\u003C/strong\u003E (from \u003Ca href=\"./07_rdma_weight_sync.html\"\u003ENB07\u003C/a\u003E): After each training step,\nweights are copied GPU -\u0026gt; CPU into a circular buffer slot. Generators read\nfrom CPU via RDMA, then copy to their own GPU. This decouples training from\nweight distribution.\u003C/span\u003E\n\u003Cdiv class=\"language-ecl codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"n\"\u003ETrainer\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGPU\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"o\"\u003E--\u003C/span\u003E\u003Cspan class=\"n\"\u003ED2H\u003C/span\u003E\u003Cspan class=\"o\"\u003E--\u0026gt;\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ECPU\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eslot\u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Ev\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"o\"\u003E%\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mi\"\u003E3\u003C/span\u003E\u003Cspan class=\"p\"\u003E]\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"o\"\u003E--\u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMA\u003C/span\u003E\u003Cspan class=\"o\"\u003E--\u0026gt;\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGenerator\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ECPU\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Estaging\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"o\"\u003E--\u003C/span\u003E\u003Cspan class=\"n\"\u003EH2D\u003C/span\u003E\u003Cspan class=\"o\"\u003E--\u0026gt;\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGenerator\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGPU\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EEach slot is a single \u003Cstrong\u003Econtiguous\u003C/strong\u003E CPU buffer \u2014 all parameters packed\nend-to-end. This means one RDMA read transfers the entire model. An\nalternative is keeping parameters scattered and batching reads with\n\u003Ccode\u003ERDMAAction\u003C/code\u003E. We go into the different patterns and trade-offs in \u003Ca href=\"./07b_weight_sync_deep_dive.html\"\u003ENB07b\u003C/a\u003E.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "9e7e0a5b383dec5068cbb6276f1bebb4", "console": [], "id": "iLit", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "ef3666a5e9fb8f300fd0f4a7ea826e47", "console": [], "id": "ZHCJ", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"how-train_step-works\"\u003EHow \u003Ccode\u003Etrain_step\u003C/code\u003E Works\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EEach trajectory arrives with pre-tokenized \u003Ccode\u003Einput_ids\u003C/code\u003E and a \u003Ccode\u003Eprompt_length\u003C/code\u003E\nboundary (computed by the generator at generation time). The trainer:\u003C/span\u003E\n\u003Col\u003E\n\u003Cli\u003E\u003Cstrong\u003ELoads the token sequence\u003C/strong\u003E directly from \u003Ccode\u003Etraj.input_ids\u003C/code\u003E -- no re-tokenization.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003ESlices at \u003Ccode\u003Eprompt_length\u003C/code\u003E\u003C/strong\u003E to separate prompt from response tokens.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EComputes log-probs\u003C/strong\u003E on response tokens only (\u003Ccode\u003Elogits[i]\u003C/code\u003E predicts \u003Ccode\u003Etoken[i+1]\u003C/code\u003E,\n   so we start at \u003Ccode\u003Eprompt_length - 1\u003C/code\u003E).\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EComputes loss\u003C/strong\u003E: \u003Ccode\u003Eloss = -sum(log_probs) * advantage\u003C/code\u003E where\n   \u003Ccode\u003Eadvantage = reward - baseline\u003C/code\u003E. Positive advantage reinforces the response.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003ESteps the optimizer\u003C/strong\u003E once for the whole batch, then publishes new weights\n   to the circular buffer.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cspan class=\"paragraph\"\u003ELook for the \u003Ccode\u003E# Step N:\u003C/code\u003E comments in \u003Ccode\u003Etrain_step\u003C/code\u003E above -- they correspond to\nthese steps.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "6075ebc58a19ce15723e86fcd43fabbb", "console": [], "id": "ROlb", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"actor-4-generatorworker\"\u003EActor 4: GeneratorWorker\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EEach generator loads its own copy of the model, generates its own tasks from\nits seeded spec, and runs inference independently. The key endpoint is\n\u003Ccode\u003Egenerate_trajectory()\u003C/code\u003E -- it generates a task, runs multi-turn inference\nwith tool execution, and returns a complete \u003Ccode\u003ETrajectory\u003C/code\u003E with pre-tokenized\n\u003Ccode\u003Einput_ids\u003C/code\u003E and \u003Ccode\u003Eprompt_length\u003C/code\u003E for the trainer.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EReward shaping.\u003C/strong\u003E Instead of a binary 0/1 reward, we decompose rewards from\nthe failure modes identified in \u003Ca href=\"./05_rl_intro.html\"\u003ENB05\u003C/a\u003E:\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EComponent\u003C/th\u003E\n\u003Cth\u003EValue\u003C/th\u003E\n\u003Cth\u003EWhy\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003ECorrect answer\u003C/td\u003E\n\u003Ctd\u003E+1.0\u003C/td\u003E\n\u003Ctd\u003EThe main signal\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EFormat compliance (\u003Ccode\u003E[ANSWER]\u003C/code\u003E tag)\u003C/td\u003E\n\u003Ctd\u003E+0.2\u003C/td\u003E\n\u003Ctd\u003ELearnable even when wrong\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003ETool spam penalty\u003C/td\u003E\n\u003Ctd\u003E-0.1 per call beyond 2\u003C/td\u003E\n\u003Ctd\u003EDiscourages degenerate loops\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003EThis means a correct, well-formatted response earns up to 1.2, while a\nformat-only success (wrong answer but used \u003Ccode\u003E[ANSWER]\u003C/code\u003E) earns 0.2. The\ngradient signal is richer than binary: the model gets \u003Cem\u003Epartial credit\u003C/em\u003E for\ngood formatting even before it learns the right answers.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWeight sync uses the pattern from \u003Ca href=\"./07_rdma_weight_sync.html\"\u003ENB07\u003C/a\u003E: the trainer publishes weights to CPU\nslots (circular buffer), and generators pull via RDMA into a CPU staging\nbuffer, then scatter into GPU parameters (H2D copy). Ideally we'd load\ndirectly from the trainer's CPU buffer into the model's \u003Ccode\u003Estate_dict\u003C/code\u003E to\navoid the extra copy, but we hit \u003Ccode\u003ERDMABuffer\u003C/code\u003E bugs doing that \u2014 will fix.\nFallback path (\u003Ccode\u003Esync_weights\u003C/code\u003E using \u003Ccode\u003Estate_dict\u003C/code\u003E) stays for when RDMA is unavailable.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "b43243d596b9b4e3119d2a4f7dd44a2d", "console": [], "id": "qnkX", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "7608b837715e9d9adfb6faf3936c9879", "console": [], "id": "TqIu", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"architecture-overview\"\u003EArchitecture Overview\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003ENow we have all our actors defined. Here's how they connect -- this is the\n\u003Cstrong\u003Esingle-controller paradigm\u003C/strong\u003E from \u003Ca href=\"./01_history_and_vision.html\"\u003ENB01\u003C/a\u003E: the notebook process orchestrates\neverything, but actors do the heavy lifting on their own GPUs.\u003C/span\u003E\n\u003Cdiv class=\"language-gdscript codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\u003Cspan class=\"w\"\u003E         \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"n\"\u003EGeneratorMesh\u003C/span\u003E\u003Cspan class=\"w\"\u003E      \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E         \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"n\"\u003EZorplexService\u003C/span\u003E\u003Cspan class=\"w\"\u003E     \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003EActorMesh\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"w\"\u003E        \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E         \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003EService\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"w\"\u003E          \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"k\"\u003Etool\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGenerator\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mi\"\u003E0\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u2500\u2500\u253c\u2500\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecalls\u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2500\u2500\u25ba\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EZorplexWorker\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGenerator\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mi\"\u003E1\u003C/span\u003E\u003Cspan class=\"w\"\u003E   \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EZorplexWorker\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u25c4\u2500\u003C/span\u003E\u003Cspan class=\"n\"\u003Eresults\u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E         \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E           \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E         \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E          \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Etrajectories\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E          \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E    \u003C/span\u003E\u003Cspan class=\"n\"\u003EReplayBuffer\u003C/span\u003E\u003Cspan class=\"w\"\u003E     \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E           \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Esample\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ebatch\u003C/span\u003E\n\u003Cspan class=\"w\"\u003E           \u003C/span\u003E\u003Cspan class=\"n\"\u003Ev\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E      \u003C/span\u003E\u003Cspan class=\"n\"\u003ETrainer\u003C/span\u003E\u003Cspan class=\"w\"\u003E        \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2502\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"p\"\u003E(\u003C/span\u003E\u003Cspan class=\"n\"\u003Ecircular\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Ebuffer\u003C/span\u003E\u003Cspan class=\"p\"\u003E)\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"err\"\u003E\u2502\u2500\u2500\u003C/span\u003E\u003Cspan class=\"o\"\u003E\u0026gt;\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMA\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eweight\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Esync\u003C/span\u003E\n\u003Cspan class=\"err\"\u003E\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u003C/span\u003E\u003Cspan class=\"w\"\u003E      \u003C/span\u003E\u003Cspan class=\"n\"\u003Eto\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGeneratorMesh\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\n\u003Cspan class=\"paragraph\"\u003EEach generator calls zorplex tool endpoints during multi-turn inference\n(e.g., \u003Ccode\u003Elookup_value\u003C/code\u003E, \u003Ccode\u003Ecompute\u003C/code\u003E). The Service routes these calls round-robin\nacross ZorplexWorkers.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EActorMesh vs Service.\u003C/strong\u003E Generators are a plain \u003Cstrong\u003EActorMesh\u003C/strong\u003E -- we address\nthem directly via \u003Ccode\u003E.call()\u003C/code\u003E (broadcast to all) or \u003Ccode\u003E.slice()\u003C/code\u003E (individual\naccess). This is natural for sync RL (broadcast generate, then train) and\nfor async RL (each thread slices its own generator). ZorplexWorkers are\nwrapped in a \u003Cstrong\u003EService\u003C/strong\u003E (\u003Ca href=\"./06_services.html\"\u003ENB06\u003C/a\u003E pattern) because they're stateless: any\nworker can handle any request, so round-robin routing and health tracking\nare useful. In production async RL, you might wrap generators in a Service\ntoo -- that gives you auto-scaling and health tracking -- but here the\nActorMesh is simpler and lets us demonstrate both addressing patterns.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "cac9da9ca1bd3826773909f78152bebe", "console": [], "id": "Vxnm", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"sync-vs-async-rl\"\u003ESync vs Async RL\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ESync RL\u003C/strong\u003E (traditional):\n\u003Cdiv class=\"language-text codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E|--generate--|--train--|--generate--|--train--|--generate--|--train--|\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\nOnly ONE thing happens at a time. GPU sits idle during generation,\ngenerator sits idle during training.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EAsync RL\u003C/strong\u003E (what we're building):\n\u003Cdiv class=\"language-text codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003EGen0:  |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\nGen1:  |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\nTrain:      |\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593|\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\nEverything runs concurrently. More data collected, better GPU utilization.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe'll run BOTH modes with the \u003Cstrong\u003Esame actors\u003C/strong\u003E and compare wall time, throughput,\nand utilization.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "85c314495b9115cfe1c1d9c730e7a2b2", "console": [], "id": "DnEU", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"configuration\"\u003EConfiguration\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EAdjust parameters for the training run. \u003Cstrong\u003EMarimo is reactive\u003C/strong\u003E: changing a slider\nre-runs all downstream cells that depend on it. This means actors will be\nre-spawned and both training loops will re-execute with the new values.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cmarimo-ui-element object-id='DnEU-0' random-id='f7dd8bd2-5e4a-5881-c51b-eb58f9a1007e'\u003E\u003Cmarimo-slider data-initial-value='20' data-label='\u0026quot;\u0026lt;span class=\u0026#92;\u0026quot;markdown prose dark:prose-invert contents\u0026#92;\u0026quot;\u0026gt;\u0026lt;span class=\u0026#92;\u0026quot;paragraph\u0026#92;\u0026quot;\u0026gt;Training steps\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026quot;' data-start='10' data-stop='100' data-steps='[]' data-debounce='false' data-disabled='false' data-orientation='\u0026quot;horizontal\u0026quot;' data-show-value='false' data-include-input='false' data-full-width='false'\u003E\u003C/marimo-slider\u003E\u003C/marimo-ui-element\u003E\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cmarimo-ui-element object-id='DnEU-1' random-id='61f373ac-8dfe-20fa-d8d1-e3b9f1261116'\u003E\u003Cmarimo-slider data-initial-value='2' data-label='\u0026quot;\u0026lt;span class=\u0026#92;\u0026quot;markdown prose dark:prose-invert contents\u0026#92;\u0026quot;\u0026gt;\u0026lt;span class=\u0026#92;\u0026quot;paragraph\u0026#92;\u0026quot;\u0026gt;Generators\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026quot;' data-start='1' data-stop='4' data-steps='[]' data-debounce='false' data-disabled='false' data-orientation='\u0026quot;horizontal\u0026quot;' data-show-value='false' data-include-input='false' data-full-width='false'\u003E\u003C/marimo-slider\u003E\u003C/marimo-ui-element\u003E\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EBatch size\u003C/strong\u003E is set to match the number of generators -- each training step\ntrains on exactly one round of generation. This keeps the comparison fair:\nsync and async train on the same amount of data per step.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ESuggestions:\u003C/strong\u003E Start with defaults (20 steps, 2 generators) to see the\nfull pipeline. Then try increasing generators to 3-4 to see the async throughput\nadvantage grow. Increasing training steps gives the model more updates but adds\nwall time. Note that re-spawning actors (loading models onto GPUs) is the most\nexpensive part of the setup -- the training loops themselves are relatively fast.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ETry this:\u003C/strong\u003E Set generators to 1 and watch the async timeline -- with only one\ngenerator, async degrades to near-sync performance because there's no parallel\ngeneration to overlap with training.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "609c448f633ccd864db86c56b25225fc", "console": [], "id": "ulZA", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "bf4586ecfdc747dcb0276e3471105b8f", "console": [], "id": "ecfG", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"spawning-and-initializing-actors\"\u003ESpawning and Initializing Actors\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EThis is the \u003Cstrong\u003Esingle-controller paradigm\u003C/strong\u003E in action. The notebook process\norchestrates a careful initialization sequence:\u003C/span\u003E\n\u003Col\u003E\n\u003Cli\u003ESpawn ZorplexWorkers via a \u003Cstrong\u003EService\u003C/strong\u003E (\u003Ca href=\"./06_services.html\"\u003ENB06\u003C/a\u003E pattern -- health tracking, round-robin)\u003C/li\u003E\n\u003Cli\u003ESpawn GeneratorWorkers as a plain \u003Cstrong\u003EActorMesh\u003C/strong\u003E and call \u003Ccode\u003Esetup()\u003C/code\u003E on all via\n   \u003Ccode\u003E.call()\u003C/code\u003E broadcast (loads model onto each GPU)\u003C/li\u003E\n\u003Cli\u003ESpawn ReplayBuffer (CPU-only, ready immediately)\u003C/li\u003E\n\u003Cli\u003ESpawn Trainer, then call \u003Ccode\u003Esetup()\u003C/code\u003E (loads model onto GPU 0, registers RDMA buffers)\u003C/li\u003E\n\u003C/ol\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "0942732c887af3453a85a25400884725", "console": [{"mimetype": "text/plain", "name": "stderr", "text": "Monarch internal logs are being written to /tmp/allencwang/monarch_log.log; execution id allencwang_Feb-07_10:56_370\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "[ReplayBuffer] Initialized with max_size=500\n[SERVICE:zorplex] 2 replicas x 1 procs each\n[SETUP] Setting up generator workers...\n[GeneratorWorker:0] Spawned, waiting for setup()...\n[GeneratorWorker:1] Spawned, waiting for setup()...\n[Trainer:0] Spawned, waiting for setup()...\n[GeneratorWorker:0] Loading model Qwen/Qwen2.5-0.5B-Instruct on GPU 1...\n[GeneratorWorker:1] Loading model Qwen/Qwen2.5-0.5B-Instruct on GPU 2...\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stderr", "text": "[actor=\u003Croot\u003E.\u003C__main__.GeneratorWorker generators{'procs': 0/2}\u003E] `torch_dtype` is deprecated! Use `dtype` instead!\n[actor=\u003Croot\u003E.\u003C__main__.GeneratorWorker generators{'procs': 1/2}\u003E] `torch_dtype` is deprecated! Use `dtype` instead!\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "[ZorplexWorker:0] Initialized with difficulty=easy\n[ZorplexWorker:0] Initialized with difficulty=easy\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stderr", "text": "\rLoading weights:   0%|          | 0/290 [00:00\u003C?, ?it/s]\rLoading weights:   0%|          | 1/290 [00:00\u003C00:00, 15363.75it/s, Materializing param=model.embed_tokens.weight]\rLoading weights:   0%|          | 1/290 [00:00\u003C00:00, 6864.65it/s, Materializing param=model.embed_tokens.weight] \rLoading weights:   1%|          | 2/290 [00:00\u003C00:00, 6652.35it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|          | 2/290 [00:00\u003C00:00, 5592.41it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|          | 3/290 [00:00\u003C00:00, 6410.04it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  \rLoading weights:   1%|          | 3/290 [00:00\u003C00:00, 5888.12it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   1%|\u258f         | 4/290 [00:00\u003C00:00, 6721.64it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   1%|\u258f         | 4/290 [00:00\u003C00:00, 6241.52it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   2%|\u258f         | 5/290 [00:00\u003C00:00, 6937.32it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \rLoading weights:   2%|\u258f         | 5/290 [00:00\u003C00:00, 6601.05it/s, Materializing param=model.layers.0.mlp.up_proj.weight]\rLoading weights:   2%|\u258f         | 6/290 [00:00\u003C00:00, 7223.26it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   2%|\u258f         | 6/290 [00:00\u003C00:00, 6898.53it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   2%|\u258f         | 7/290 [00:00\u003C00:00, 7432.94it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          \rLoading weights:   2%|\u258f         | 7/290 [00:00\u003C00:00, 7157.52it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]\rLoading weights:   3%|\u258e         | 8/290 [00:00\u003C00:00, 7626.01it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|\u258e         | 8/290 [00:00\u003C00:00, 7395.73it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|\u258e         | 9/290 [00:00\u003C00:00, 7746.51it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   3%|\u258e         | 9/290 [00:00\u003C00:00, 7534.68it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   3%|\u258e         | 10/290 [00:00\u003C00:00, 7925.74it/s, Materializing param=model.layers.0.self_attn.q_proj.bias] \rLoading weights:   3%|\u258e         | 10/290 [00:00\u003C00:00, 7720.05it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]\rLoading weights:   4%|\u258d         | 11/290 [00:00\u003C00:00, 8077.27it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   4%|\u258d         | 11/290 [00:00\u003C00:00, 7882.68it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   4%|\u258d         | 12/290 [00:00\u003C00:00, 8205.36it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  \rLoading weights:   4%|\u258d         | 12/290 [00:00\u003C00:00, 8031.22it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]\rLoading weights:   4%|\u258d         | 13/290 [00:00\u003C00:00, 8332.21it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   4%|\u258d         | 13/290 [00:00\u003C00:00, 8154.02it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   5%|\u258d         | 14/290 [00:00\u003C00:00, 8416.26it/s, Materializing param=model.layers.1.input_layernorm.weight] \rLoading weights:   5%|\u258d         | 14/290 [00:00\u003C00:00, 8246.07it/s, Materializing param=model.layers.1.input_layernorm.weight]\rLoading weights:   5%|\u258c         | 15/290 [00:00\u003C00:00, 8505.42it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  \rLoading weights:   5%|\u258c         | 15/290 [00:00\u003C00:00, 8334.16it/s, Materializing param=model.layers.1.mlp.down_proj.weight]\rLoading weights:   6%|\u258c         | 16/290 [00:00\u003C00:00, 8559.80it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   6%|\u258c         | 16/290 [00:00\u003C00:00, 8404.37it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   6%|\u258c         | 17/290 [00:00\u003C00:00, 8616.70it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  \rLoading weights:   6%|\u258c         | 17/290 [00:00\u003C00:00, 8469.32it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\rLoading weights:   6%|\u258c         | 18/290 [00:00\u003C00:00, 8675.88it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:   6%|\u258c         | 18/290 [00:00\u003C00:00, 8509.63it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:   7%|\u258b         | 19/290 [00:00\u003C00:00, 8704.73it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          \rLoading weights:   7%|\u258b         | 19/290 [00:00\u003C00:00, 8569.93it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]\rLoading weights:   7%|\u258b         | 20/290 [00:00\u003C00:00, 8752.72it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:   7%|\u258b         | 20/290 [00:00\u003C00:00, 8624.93it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:   7%|\u258b         | 21/290 [00:00\u003C00:00, 8805.40it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:   7%|\u258b         | 21/290 [00:00\u003C00:00, 8682.15it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:   8%|\u258a         | 22/290 [00:00\u003C00:00, 8851.29it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  \rLoading weights:   8%|\u258a         | 22/290 [00:00\u003C00:00, 8734.00it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]\rLoading weights:   8%|\u258a         | 23/290 [00:00\u003C00:00, 8901.82it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:   8%|\u258a         | 23/290 [00:00\u003C00:00, 8789.08it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:   8%|\u258a         | 24/290 [00:00\u003C00:00, 8948.64it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  \rLoading weights:   8%|\u258a         | 24/290 [00:00\u003C00:00, 8839.42it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]\rLoading weights:   9%|\u258a         | 25/290 [00:00\u003C00:00, 8996.02it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:   9%|\u258a         | 25/290 [00:00\u003C00:00, 8893.77it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:   9%|\u2589         | 26/290 [00:00\u003C00:00, 9041.70it/s, Materializing param=model.layers.2.input_layernorm.weight] \rLoading weights:   9%|\u2589         | 26/290 [00:00\u003C00:00, 8939.41it/s, Materializing param=model.layers.2.input_layernorm.weight]\rLoading weights:   9%|\u2589         | 27/290 [00:00\u003C00:00, 9077.85it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  \rLoading weights:   9%|\u2589         | 27/290 [00:00\u003C00:00, 8978.53it/s, Materializing param=model.layers.2.mlp.down_proj.weight]\rLoading weights:  10%|\u2589         | 28/290 [00:00\u003C00:00, 9107.45it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  10%|\u2589         | 28/290 [00:00\u003C00:00, 8964.92it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  10%|\u2588         | 29/290 [00:00\u003C00:00, 9093.51it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  \rLoading weights:  10%|\u2588         | 29/290 [00:00\u003C00:00, 8991.34it/s, Materializing param=model.layers.2.mlp.up_proj.weight]\rLoading weights:  10%|\u2588         | 30/290 [00:00\u003C00:00, 9111.45it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  10%|\u2588         | 30/290 [00:00\u003C00:00, 9016.13it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  11%|\u2588         | 31/290 [00:00\u003C00:00, 9137.28it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          \rLoading weights:  11%|\u2588         | 31/290 [00:00\u003C00:00, 9052.03it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]\rLoading weights:  11%|\u2588         | 32/290 [00:00\u003C00:00, 9170.38it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  11%|\u2588         | 32/290 [00:00\u003C00:00, 9086.57it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  11%|\u2588\u258f        | 33/290 [00:00\u003C00:00, 9201.70it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  11%|\u2588\u258f        | 33/290 [00:00\u003C00:00, 9116.85it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 34/290 [00:00\u003C00:00, 9227.20it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  \rLoading weights:  12%|\u2588\u258f        | 34/290 [00:00\u003C00:00, 9148.47it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]\rLoading weights:  12%|\u2588\u258f        | 35/290 [00:00\u003C00:00, 9253.70it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 35/290 [00:00\u003C00:00, 9173.32it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 36/290 [00:00\u003C00:00, 9273.73it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  \rLoading weights:  12%|\u2588\u258f        | 36/290 [00:00\u003C00:00, 9196.35it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]\rLoading weights:  13%|\u2588\u258e        | 37/290 [00:00\u003C00:00, 9300.01it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  13%|\u2588\u258e        | 37/290 [00:00\u003C00:00, 9225.92it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  13%|\u2588\u258e        | 38/290 [00:00\u003C00:00, 8468.39it/s, Materializing param=model.layers.3.input_layernorm.weight] \rLoading weights:  13%|\u2588\u258e        | 38/290 [00:00\u003C00:00, 8404.53it/s, Materializing param=model.layers.3.input_layernorm.weight]\rLoading weights:  13%|\u2588\u258e        | 39/290 [00:00\u003C00:00, 8507.27it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \rLoading weights:  13%|\u2588\u258e        | 39/290 [00:00\u003C00:00, 8447.96it/s, Materializing param=model.layers.3.mlp.down_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 40/290 [00:00\u003C00:00, 8543.24it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 40/290 [00:00\u003C00:00, 8486.20it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 41/290 [00:00\u003C00:00, 8583.30it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  \rLoading weights:  14%|\u2588\u258d        | 41/290 [00:00\u003C00:00, 8527.54it/s, Materializing param=model.layers.3.mlp.up_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 42/290 [00:00\u003C00:00, 8032.50it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  14%|\u2588\u258d        | 42/290 [00:00\u003C00:00, 7966.39it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  15%|\u2588\u258d        | 43/290 [00:00\u003C00:00, 8053.00it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          \rLoading weights:  15%|\u2588\u258d        | 43/290 [00:00\u003C00:00, 8004.40it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]\rLoading weights:  15%|\u2588\u258c        | 44/290 [00:00\u003C00:00, 8092.50it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  15%|\u2588\u258c        | 44/290 [00:00\u003C00:00, 8044.87it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 45/290 [00:00\u003C00:00, 8134.80it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 45/290 [00:00\u003C00:00, 8089.48it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 46/290 [00:00\u003C00:00, 8173.61it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  \rLoading weights:  16%|\u2588\u258c        | 46/290 [00:00\u003C00:00, 8128.15it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]\rLoading weights:  16%|\u2588\u258c        | 47/290 [00:00\u003C00:00, 8209.40it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 47/290 [00:00\u003C00:00, 8162.49it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 48/290 [00:00\u003C00:00, 8028.34it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  \rLoading weights:  17%|\u2588\u258b        | 48/290 [00:00\u003C00:00, 7982.18it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]\rLoading weights:  17%|\u2588\u258b        | 49/290 [00:00\u003C00:00, 8060.91it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 49/290 [00:00\u003C00:00, 8016.26it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 50/290 [00:00\u003C00:00, 7839.23it/s, Materializing param=model.layers.4.input_layernorm.weight] \rLoading weights:  17%|\u2588\u258b        | 50/290 [00:00\u003C00:00, 7797.26it/s, Materializing param=model.layers.4.input_layernorm.weight]\rLoading weights:  18%|\u2588\u258a        | 51/290 [00:00\u003C00:00, 7521.43it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  \rLoading weights:  18%|\u2588\u258a        | 51/290 [00:00\u003C00:00, 7482.49it/s, Materializing param=model.layers.4.mlp.down_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 52/290 [00:00\u003C00:00, 7552.33it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 52/290 [00:00\u003C00:00, 7518.49it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 53/290 [00:00\u003C00:00, 7592.93it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  \rLoading weights:  18%|\u2588\u258a        | 53/290 [00:00\u003C00:00, 7559.10it/s, Materializing param=model.layers.4.mlp.up_proj.weight]\rLoading weights:  19%|\u2588\u258a        | 54/290 [00:00\u003C00:00, 7630.37it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  19%|\u2588\u258a        | 54/290 [00:00\u003C00:00, 7589.47it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  19%|\u2588\u2589        | 55/290 [00:00\u003C00:00, 7657.90it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          \rLoading weights:  19%|\u2588\u2589        | 55/290 [00:00\u003C00:00, 7621.72it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]\rLoading weights:  19%|\u2588\u2589        | 56/290 [00:00\u003C00:00, 7690.68it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  19%|\u2588\u2589        | 56/290 [00:00\u003C00:00, 7657.83it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  20%|\u2588\u2589        | 57/290 [00:00\u003C00:00, 7575.74it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  20%|\u2588\u2589        | 57/290 [00:00\u003C00:00, 7541.57it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  20%|\u2588\u2588        | 58/290 [00:00\u003C00:00, 7607.64it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  \rLoading weights:  20%|\u2588\u2588        | 58/290 [00:00\u003C00:00, 7574.01it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]\rLoading weights:  20%|\u2588\u2588        | 59/290 [00:00\u003C00:00, 7640.37it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  20%|\u2588\u2588        | 59/290 [00:00\u003C00:00, 7608.42it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  21%|\u2588\u2588        | 60/290 [00:00\u003C00:00, 7673.21it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  \rLoading weights:  21%|\u2588\u2588        | 60/290 [00:00\u003C00:00, 7641.99it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]\rLoading weights:  21%|\u2588\u2588        | 61/290 [00:00\u003C00:00, 7449.48it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  21%|\u2588\u2588        | 61/290 [00:00\u003C00:00, 7417.74it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  21%|\u2588\u2588\u258f       | 62/290 [00:00\u003C00:00, 7478.84it/s, Materializing param=model.layers.5.input_layernorm.weight] \rLoading weights:  21%|\u2588\u2588\u258f       | 62/290 [00:00\u003C00:00, 7449.06it/s, Materializing param=model.layers.5.input_layernorm.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 63/290 [00:00\u003C00:00, 7250.21it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  \rLoading weights:  22%|\u2588\u2588\u258f       | 63/290 [00:00\u003C00:00, 7220.30it/s, Materializing param=model.layers.5.mlp.down_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 64/290 [00:00\u003C00:00, 7277.83it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 64/290 [00:00\u003C00:00, 7250.51it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 65/290 [00:00\u003C00:00, 7309.50it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \rLoading weights:  22%|\u2588\u2588\u258f       | 65/290 [00:00\u003C00:00, 7282.94it/s, Materializing param=model.layers.5.mlp.up_proj.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 66/290 [00:00\u003C00:00, 7341.64it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 66/290 [00:00\u003C00:00, 7314.10it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 67/290 [00:00\u003C00:00, 7362.48it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          \rLoading weights:  23%|\u2588\u2588\u258e       | 67/290 [00:00\u003C00:00, 7335.38it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]\rLoading weights:  23%|\u2588\u2588\u258e       | 68/290 [00:00\u003C00:00, 7339.49it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 68/290 [00:00\u003C00:00, 7312.96it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 69/290 [00:00\u003C00:00, 7293.52it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 69/290 [00:00\u003C00:00, 7267.51it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 70/290 [00:00\u003C00:00, 7322.09it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  \rLoading weights:  24%|\u2588\u2588\u258d       | 70/290 [00:00\u003C00:00, 7297.89it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]\rLoading weights:  24%|\u2588\u2588\u258d       | 71/290 [00:00\u003C00:00, 7352.98it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 71/290 [00:00\u003C00:00, 7329.09it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  25%|\u2588\u2588\u258d       | 72/290 [00:00\u003C00:00, 7385.24it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  \rLoading weights:  25%|\u2588\u2588\u258d       | 72/290 [00:00\u003C00:00, 7361.48it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]\rLoading weights:  25%|\u2588\u2588\u258c       | 73/290 [00:00\u003C00:00, 7002.82it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  25%|\u2588\u2588\u258c       | 73/290 [00:00\u003C00:00, 6978.56it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 74/290 [00:00\u003C00:00, 7029.45it/s, Materializing param=model.layers.6.input_layernorm.weight] \rLoading weights:  26%|\u2588\u2588\u258c       | 74/290 [00:00\u003C00:00, 7007.08it/s, Materializing param=model.layers.6.input_layernorm.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 75/290 [00:00\u003C00:00, 7057.32it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  \rLoading weights:  26%|\u2588\u2588\u258c       | 75/290 [00:00\u003C00:00, 7036.64it/s, Materializing param=model.layers.6.mlp.down_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 76/290 [00:00\u003C00:00, 7087.02it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 76/290 [00:00\u003C00:00, 7066.60it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 77/290 [00:00\u003C00:00, 7116.51it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  \rLoading weights:  27%|\u2588\u2588\u258b       | 77/290 [00:00\u003C00:00, 7094.00it/s, Materializing param=model.layers.6.mlp.up_proj.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 78/290 [00:00\u003C00:00, 7142.83it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 78/290 [00:00\u003C00:00, 7121.99it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 79/290 [00:00\u003C00:00, 7171.15it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          \rLoading weights:  27%|\u2588\u2588\u258b       | 79/290 [00:00\u003C00:00, 7144.55it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]\rLoading weights:  28%|\u2588\u2588\u258a       | 80/290 [00:00\u003C00:00, 7170.52it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 80/290 [00:00\u003C00:00, 7146.54it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 81/290 [00:00\u003C00:00, 7194.04it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 81/290 [00:00\u003C00:00, 7173.99it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 82/290 [00:00\u003C00:00, 6961.92it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  \rLoading weights:  28%|\u2588\u2588\u258a       | 82/290 [00:00\u003C00:00, 6940.71it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]\rLoading weights:  29%|\u2588\u2588\u258a       | 83/290 [00:00\u003C00:00, 6985.18it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  29%|\u2588\u2588\u258a       | 83/290 [00:00\u003C00:00, 6966.17it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  29%|\u2588\u2588\u2589       | 84/290 [00:00\u003C00:00, 7011.79it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  \rLoading weights:  29%|\u2588\u2588\u2589       | 84/290 [00:00\u003C00:00, 6993.70it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]\rLoading weights:  29%|\u2588\u2588\u2589       | 85/290 [00:00\u003C00:00, 7040.62it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  29%|\u2588\u2588\u2589       | 85/290 [00:00\u003C00:00, 7019.41it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2589       | 86/290 [00:00\u003C00:00, 7065.54it/s, Materializing param=model.layers.7.input_layernorm.weight] \rLoading weights:  30%|\u2588\u2588\u2589       | 86/290 [00:00\u003C00:00, 7045.95it/s, Materializing param=model.layers.7.input_layernorm.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 87/290 [00:00\u003C00:00, 7090.48it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  \rLoading weights:  30%|\u2588\u2588\u2588       | 87/290 [00:00\u003C00:00, 7071.24it/s, Materializing param=model.layers.7.mlp.down_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 88/290 [00:00\u003C00:00, 7115.02it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 88/290 [00:00\u003C00:00, 7096.82it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 89/290 [00:00\u003C00:00, 7141.77it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  \rLoading weights:  31%|\u2588\u2588\u2588       | 89/290 [00:00\u003C00:00, 7123.64it/s, Materializing param=model.layers.7.mlp.up_proj.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 90/290 [00:00\u003C00:00, 7045.83it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 90/290 [00:00\u003C00:00, 7026.42it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  31%|\u2588\u2588\u2588\u258f      | 91/290 [00:00\u003C00:00, 7067.92it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          \rLoading weights:  31%|\u2588\u2588\u2588\u258f      | 91/290 [00:00\u003C00:00, 7049.90it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 92/290 [00:00\u003C00:00, 7022.95it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 92/290 [00:00\u003C00:00, 7004.85it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 93/290 [00:00\u003C00:00, 7046.58it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 93/290 [00:00\u003C00:00, 7028.67it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 94/290 [00:00\u003C00:00, 7068.97it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  \rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 94/290 [00:00\u003C00:00, 7052.28it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 95/290 [00:00\u003C00:00, 7016.60it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 95/290 [00:00\u003C00:00, 6999.59it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 96/290 [00:00\u003C00:00, 7038.78it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  \rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 96/290 [00:00\u003C00:00, 7023.06it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 97/290 [00:00\u003C00:00, 7006.28it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 97/290 [00:00\u003C00:00, 6989.43it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 98/290 [00:00\u003C00:00, 6961.85it/s, Materializing param=model.layers.8.input_layernorm.weight] \rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 98/290 [00:00\u003C00:00, 6943.74it/s, Materializing param=model.layers.8.input_layernorm.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 99/290 [00:00\u003C00:00, 6983.69it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  \rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 99/290 [00:00\u003C00:00, 6968.33it/s, Materializing param=model.layers.8.mlp.down_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 100/290 [00:00\u003C00:00, 7008.03it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 100/290 [00:00\u003C00:00, 6992.25it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258d      | 101/290 [00:00\u003C00:00, 6866.55it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  \rLoading weights:  35%|\u2588\u2588\u2588\u258d      | 101/290 [00:00\u003C00:00, 6850.78it/s, Materializing param=model.layers.8.mlp.up_proj.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258c      | 102/290 [00:00\u003C00:00, 6888.97it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258c      | 102/290 [00:00\u003C00:00, 6872.93it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 103/290 [00:00\u003C00:00, 6896.54it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          \rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 103/290 [00:00\u003C00:00, 6882.04it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 104/290 [00:00\u003C00:00, 6919.10it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 104/290 [00:00\u003C00:00, 6904.97it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 105/290 [00:00\u003C00:00, 6935.90it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 105/290 [00:00\u003C00:00, 6921.29it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 106/290 [00:00\u003C00:00, 6948.88it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  \rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 106/290 [00:00\u003C00:00, 6934.79it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 107/290 [00:00\u003C00:00, 6971.50it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 107/290 [00:00\u003C00:00, 6957.24it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 108/290 [00:00\u003C00:00, 6814.67it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  \rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 108/290 [00:00\u003C00:00, 6800.66it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 109/290 [00:00\u003C00:00, 6835.51it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 109/290 [00:00\u003C00:00, 6821.84it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 110/290 [00:00\u003C00:00, 6857.82it/s, Materializing param=model.layers.9.input_layernorm.weight] \rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 110/290 [00:00\u003C00:00, 6844.49it/s, Materializing param=model.layers.9.input_layernorm.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 111/290 [00:00\u003C00:00, 6876.42it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  \rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 111/290 [00:00\u003C00:00, 6863.14it/s, Materializing param=model.layers.9.mlp.down_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u258a      | 112/290 [00:00\u003C00:00, 6898.02it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u258a      | 112/290 [00:00\u003C00:00, 6884.17it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 113/290 [00:00\u003C00:00, 6831.31it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  \rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 113/290 [00:00\u003C00:00, 6816.08it/s, Materializing param=model.layers.9.mlp.up_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 114/290 [00:00\u003C00:00, 6848.43it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 114/290 [00:00\u003C00:00, 6833.75it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2589      | 115/290 [00:00\u003C00:00, 6867.00it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          \rLoading weights:  40%|\u2588\u2588\u2588\u2589      | 115/290 [00:00\u003C00:00, 6853.44it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 116/290 [00:00\u003C00:00, 6886.22it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 116/290 [00:00\u003C00:00, 6872.51it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 117/290 [00:00\u003C00:00, 6904.64it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 117/290 [00:00\u003C00:00, 6891.07it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 118/290 [00:00\u003C00:00, 6922.94it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  \rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 118/290 [00:00\u003C00:00, 6909.60it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 119/290 [00:00\u003C00:00, 6845.07it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 119/290 [00:00\u003C00:00, 6831.68it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588\u258f     | 120/290 [00:00\u003C00:00, 6863.34it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  \rLoading weights:  41%|\u2588\u2588\u2588\u2588\u258f     | 120/290 [00:00\u003C00:00, 6850.73it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 121/290 [00:00\u003C00:00, 6882.16it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 121/290 [00:00\u003C00:00, 6869.21it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 122/290 [00:00\u003C00:00, 6900.20it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 122/290 [00:00\u003C00:00, 6887.85it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 123/290 [00:00\u003C00:00, 6717.44it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  \rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 123/290 [00:00\u003C00:00, 6704.43it/s, Materializing param=model.layers.10.mlp.down_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 124/290 [00:00\u003C00:00, 6733.74it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 124/290 [00:00\u003C00:00, 6720.43it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 125/290 [00:00\u003C00:00, 6750.11it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  \rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 125/290 [00:00\u003C00:00, 6738.66it/s, Materializing param=model.layers.10.mlp.up_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 126/290 [00:00\u003C00:00, 6769.17it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 126/290 [00:00\u003C00:00, 6756.62it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 127/290 [00:00\u003C00:00, 6785.78it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          \rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 127/290 [00:00\u003C00:00, 6774.21it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 128/290 [00:00\u003C00:00, 6804.62it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 128/290 [00:00\u003C00:00, 6793.00it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 129/290 [00:00\u003C00:00, 6824.39it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 129/290 [00:00\u003C00:00, 6813.57it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258d     | 130/290 [00:00\u003C00:00, 6843.37it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  \rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258d     | 130/290 [00:00\u003C00:00, 6831.71it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258c     | 131/290 [00:00\u003C00:00, 6859.68it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258c     | 131/290 [00:00\u003C00:00, 6848.74it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 132/290 [00:00\u003C00:00, 6833.22it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  \rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 132/290 [00:00\u003C00:00, 6820.76it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 133/290 [00:00\u003C00:00, 6848.39it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 133/290 [00:00\u003C00:00, 6836.56it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 134/290 [00:00\u003C00:00, 6863.65it/s, Materializing param=model.layers.11.input_layernorm.weight] \rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 134/290 [00:00\u003C00:00, 6847.76it/s, Materializing param=model.layers.11.input_layernorm.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 135/290 [00:00\u003C00:00, 6876.08it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  \rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 135/290 [00:00\u003C00:00, 6865.49it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 136/290 [00:00\u003C00:00, 6845.21it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 136/290 [00:00\u003C00:00, 6833.49it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 137/290 [00:00\u003C00:00, 6837.78it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 137/290 [00:00\u003C00:00, 6826.65it/s, Materializing param=model.layers.11.mlp.up_proj.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 138/290 [00:00\u003C00:00, 6740.51it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 138/290 [00:00\u003C00:00, 6728.99it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 139/290 [00:00\u003C00:00, 6756.07it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          \rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 139/290 [00:00\u003C00:00, 6745.44it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 140/290 [00:00\u003C00:00, 6772.65it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 140/290 [00:00\u003C00:00, 6762.36it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u258a     | 141/290 [00:00\u003C00:00, 6789.24it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u258a     | 141/290 [00:00\u003C00:00, 6778.96it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 142/290 [00:00\u003C00:00, 6805.05it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  \rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 142/290 [00:00\u003C00:00, 6794.87it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 143/290 [00:00\u003C00:00, 6822.72it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 143/290 [00:00\u003C00:00, 6812.57it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2589     | 144/290 [00:00\u003C00:00, 6839.78it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  \rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2589     | 144/290 [00:00\u003C00:00, 6829.80it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 145/290 [00:00\u003C00:00, 6814.43it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 145/290 [00:00\u003C00:00, 6803.60it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 146/290 [00:00\u003C00:00, 6787.43it/s, Materializing param=model.layers.12.input_layernorm.weight] \rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 146/290 [00:00\u003C00:00, 6777.29it/s, Materializing param=model.layers.12.input_layernorm.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 147/290 [00:00\u003C00:00, 6803.23it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  \rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 147/290 [00:00\u003C00:00, 6793.26it/s, Materializing param=model.layers.12.mlp.down_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 148/290 [00:00\u003C00:00, 6819.86it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 148/290 [00:00\u003C00:00, 6809.83it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 149/290 [00:00\u003C00:00, 6700.31it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  \rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 149/290 [00:00\u003C00:00, 6690.12it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 150/290 [00:00\u003C00:00, 6714.75it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 150/290 [00:00\u003C00:00, 6704.31it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 151/290 [00:00\u003C00:00, 6729.64it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          \rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 151/290 [00:00\u003C00:00, 6719.64it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 152/290 [00:00\u003C00:00, 6744.54it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 152/290 [00:00\u003C00:00, 6734.71it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 153/290 [00:00\u003C00:00, 6760.16it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 153/290 [00:00\u003C00:00, 6750.42it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 154/290 [00:00\u003C00:00, 6775.30it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  \rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 154/290 [00:00\u003C00:00, 6765.64it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 155/290 [00:00\u003C00:00, 6790.80it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 155/290 [00:00\u003C00:00, 6781.38it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 156/290 [00:00\u003C00:00, 6806.74it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  \rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 156/290 [00:00\u003C00:00, 6796.84it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 157/290 [00:00\u003C00:00, 6821.42it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 157/290 [00:00\u003C00:00, 6811.68it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 158/290 [00:00\u003C00:00, 6790.10it/s, Materializing param=model.layers.13.input_layernorm.weight] \rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 158/290 [00:00\u003C00:00, 6780.03it/s, Materializing param=model.layers.13.input_layernorm.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 159/290 [00:00\u003C00:00, 6803.17it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  \rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 159/290 [00:00\u003C00:00, 6793.81it/s, Materializing param=model.layers.13.mlp.down_proj.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 160/290 [00:00\u003C00:00, 6818.41it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 160/290 [00:00\u003C00:00, 6809.42it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 161/290 [00:00\u003C00:00, 6834.09it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  \rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 161/290 [00:00\u003C00:00, 6824.90it/s, Materializing param=model.layers.13.mlp.up_proj.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 162/290 [00:00\u003C00:00, 6790.83it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 162/290 [00:00\u003C00:00, 6780.60it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 163/290 [00:00\u003C00:00, 6803.51it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          \rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 163/290 [00:00\u003C00:00, 6794.12it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 164/290 [00:00\u003C00:00, 6816.90it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 164/290 [00:00\u003C00:00, 6806.37it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 165/290 [00:00\u003C00:00, 6810.54it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 165/290 [00:00\u003C00:00, 6801.11it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 166/290 [00:00\u003C00:00, 6802.75it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  \rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 166/290 [00:00\u003C00:00, 6793.32it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 167/290 [00:00\u003C00:00, 6772.27it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 167/290 [00:00\u003C00:00, 6762.20it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 168/290 [00:00\u003C00:00, 6783.83it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  \rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 168/290 [00:00\u003C00:00, 6775.02it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 169/290 [00:00\u003C00:00, 6763.01it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 169/290 [00:00\u003C00:00, 6754.05it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 170/290 [00:00\u003C00:00, 6776.84it/s, Materializing param=model.layers.14.input_layernorm.weight] \rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 170/290 [00:00\u003C00:00, 6767.45it/s, Materializing param=model.layers.14.input_layernorm.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 171/290 [00:00\u003C00:00, 6790.18it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  \rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 171/290 [00:00\u003C00:00, 6781.70it/s, Materializing param=model.layers.14.mlp.down_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 172/290 [00:00\u003C00:00, 6803.80it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 172/290 [00:00\u003C00:00, 6795.40it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 173/290 [00:00\u003C00:00, 6737.24it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  \rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 173/290 [00:00\u003C00:00, 6728.25it/s, Materializing param=model.layers.14.mlp.up_proj.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 174/290 [00:00\u003C00:00, 6749.80it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 174/290 [00:00\u003C00:00, 6740.89it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 175/290 [00:00\u003C00:00, 6762.08it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          \rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 175/290 [00:00\u003C00:00, 6753.62it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 176/290 [00:00\u003C00:00, 6775.38it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 176/290 [00:00\u003C00:00, 6767.18it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 177/290 [00:00\u003C00:00, 6703.74it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 177/290 [00:00\u003C00:00, 6694.49it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 178/290 [00:00\u003C00:00, 6715.29it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  \rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 178/290 [00:00\u003C00:00, 6707.03it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 179/290 [00:00\u003C00:00, 6728.09it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 179/290 [00:00\u003C00:00, 6720.26it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 180/290 [00:00\u003C00:00, 6741.99it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  \rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 180/290 [00:00\u003C00:00, 6733.75it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 181/290 [00:00\u003C00:00, 6755.25it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 181/290 [00:00\u003C00:00, 6747.81it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 182/290 [00:00\u003C00:00, 6693.59it/s, Materializing param=model.layers.15.input_layernorm.weight] \rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 182/290 [00:00\u003C00:00, 6684.85it/s, Materializing param=model.layers.15.input_layernorm.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 183/290 [00:00\u003C00:00, 6704.44it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 183/290 [00:00\u003C00:00, 6696.66it/s, Materializing param=model.layers.15.mlp.down_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 184/290 [00:00\u003C00:00, 6716.96it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 184/290 [00:00\u003C00:00, 6709.25it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 185/290 [00:00\u003C00:00, 6729.92it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  \rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 185/290 [00:00\u003C00:00, 6721.70it/s, Materializing param=model.layers.15.mlp.up_proj.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 186/290 [00:00\u003C00:00, 6742.03it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 186/290 [00:00\u003C00:00, 6733.36it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 187/290 [00:00\u003C00:00, 6753.47it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          \rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 187/290 [00:00\u003C00:00, 6745.69it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188/290 [00:00\u003C00:00, 6749.55it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188/290 [00:00\u003C00:00, 6741.35it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 189/290 [00:00\u003C00:00, 6761.54it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 189/290 [00:00\u003C00:00, 6753.65it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 190/290 [00:00\u003C00:00, 6773.80it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  \rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 190/290 [00:00\u003C00:00, 6765.47it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 191/290 [00:00\u003C00:00, 6783.34it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 191/290 [00:00\u003C00:00, 6775.48it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 192/290 [00:00\u003C00:00, 6795.83it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  \rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 192/290 [00:00\u003C00:00, 6788.16it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 193/290 [00:00\u003C00:00, 6807.96it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 193/290 [00:00\u003C00:00, 6800.30it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 194/290 [00:00\u003C00:00, 6790.41it/s, Materializing param=model.layers.16.input_layernorm.weight] \rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 194/290 [00:00\u003C00:00, 6781.64it/s, Materializing param=model.layers.16.input_layernorm.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 195/290 [00:00\u003C00:00, 6535.12it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  \rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 195/290 [00:00\u003C00:00, 6526.67it/s, Materializing param=model.layers.16.mlp.down_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 196/290 [00:00\u003C00:00, 6544.94it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 196/290 [00:00\u003C00:00, 6537.96it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 197/290 [00:00\u003C00:00, 6556.82it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  \rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 197/290 [00:00\u003C00:00, 6549.81it/s, Materializing param=model.layers.16.mlp.up_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 198/290 [00:00\u003C00:00, 6567.59it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 198/290 [00:00\u003C00:00, 6559.81it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 199/290 [00:00\u003C00:00, 6577.87it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          \rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 199/290 [00:00\u003C00:00, 6570.68it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 200/290 [00:00\u003C00:00, 6588.86it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 200/290 [00:00\u003C00:00, 6581.42it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 201/290 [00:00\u003C00:00, 6599.88it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 201/290 [00:00\u003C00:00, 6592.70it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 202/290 [00:00\u003C00:00, 6611.54it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  \rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 202/290 [00:00\u003C00:00, 6604.48it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 203/290 [00:00\u003C00:00, 6622.21it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 203/290 [00:00\u003C00:00, 6613.82it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 204/290 [00:00\u003C00:00, 6632.70it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  \rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 204/290 [00:00\u003C00:00, 6625.97it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 205/290 [00:00\u003C00:00, 6644.81it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 205/290 [00:00\u003C00:00, 6637.53it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 206/290 [00:00\u003C00:00, 6656.04it/s, Materializing param=model.layers.17.input_layernorm.weight] \rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 206/290 [00:00\u003C00:00, 6648.86it/s, Materializing param=model.layers.17.input_layernorm.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 207/290 [00:00\u003C00:00, 6667.19it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  \rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 207/290 [00:00\u003C00:00, 6660.33it/s, Materializing param=model.layers.17.mlp.down_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 208/290 [00:00\u003C00:00, 6676.99it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 208/290 [00:00\u003C00:00, 6669.94it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 209/290 [00:00\u003C00:00, 6688.71it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  \rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 209/290 [00:00\u003C00:00, 6681.73it/s, Materializing param=model.layers.17.mlp.up_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 210/290 [00:00\u003C00:00, 6700.01it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 210/290 [00:00\u003C00:00, 6692.99it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 211/290 [00:00\u003C00:00, 6711.14it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          \rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 211/290 [00:00\u003C00:00, 6704.07it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 212/290 [00:00\u003C00:00, 6722.00it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 212/290 [00:00\u003C00:00, 6715.24it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 213/290 [00:00\u003C00:00, 6731.77it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 213/290 [00:00\u003C00:00, 6724.93it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 214/290 [00:00\u003C00:00, 6729.75it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  \rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 214/290 [00:00\u003C00:00, 6722.30it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 215/290 [00:00\u003C00:00, 6739.02it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 215/290 [00:00\u003C00:00, 6732.28it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 216/290 [00:00\u003C00:00, 6750.04it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  \rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 216/290 [00:00\u003C00:00, 6743.10it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 217/290 [00:00\u003C00:00, 6759.68it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 217/290 [00:00\u003C00:00, 6752.96it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 218/290 [00:00\u003C00:00, 6770.97it/s, Materializing param=model.layers.18.input_layernorm.weight] \rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 218/290 [00:00\u003C00:00, 6764.01it/s, Materializing param=model.layers.18.input_layernorm.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 219/290 [00:00\u003C00:00, 6781.54it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  \rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 219/290 [00:00\u003C00:00, 6774.74it/s, Materializing param=model.layers.18.mlp.down_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 220/290 [00:00\u003C00:00, 6792.40it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 220/290 [00:00\u003C00:00, 6785.65it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 221/290 [00:00\u003C00:00, 6803.49it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  \rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 221/290 [00:00\u003C00:00, 6796.75it/s, Materializing param=model.layers.18.mlp.up_proj.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 222/290 [00:00\u003C00:00, 6759.51it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 222/290 [00:00\u003C00:00, 6751.96it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 223/290 [00:00\u003C00:00, 6767.65it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          \rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 223/290 [00:00\u003C00:00, 6760.70it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 224/290 [00:00\u003C00:00, 6777.40it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 224/290 [00:00\u003C00:00, 6770.76it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 225/290 [00:00\u003C00:00, 6785.63it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 225/290 [00:00\u003C00:00, 6778.32it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 226/290 [00:00\u003C00:00, 6794.73it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  \rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 226/290 [00:00\u003C00:00, 6788.26it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 227/290 [00:00\u003C00:00, 6792.03it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 227/290 [00:00\u003C00:00, 6785.50it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 228/290 [00:00\u003C00:00, 6736.41it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  \rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 228/290 [00:00\u003C00:00, 6729.49it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 229/290 [00:00\u003C00:00, 6745.29it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 229/290 [00:00\u003C00:00, 6738.81it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 230/290 [00:00\u003C00:00, 6755.30it/s, Materializing param=model.layers.19.input_layernorm.weight] \rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 230/290 [00:00\u003C00:00, 6748.73it/s, Materializing param=model.layers.19.input_layernorm.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 231/290 [00:00\u003C00:00, 6751.15it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  \rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 231/290 [00:00\u003C00:00, 6744.90it/s, Materializing param=model.layers.19.mlp.down_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 232/290 [00:00\u003C00:00, 6760.73it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 232/290 [00:00\u003C00:00, 6754.35it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 233/290 [00:00\u003C00:00, 6770.96it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  \rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 233/290 [00:00\u003C00:00, 6764.91it/s, Materializing param=model.layers.19.mlp.up_proj.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 234/290 [00:00\u003C00:00, 6734.56it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 234/290 [00:00\u003C00:00, 6727.58it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 235/290 [00:00\u003C00:00, 6743.53it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          \rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 235/290 [00:00\u003C00:00, 6737.08it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 236/290 [00:00\u003C00:00, 6752.78it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 236/290 [00:00\u003C00:00, 6746.61it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 237/290 [00:00\u003C00:00, 6762.75it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 237/290 [00:00\u003C00:00, 6756.41it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 238/290 [00:00\u003C00:00, 6770.60it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  \rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 238/290 [00:00\u003C00:00, 6764.46it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 239/290 [00:00\u003C00:00, 6778.78it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 239/290 [00:00\u003C00:00, 6771.82it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 240/290 [00:00\u003C00:00, 6738.38it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  \rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 240/290 [00:00\u003C00:00, 6731.80it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 241/290 [00:00\u003C00:00, 6747.35it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 241/290 [00:00\u003C00:00, 6741.10it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 242/290 [00:00\u003C00:00, 6756.59it/s, Materializing param=model.layers.20.input_layernorm.weight] \rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 242/290 [00:00\u003C00:00, 6750.29it/s, Materializing param=model.layers.20.input_layernorm.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 243/290 [00:00\u003C00:00, 6765.99it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  \rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 243/290 [00:00\u003C00:00, 6759.94it/s, Materializing param=model.layers.20.mlp.down_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 244/290 [00:00\u003C00:00, 6748.63it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 244/290 [00:00\u003C00:00, 6742.19it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 245/290 [00:00\u003C00:00, 6675.01it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  \rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 245/290 [00:00\u003C00:00, 6668.64it/s, Materializing param=model.layers.20.mlp.up_proj.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 246/290 [00:00\u003C00:00, 6683.72it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 246/290 [00:00\u003C00:00, 6677.57it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 247/290 [00:00\u003C00:00, 6692.81it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          \rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 247/290 [00:00\u003C00:00, 6686.98it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 248/290 [00:00\u003C00:00, 6702.32it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 248/290 [00:00\u003C00:00, 6696.41it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 249/290 [00:00\u003C00:00, 6710.37it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 249/290 [00:00\u003C00:00, 6704.51it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 250/290 [00:00\u003C00:00, 6719.96it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  \rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 250/290 [00:00\u003C00:00, 6714.28it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 251/290 [00:00\u003C00:00, 6729.68it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 251/290 [00:00\u003C00:00, 6723.74it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 252/290 [00:00\u003C00:00, 6738.05it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  \rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 252/290 [00:00\u003C00:00, 6732.09it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 253/290 [00:00\u003C00:00, 6747.24it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 253/290 [00:00\u003C00:00, 6741.41it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 254/290 [00:00\u003C00:00, 6756.21it/s, Materializing param=model.layers.21.input_layernorm.weight] \rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 254/290 [00:00\u003C00:00, 6750.77it/s, Materializing param=model.layers.21.input_layernorm.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 255/290 [00:00\u003C00:00, 6738.11it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  \rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 255/290 [00:00\u003C00:00, 6732.09it/s, Materializing param=model.layers.21.mlp.down_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 256/290 [00:00\u003C00:00, 6746.26it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 256/290 [00:00\u003C00:00, 6740.54it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 257/290 [00:00\u003C00:00, 6755.72it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  \rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 257/290 [00:00\u003C00:00, 6749.76it/s, Materializing param=model.layers.21.mlp.up_proj.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 258/290 [00:00\u003C00:00, 6735.66it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 258/290 [00:00\u003C00:00, 6729.42it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 259/290 [00:00\u003C00:00, 6728.05it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          \rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 259/290 [00:00\u003C00:00, 6722.14it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 260/290 [00:00\u003C00:00, 6723.71it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 260/290 [00:00\u003C00:00, 6717.87it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 261/290 [00:00\u003C00:00, 6732.18it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 261/290 [00:00\u003C00:00, 6726.39it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 262/290 [00:00\u003C00:00, 6683.99it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  \rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 262/290 [00:00\u003C00:00, 6676.52it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 263/290 [00:00\u003C00:00, 6689.85it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 263/290 [00:00\u003C00:00, 6684.29it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 264/290 [00:00\u003C00:00, 6698.71it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  \rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 264/290 [00:00\u003C00:00, 6693.28it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 265/290 [00:00\u003C00:00, 6707.40it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 265/290 [00:00\u003C00:00, 6701.66it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 266/290 [00:00\u003C00:00, 6715.73it/s, Materializing param=model.layers.22.input_layernorm.weight] \rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 266/290 [00:00\u003C00:00, 6710.16it/s, Materializing param=model.layers.22.input_layernorm.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 267/290 [00:00\u003C00:00, 6724.22it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  \rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 267/290 [00:00\u003C00:00, 6718.42it/s, Materializing param=model.layers.22.mlp.down_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 268/290 [00:00\u003C00:00, 6732.51it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 268/290 [00:00\u003C00:00, 6727.07it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 269/290 [00:00\u003C00:00, 6741.16it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  \rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 269/290 [00:00\u003C00:00, 6735.81it/s, Materializing param=model.layers.22.mlp.up_proj.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 270/290 [00:00\u003C00:00, 6699.77it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 270/290 [00:00\u003C00:00, 6693.55it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 271/290 [00:00\u003C00:00, 6706.69it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          \rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 271/290 [00:00\u003C00:00, 6701.00it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 272/290 [00:00\u003C00:00, 6713.81it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 272/290 [00:00\u003C00:00, 6708.24it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 273/290 [00:00\u003C00:00, 6721.48it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 273/290 [00:00\u003C00:00, 6716.08it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 274/290 [00:00\u003C00:00, 6694.82it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  \rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 274/290 [00:00\u003C00:00, 6689.17it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 275/290 [00:00\u003C00:00, 6702.62it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 275/290 [00:00\u003C00:00, 6697.25it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 276/290 [00:00\u003C00:00, 6710.58it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  \rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 276/290 [00:00\u003C00:00, 6705.52it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 277/290 [00:00\u003C00:00, 6719.19it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 277/290 [00:00\u003C00:00, 6714.03it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 278/290 [00:00\u003C00:00, 6727.85it/s, Materializing param=model.layers.23.input_layernorm.weight] \rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 278/290 [00:00\u003C00:00, 6722.80it/s, Materializing param=model.layers.23.input_layernorm.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 279/290 [00:00\u003C00:00, 6736.07it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  \rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 279/290 [00:00\u003C00:00, 6730.77it/s, Materializing param=model.layers.23.mlp.down_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 280/290 [00:00\u003C00:00, 6744.30it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 280/290 [00:00\u003C00:00, 6739.23it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 281/290 [00:00\u003C00:00, 6726.70it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  \rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 281/290 [00:00\u003C00:00, 6721.49it/s, Materializing param=model.layers.23.mlp.up_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 282/290 [00:00\u003C00:00, 6731.86it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 282/290 [00:00\u003C00:00, 6726.23it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 283/290 [00:00\u003C00:00, 6739.12it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          \rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 283/290 [00:00\u003C00:00, 6734.15it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 284/290 [00:00\u003C00:00, 6705.14it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 284/290 [00:00\u003C00:00, 6699.68it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 285/290 [00:00\u003C00:00, 6709.79it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 285/290 [00:00\u003C00:00, 6704.53it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 286/290 [00:00\u003C00:00, 6712.76it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  \rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 286/290 [00:00\u003C00:00, 6707.81it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 287/290 [00:00\u003C00:00, 6721.90it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 287/290 [00:00\u003C00:00, 6717.03it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 288/290 [00:00\u003C00:00, 6731.45it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  \rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 288/290 [00:00\u003C00:00, 6726.66it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 289/290 [00:00\u003C00:00, 6741.34it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 289/290 [00:00\u003C00:00, 6736.25it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 6751.08it/s, Materializing param=model.norm.weight]                      \rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 6746.28it/s, Materializing param=model.norm.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 6733.03it/s, Materializing param=model.norm.weight]\n\rLoading weights:   0%|          | 0/290 [00:00\u003C?, ?it/s]\rLoading weights:   0%|          | 1/290 [00:00\u003C00:00, 17697.49it/s, Materializing param=model.embed_tokens.weight]\rLoading weights:   0%|          | 1/290 [00:00\u003C00:00, 7598.38it/s, Materializing param=model.embed_tokens.weight] \rLoading weights:   1%|          | 2/290 [00:00\u003C00:00, 7078.99it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|          | 2/290 [00:00\u003C00:00, 6100.81it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|          | 3/290 [00:00\u003C00:00, 6288.31it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  \rLoading weights:   1%|          | 3/290 [00:00\u003C00:00, 5809.29it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   1%|\u258f         | 4/290 [00:00\u003C00:00, 6660.27it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   1%|\u258f         | 4/290 [00:00\u003C00:00, 6236.88it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   2%|\u258f         | 5/290 [00:00\u003C00:00, 6971.91it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \rLoading weights:   2%|\u258f         | 5/290 [00:00\u003C00:00, 6630.26it/s, Materializing param=model.layers.0.mlp.up_proj.weight]\rLoading weights:   2%|\u258f         | 6/290 [00:00\u003C00:00, 7227.40it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   2%|\u258f         | 6/290 [00:00\u003C00:00, 6906.10it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   2%|\u258f         | 7/290 [00:00\u003C00:00, 7376.92it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          \rLoading weights:   2%|\u258f         | 7/290 [00:00\u003C00:00, 7124.52it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]\rLoading weights:   3%|\u258e         | 8/290 [00:00\u003C00:00, 7553.90it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|\u258e         | 8/290 [00:00\u003C00:00, 7251.88it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|\u258e         | 9/290 [00:00\u003C00:00, 6740.85it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   3%|\u258e         | 9/290 [00:00\u003C00:00, 6553.60it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   3%|\u258e         | 10/290 [00:00\u003C00:00, 6923.58it/s, Materializing param=model.layers.0.self_attn.q_proj.bias] \rLoading weights:   3%|\u258e         | 10/290 [00:00\u003C00:00, 6760.64it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]\rLoading weights:   4%|\u258d         | 11/290 [00:00\u003C00:00, 7108.99it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   4%|\u258d         | 11/290 [00:00\u003C00:00, 6958.88it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   4%|\u258d         | 12/290 [00:00\u003C00:00, 7261.82it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  \rLoading weights:   4%|\u258d         | 12/290 [00:00\u003C00:00, 7115.02it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]\rLoading weights:   4%|\u258d         | 13/290 [00:00\u003C00:00, 7418.50it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   4%|\u258d         | 13/290 [00:00\u003C00:00, 7271.10it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   5%|\u258d         | 14/290 [00:00\u003C00:00, 7527.27it/s, Materializing param=model.layers.1.input_layernorm.weight] \rLoading weights:   5%|\u258d         | 14/290 [00:00\u003C00:00, 7379.70it/s, Materializing param=model.layers.1.input_layernorm.weight]\rLoading weights:   5%|\u258c         | 15/290 [00:00\u003C00:00, 7640.83it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  \rLoading weights:   5%|\u258c         | 15/290 [00:00\u003C00:00, 7510.39it/s, Materializing param=model.layers.1.mlp.down_proj.weight]\rLoading weights:   6%|\u258c         | 16/290 [00:00\u003C00:00, 7735.00it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   6%|\u258c         | 16/290 [00:00\u003C00:00, 7577.79it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   6%|\u258c         | 17/290 [00:00\u003C00:00, 7108.28it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  \rLoading weights:   6%|\u258c         | 17/290 [00:00\u003C00:00, 7003.55it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\rLoading weights:   6%|\u258c         | 18/290 [00:00\u003C00:00, 7216.35it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:   6%|\u258c         | 18/290 [00:00\u003C00:00, 7111.00it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:   7%|\u258b         | 19/290 [00:00\u003C00:00, 7299.79it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          \rLoading weights:   7%|\u258b         | 19/290 [00:00\u003C00:00, 7204.10it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]\rLoading weights:   7%|\u258b         | 20/290 [00:00\u003C00:00, 7397.36it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:   7%|\u258b         | 20/290 [00:00\u003C00:00, 7307.15it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:   7%|\u258b         | 21/290 [00:00\u003C00:00, 7487.28it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:   7%|\u258b         | 21/290 [00:00\u003C00:00, 7399.85it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:   8%|\u258a         | 22/290 [00:00\u003C00:00, 7389.66it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  \rLoading weights:   8%|\u258a         | 22/290 [00:00\u003C00:00, 7304.84it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]\rLoading weights:   8%|\u258a         | 23/290 [00:00\u003C00:00, 7462.02it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:   8%|\u258a         | 23/290 [00:00\u003C00:00, 7360.11it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:   8%|\u258a         | 24/290 [00:00\u003C00:00, 7516.11it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  \rLoading weights:   8%|\u258a         | 24/290 [00:00\u003C00:00, 7440.56it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]\rLoading weights:   9%|\u258a         | 25/290 [00:00\u003C00:00, 7590.13it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:   9%|\u258a         | 25/290 [00:00\u003C00:00, 7513.98it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:   9%|\u2589         | 26/290 [00:00\u003C00:00, 7664.06it/s, Materializing param=model.layers.2.input_layernorm.weight] \rLoading weights:   9%|\u2589         | 26/290 [00:00\u003C00:00, 7590.97it/s, Materializing param=model.layers.2.input_layernorm.weight]\rLoading weights:   9%|\u2589         | 27/290 [00:00\u003C00:00, 7728.53it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  \rLoading weights:   9%|\u2589         | 27/290 [00:00\u003C00:00, 7653.84it/s, Materializing param=model.layers.2.mlp.down_proj.weight]\rLoading weights:  10%|\u2589         | 28/290 [00:00\u003C00:00, 7787.83it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  10%|\u2589         | 28/290 [00:00\u003C00:00, 7715.69it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  10%|\u2588         | 29/290 [00:00\u003C00:00, 7841.34it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  \rLoading weights:  10%|\u2588         | 29/290 [00:00\u003C00:00, 7761.78it/s, Materializing param=model.layers.2.mlp.up_proj.weight]\rLoading weights:  10%|\u2588         | 30/290 [00:00\u003C00:00, 7884.52it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  10%|\u2588         | 30/290 [00:00\u003C00:00, 7813.05it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  11%|\u2588         | 31/290 [00:00\u003C00:00, 7651.14it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          \rLoading weights:  11%|\u2588         | 31/290 [00:00\u003C00:00, 7583.75it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]\rLoading weights:  11%|\u2588         | 32/290 [00:00\u003C00:00, 7680.56it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  11%|\u2588         | 32/290 [00:00\u003C00:00, 7619.51it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  11%|\u2588\u258f        | 33/290 [00:00\u003C00:00, 7727.34it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  11%|\u2588\u258f        | 33/290 [00:00\u003C00:00, 7666.98it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 34/290 [00:00\u003C00:00, 7771.46it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  \rLoading weights:  12%|\u2588\u258f        | 34/290 [00:00\u003C00:00, 7708.87it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]\rLoading weights:  12%|\u2588\u258f        | 35/290 [00:00\u003C00:00, 6985.85it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 35/290 [00:00\u003C00:00, 6935.03it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 36/290 [00:00\u003C00:00, 6990.51it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  \rLoading weights:  12%|\u2588\u258f        | 36/290 [00:00\u003C00:00, 6946.13it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]\rLoading weights:  13%|\u2588\u258e        | 37/290 [00:00\u003C00:00, 7045.73it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  13%|\u2588\u258e        | 37/290 [00:00\u003C00:00, 7003.44it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  13%|\u2588\u258e        | 38/290 [00:00\u003C00:00, 7099.17it/s, Materializing param=model.layers.3.input_layernorm.weight] \rLoading weights:  13%|\u2588\u258e        | 38/290 [00:00\u003C00:00, 7055.18it/s, Materializing param=model.layers.3.input_layernorm.weight]\rLoading weights:  13%|\u2588\u258e        | 39/290 [00:00\u003C00:00, 7152.20it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \rLoading weights:  13%|\u2588\u258e        | 39/290 [00:00\u003C00:00, 7109.92it/s, Materializing param=model.layers.3.mlp.down_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 40/290 [00:00\u003C00:00, 7202.38it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 40/290 [00:00\u003C00:00, 7162.71it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 41/290 [00:00\u003C00:00, 7259.95it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  \rLoading weights:  14%|\u2588\u258d        | 41/290 [00:00\u003C00:00, 7218.20it/s, Materializing param=model.layers.3.mlp.up_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 42/290 [00:00\u003C00:00, 7308.06it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  14%|\u2588\u258d        | 42/290 [00:00\u003C00:00, 7264.66it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  15%|\u2588\u258d        | 43/290 [00:00\u003C00:00, 7353.93it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          \rLoading weights:  15%|\u2588\u258d        | 43/290 [00:00\u003C00:00, 7311.89it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]\rLoading weights:  15%|\u2588\u258c        | 44/290 [00:00\u003C00:00, 7400.03it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  15%|\u2588\u258c        | 44/290 [00:00\u003C00:00, 7358.43it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 45/290 [00:00\u003C00:00, 7443.75it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 45/290 [00:00\u003C00:00, 7404.33it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 46/290 [00:00\u003C00:00, 7480.83it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  \rLoading weights:  16%|\u2588\u258c        | 46/290 [00:00\u003C00:00, 7442.45it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]\rLoading weights:  16%|\u2588\u258c        | 47/290 [00:00\u003C00:00, 7307.69it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 47/290 [00:00\u003C00:00, 7268.09it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 48/290 [00:00\u003C00:00, 7349.56it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  \rLoading weights:  17%|\u2588\u258b        | 48/290 [00:00\u003C00:00, 7314.05it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]\rLoading weights:  17%|\u2588\u258b        | 49/290 [00:00\u003C00:00, 7392.31it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 49/290 [00:00\u003C00:00, 7358.69it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 50/290 [00:00\u003C00:00, 7437.76it/s, Materializing param=model.layers.4.input_layernorm.weight] \rLoading weights:  17%|\u2588\u258b        | 50/290 [00:00\u003C00:00, 7403.37it/s, Materializing param=model.layers.4.input_layernorm.weight]\rLoading weights:  18%|\u2588\u258a        | 51/290 [00:00\u003C00:00, 7186.13it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  \rLoading weights:  18%|\u2588\u258a        | 51/290 [00:00\u003C00:00, 7152.01it/s, Materializing param=model.layers.4.mlp.down_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 52/290 [00:00\u003C00:00, 7220.07it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 52/290 [00:00\u003C00:00, 7187.23it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 53/290 [00:00\u003C00:00, 7259.90it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  \rLoading weights:  18%|\u2588\u258a        | 53/290 [00:00\u003C00:00, 7226.39it/s, Materializing param=model.layers.4.mlp.up_proj.weight]\rLoading weights:  19%|\u2588\u258a        | 54/290 [00:00\u003C00:00, 7298.67it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  19%|\u2588\u258a        | 54/290 [00:00\u003C00:00, 7266.13it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  19%|\u2588\u2589        | 55/290 [00:00\u003C00:00, 7043.01it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          \rLoading weights:  19%|\u2588\u2589        | 55/290 [00:00\u003C00:00, 7003.03it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]\rLoading weights:  19%|\u2588\u2589        | 56/290 [00:00\u003C00:00, 6737.26it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  19%|\u2588\u2589        | 56/290 [00:00\u003C00:00, 6708.78it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  20%|\u2588\u2589        | 57/290 [00:00\u003C00:00, 6769.41it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  20%|\u2588\u2589        | 57/290 [00:00\u003C00:00, 6745.35it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  20%|\u2588\u2588        | 58/290 [00:00\u003C00:00, 6678.46it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  \rLoading weights:  20%|\u2588\u2588        | 58/290 [00:00\u003C00:00, 6651.80it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]\rLoading weights:  20%|\u2588\u2588        | 59/290 [00:00\u003C00:00, 6711.98it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  20%|\u2588\u2588        | 59/290 [00:00\u003C00:00, 6688.58it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  21%|\u2588\u2588        | 60/290 [00:00\u003C00:00, 6749.22it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  \rLoading weights:  21%|\u2588\u2588        | 60/290 [00:00\u003C00:00, 6724.51it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]\rLoading weights:  21%|\u2588\u2588        | 61/290 [00:00\u003C00:00, 6782.76it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  21%|\u2588\u2588        | 61/290 [00:00\u003C00:00, 6757.86it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  21%|\u2588\u2588\u258f       | 62/290 [00:00\u003C00:00, 6805.19it/s, Materializing param=model.layers.5.input_layernorm.weight] \rLoading weights:  21%|\u2588\u2588\u258f       | 62/290 [00:00\u003C00:00, 6782.47it/s, Materializing param=model.layers.5.input_layernorm.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 63/290 [00:00\u003C00:00, 6837.65it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  \rLoading weights:  22%|\u2588\u2588\u258f       | 63/290 [00:00\u003C00:00, 6812.97it/s, Materializing param=model.layers.5.mlp.down_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 64/290 [00:00\u003C00:00, 6870.80it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 64/290 [00:00\u003C00:00, 6848.02it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 65/290 [00:00\u003C00:00, 6776.44it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \rLoading weights:  22%|\u2588\u2588\u258f       | 65/290 [00:00\u003C00:00, 6751.44it/s, Materializing param=model.layers.5.mlp.up_proj.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 66/290 [00:00\u003C00:00, 6808.10it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 66/290 [00:00\u003C00:00, 6784.24it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 67/290 [00:00\u003C00:00, 6837.43it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          \rLoading weights:  23%|\u2588\u2588\u258e       | 67/290 [00:00\u003C00:00, 6815.38it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]\rLoading weights:  23%|\u2588\u2588\u258e       | 68/290 [00:00\u003C00:00, 6869.95it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 68/290 [00:00\u003C00:00, 6848.34it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 69/290 [00:00\u003C00:00, 6578.63it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 69/290 [00:00\u003C00:00, 6555.68it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 70/290 [00:00\u003C00:00, 6492.44it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  \rLoading weights:  24%|\u2588\u2588\u258d       | 70/290 [00:00\u003C00:00, 6468.70it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]\rLoading weights:  24%|\u2588\u2588\u258d       | 71/290 [00:00\u003C00:00, 6434.65it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 71/290 [00:00\u003C00:00, 6414.00it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  25%|\u2588\u2588\u258d       | 72/290 [00:00\u003C00:00, 6460.51it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  \rLoading weights:  25%|\u2588\u2588\u258d       | 72/290 [00:00\u003C00:00, 6441.63it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]\rLoading weights:  25%|\u2588\u2588\u258c       | 73/290 [00:00\u003C00:00, 6388.02it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  25%|\u2588\u2588\u258c       | 73/290 [00:00\u003C00:00, 6367.30it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 74/290 [00:00\u003C00:00, 6416.09it/s, Materializing param=model.layers.6.input_layernorm.weight] \rLoading weights:  26%|\u2588\u2588\u258c       | 74/290 [00:00\u003C00:00, 6397.31it/s, Materializing param=model.layers.6.input_layernorm.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 75/290 [00:00\u003C00:00, 6447.35it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  \rLoading weights:  26%|\u2588\u2588\u258c       | 75/290 [00:00\u003C00:00, 6428.91it/s, Materializing param=model.layers.6.mlp.down_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 76/290 [00:00\u003C00:00, 6475.85it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 76/290 [00:00\u003C00:00, 6458.40it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 77/290 [00:00\u003C00:00, 6506.99it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  \rLoading weights:  27%|\u2588\u2588\u258b       | 77/290 [00:00\u003C00:00, 6489.08it/s, Materializing param=model.layers.6.mlp.up_proj.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 78/290 [00:00\u003C00:00, 6536.32it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 78/290 [00:00\u003C00:00, 6518.34it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 79/290 [00:00\u003C00:00, 6566.72it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          \rLoading weights:  27%|\u2588\u2588\u258b       | 79/290 [00:00\u003C00:00, 6548.94it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]\rLoading weights:  28%|\u2588\u2588\u258a       | 80/290 [00:00\u003C00:00, 6560.39it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 80/290 [00:00\u003C00:00, 6542.61it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 81/290 [00:00\u003C00:00, 6587.66it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 81/290 [00:00\u003C00:00, 6571.09it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 82/290 [00:00\u003C00:00, 6616.51it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  \rLoading weights:  28%|\u2588\u2588\u258a       | 82/290 [00:00\u003C00:00, 6599.88it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]\rLoading weights:  29%|\u2588\u2588\u258a       | 83/290 [00:00\u003C00:00, 6645.17it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  29%|\u2588\u2588\u258a       | 83/290 [00:00\u003C00:00, 6628.60it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  29%|\u2588\u2588\u2589       | 84/290 [00:00\u003C00:00, 6673.51it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  \rLoading weights:  29%|\u2588\u2588\u2589       | 84/290 [00:00\u003C00:00, 6656.37it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]\rLoading weights:  29%|\u2588\u2588\u2589       | 85/290 [00:00\u003C00:00, 6512.30it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  29%|\u2588\u2588\u2589       | 85/290 [00:00\u003C00:00, 6494.03it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2589       | 86/290 [00:00\u003C00:00, 6533.66it/s, Materializing param=model.layers.7.input_layernorm.weight] \rLoading weights:  30%|\u2588\u2588\u2589       | 86/290 [00:00\u003C00:00, 6517.84it/s, Materializing param=model.layers.7.input_layernorm.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 87/290 [00:00\u003C00:00, 6559.02it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  \rLoading weights:  30%|\u2588\u2588\u2588       | 87/290 [00:00\u003C00:00, 6543.14it/s, Materializing param=model.layers.7.mlp.down_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 88/290 [00:00\u003C00:00, 6333.63it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 88/290 [00:00\u003C00:00, 6317.05it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 89/290 [00:00\u003C00:00, 6292.23it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  \rLoading weights:  31%|\u2588\u2588\u2588       | 89/290 [00:00\u003C00:00, 6276.89it/s, Materializing param=model.layers.7.mlp.up_proj.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 90/290 [00:00\u003C00:00, 6318.41it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 90/290 [00:00\u003C00:00, 6303.43it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  31%|\u2588\u2588\u2588\u258f      | 91/290 [00:00\u003C00:00, 6344.23it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          \rLoading weights:  31%|\u2588\u2588\u2588\u258f      | 91/290 [00:00\u003C00:00, 6329.92it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 92/290 [00:00\u003C00:00, 6366.75it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 92/290 [00:00\u003C00:00, 6352.70it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 93/290 [00:00\u003C00:00, 6393.13it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 93/290 [00:00\u003C00:00, 6378.28it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 94/290 [00:00\u003C00:00, 6418.63it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  \rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 94/290 [00:00\u003C00:00, 6404.45it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 95/290 [00:00\u003C00:00, 6438.28it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 95/290 [00:00\u003C00:00, 6424.27it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 96/290 [00:00\u003C00:00, 6463.96it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  \rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 96/290 [00:00\u003C00:00, 6450.60it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 97/290 [00:00\u003C00:00, 6488.90it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 97/290 [00:00\u003C00:00, 6475.16it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 98/290 [00:00\u003C00:00, 6512.69it/s, Materializing param=model.layers.8.input_layernorm.weight] \rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 98/290 [00:00\u003C00:00, 6498.38it/s, Materializing param=model.layers.8.input_layernorm.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 99/290 [00:00\u003C00:00, 6534.11it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  \rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 99/290 [00:00\u003C00:00, 6520.16it/s, Materializing param=model.layers.8.mlp.down_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 100/290 [00:00\u003C00:00, 6557.80it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 100/290 [00:00\u003C00:00, 6544.09it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258d      | 101/290 [00:00\u003C00:00, 6516.20it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  \rLoading weights:  35%|\u2588\u2588\u2588\u258d      | 101/290 [00:00\u003C00:00, 6501.80it/s, Materializing param=model.layers.8.mlp.up_proj.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258c      | 102/290 [00:00\u003C00:00, 6537.78it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258c      | 102/290 [00:00\u003C00:00, 6523.12it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 103/290 [00:00\u003C00:00, 6558.18it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          \rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 103/290 [00:00\u003C00:00, 6545.16it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 104/290 [00:00\u003C00:00, 6507.07it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 104/290 [00:00\u003C00:00, 6493.12it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 105/290 [00:00\u003C00:00, 6528.05it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 105/290 [00:00\u003C00:00, 6513.57it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 106/290 [00:00\u003C00:00, 6412.29it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  \rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 106/290 [00:00\u003C00:00, 6398.82it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 107/290 [00:00\u003C00:00, 6432.80it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 107/290 [00:00\u003C00:00, 6419.36it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 108/290 [00:00\u003C00:00, 6453.42it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  \rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 108/290 [00:00\u003C00:00, 6440.30it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 109/290 [00:00\u003C00:00, 6472.69it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 109/290 [00:00\u003C00:00, 6459.43it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 110/290 [00:00\u003C00:00, 6493.00it/s, Materializing param=model.layers.9.input_layernorm.weight] \rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 110/290 [00:00\u003C00:00, 6480.15it/s, Materializing param=model.layers.9.input_layernorm.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 111/290 [00:00\u003C00:00, 6513.53it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  \rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 111/290 [00:00\u003C00:00, 6501.25it/s, Materializing param=model.layers.9.mlp.down_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u258a      | 112/290 [00:00\u003C00:00, 6530.37it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u258a      | 112/290 [00:00\u003C00:00, 6518.05it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 113/290 [00:00\u003C00:00, 6496.65it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  \rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 113/290 [00:00\u003C00:00, 6483.14it/s, Materializing param=model.layers.9.mlp.up_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 114/290 [00:00\u003C00:00, 6514.58it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 114/290 [00:00\u003C00:00, 6501.65it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2589      | 115/290 [00:00\u003C00:00, 6444.93it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          \rLoading weights:  40%|\u2588\u2588\u2588\u2589      | 115/290 [00:00\u003C00:00, 6432.12it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 116/290 [00:00\u003C00:00, 6462.46it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 116/290 [00:00\u003C00:00, 6450.12it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 117/290 [00:00\u003C00:00, 6176.48it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 117/290 [00:00\u003C00:00, 6164.38it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 118/290 [00:00\u003C00:00, 6193.33it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  \rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 118/290 [00:00\u003C00:00, 6182.73it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 119/290 [00:00\u003C00:00, 6211.46it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 119/290 [00:00\u003C00:00, 6201.12it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588\u258f     | 120/290 [00:00\u003C00:00, 6231.17it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  \rLoading weights:  41%|\u2588\u2588\u2588\u2588\u258f     | 120/290 [00:00\u003C00:00, 6220.46it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 121/290 [00:00\u003C00:00, 6250.44it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 121/290 [00:00\u003C00:00, 6239.91it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 122/290 [00:00\u003C00:00, 6267.44it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 122/290 [00:00\u003C00:00, 6257.02it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 123/290 [00:00\u003C00:00, 6286.09it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  \rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 123/290 [00:00\u003C00:00, 6276.15it/s, Materializing param=model.layers.10.mlp.down_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 124/290 [00:00\u003C00:00, 6305.77it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 124/290 [00:00\u003C00:00, 6294.32it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 125/290 [00:00\u003C00:00, 6322.51it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  \rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 125/290 [00:00\u003C00:00, 6312.08it/s, Materializing param=model.layers.10.mlp.up_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 126/290 [00:00\u003C00:00, 6213.49it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 126/290 [00:00\u003C00:00, 6201.61it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 127/290 [00:00\u003C00:00, 6228.02it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          \rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 127/290 [00:00\u003C00:00, 6217.70it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 128/290 [00:00\u003C00:00, 6245.59it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 128/290 [00:00\u003C00:00, 6235.22it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 129/290 [00:00\u003C00:00, 6263.85it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 129/290 [00:00\u003C00:00, 6254.08it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258d     | 130/290 [00:00\u003C00:00, 6282.08it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  \rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258d     | 130/290 [00:00\u003C00:00, 6272.69it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258c     | 131/290 [00:00\u003C00:00, 6301.87it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258c     | 131/290 [00:00\u003C00:00, 6292.70it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 132/290 [00:00\u003C00:00, 6185.53it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  \rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 132/290 [00:00\u003C00:00, 6152.94it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 133/290 [00:00\u003C00:00, 6168.71it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 133/290 [00:00\u003C00:00, 6157.00it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 134/290 [00:00\u003C00:00, 6180.03it/s, Materializing param=model.layers.11.input_layernorm.weight] \rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 134/290 [00:00\u003C00:00, 6170.19it/s, Materializing param=model.layers.11.input_layernorm.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 135/290 [00:00\u003C00:00, 6104.39it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  \rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 135/290 [00:00\u003C00:00, 6093.94it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 136/290 [00:00\u003C00:00, 6116.77it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 136/290 [00:00\u003C00:00, 6107.66it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 137/290 [00:00\u003C00:00, 6131.63it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 137/290 [00:00\u003C00:00, 6119.88it/s, Materializing param=model.layers.11.mlp.up_proj.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 138/290 [00:00\u003C00:00, 6141.33it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 138/290 [00:00\u003C00:00, 6131.31it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 139/290 [00:00\u003C00:00, 6156.24it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          \rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 139/290 [00:00\u003C00:00, 6146.83it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 140/290 [00:00\u003C00:00, 6171.53it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 140/290 [00:00\u003C00:00, 6162.46it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u258a     | 141/290 [00:00\u003C00:00, 6077.14it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u258a     | 141/290 [00:00\u003C00:00, 6067.04it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 142/290 [00:00\u003C00:00, 6090.26it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  \rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 142/290 [00:00\u003C00:00, 6082.05it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 143/290 [00:00\u003C00:00, 6106.80it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 143/290 [00:00\u003C00:00, 6098.23it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2589     | 144/290 [00:00\u003C00:00, 6124.13it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  \rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2589     | 144/290 [00:00\u003C00:00, 6114.83it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 145/290 [00:00\u003C00:00, 6139.95it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 145/290 [00:00\u003C00:00, 6131.28it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 146/290 [00:00\u003C00:00, 6156.00it/s, Materializing param=model.layers.12.input_layernorm.weight] \rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 146/290 [00:00\u003C00:00, 6147.29it/s, Materializing param=model.layers.12.input_layernorm.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 147/290 [00:00\u003C00:00, 6062.44it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  \rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 147/290 [00:00\u003C00:00, 6053.75it/s, Materializing param=model.layers.12.mlp.down_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 148/290 [00:00\u003C00:00, 6075.49it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 148/290 [00:00\u003C00:00, 6067.00it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 149/290 [00:00\u003C00:00, 6090.19it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  \rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 149/290 [00:00\u003C00:00, 6082.37it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 150/290 [00:00\u003C00:00, 6105.84it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 150/290 [00:00\u003C00:00, 6097.38it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 151/290 [00:00\u003C00:00, 6119.94it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          \rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 151/290 [00:00\u003C00:00, 6111.73it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 152/290 [00:00\u003C00:00, 6104.54it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 152/290 [00:00\u003C00:00, 6095.85it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 153/290 [00:00\u003C00:00, 6119.74it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 153/290 [00:00\u003C00:00, 6111.70it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 154/290 [00:00\u003C00:00, 6069.90it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  \rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 154/290 [00:00\u003C00:00, 6061.08it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 155/290 [00:00\u003C00:00, 6041.81it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 155/290 [00:00\u003C00:00, 6033.23it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 156/290 [00:00\u003C00:00, 6055.86it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  \rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 156/290 [00:00\u003C00:00, 6047.86it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 157/290 [00:00\u003C00:00, 6070.86it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 157/290 [00:00\u003C00:00, 6063.25it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 158/290 [00:00\u003C00:00, 6085.29it/s, Materializing param=model.layers.13.input_layernorm.weight] \rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 158/290 [00:00\u003C00:00, 6077.87it/s, Materializing param=model.layers.13.input_layernorm.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 159/290 [00:00\u003C00:00, 6018.09it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  \rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 159/290 [00:00\u003C00:00, 6010.01it/s, Materializing param=model.layers.13.mlp.down_proj.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 160/290 [00:00\u003C00:00, 6030.96it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 160/290 [00:00\u003C00:00, 6023.27it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 161/290 [00:00\u003C00:00, 6045.13it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  \rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 161/290 [00:00\u003C00:00, 6037.67it/s, Materializing param=model.layers.13.mlp.up_proj.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 162/290 [00:00\u003C00:00, 6059.24it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 162/290 [00:00\u003C00:00, 6051.31it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 163/290 [00:00\u003C00:00, 6031.78it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          \rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 163/290 [00:00\u003C00:00, 6023.70it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 164/290 [00:00\u003C00:00, 6027.25it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 164/290 [00:00\u003C00:00, 6019.60it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 165/290 [00:00\u003C00:00, 5884.62it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 165/290 [00:00\u003C00:00, 5876.82it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 166/290 [00:00\u003C00:00, 5895.62it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  \rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 166/290 [00:00\u003C00:00, 5888.19it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 167/290 [00:00\u003C00:00, 5909.07it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 167/290 [00:00\u003C00:00, 5902.09it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 168/290 [00:00\u003C00:00, 5922.32it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  \rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 168/290 [00:00\u003C00:00, 5915.11it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 169/290 [00:00\u003C00:00, 5935.92it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 169/290 [00:00\u003C00:00, 5928.92it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 170/290 [00:00\u003C00:00, 5950.51it/s, Materializing param=model.layers.14.input_layernorm.weight] \rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 170/290 [00:00\u003C00:00, 5943.91it/s, Materializing param=model.layers.14.input_layernorm.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 171/290 [00:00\u003C00:00, 5942.27it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  \rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 171/290 [00:00\u003C00:00, 5934.94it/s, Materializing param=model.layers.14.mlp.down_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 172/290 [00:00\u003C00:00, 5955.60it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 172/290 [00:00\u003C00:00, 5948.68it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 173/290 [00:00\u003C00:00, 5969.29it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  \rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 173/290 [00:00\u003C00:00, 5961.98it/s, Materializing param=model.layers.14.mlp.up_proj.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 174/290 [00:00\u003C00:00, 5982.29it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 174/290 [00:00\u003C00:00, 5972.79it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 175/290 [00:00\u003C00:00, 5979.56it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          \rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 175/290 [00:00\u003C00:00, 5972.26it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 176/290 [00:00\u003C00:00, 5992.15it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 176/290 [00:00\u003C00:00, 5985.16it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 177/290 [00:00\u003C00:00, 6005.43it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 177/290 [00:00\u003C00:00, 5998.25it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 178/290 [00:00\u003C00:00, 5995.71it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  \rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 178/290 [00:00\u003C00:00, 5988.64it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 179/290 [00:00\u003C00:00, 6007.93it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 179/290 [00:00\u003C00:00, 6001.25it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 180/290 [00:00\u003C00:00, 6021.44it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  \rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 180/290 [00:00\u003C00:00, 6014.97it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 181/290 [00:00\u003C00:00, 6034.68it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 181/290 [00:00\u003C00:00, 6027.88it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 182/290 [00:00\u003C00:00, 6045.25it/s, Materializing param=model.layers.15.input_layernorm.weight] \rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 182/290 [00:00\u003C00:00, 6038.79it/s, Materializing param=model.layers.15.input_layernorm.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 183/290 [00:00\u003C00:00, 6058.07it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 183/290 [00:00\u003C00:00, 6051.48it/s, Materializing param=model.layers.15.mlp.down_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 184/290 [00:00\u003C00:00, 6070.29it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 184/290 [00:00\u003C00:00, 6063.66it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 185/290 [00:00\u003C00:00, 6075.61it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  \rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 185/290 [00:00\u003C00:00, 6068.86it/s, Materializing param=model.layers.15.mlp.up_proj.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 186/290 [00:00\u003C00:00, 6087.33it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 186/290 [00:00\u003C00:00, 6080.26it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 187/290 [00:00\u003C00:00, 6057.06it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          \rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 187/290 [00:00\u003C00:00, 6049.87it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188/290 [00:00\u003C00:00, 6067.20it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188/290 [00:00\u003C00:00, 6060.43it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 189/290 [00:00\u003C00:00, 6061.18it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 189/290 [00:00\u003C00:00, 6054.51it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 190/290 [00:00\u003C00:00, 6072.08it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  \rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 190/290 [00:00\u003C00:00, 6065.56it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 191/290 [00:00\u003C00:00, 6084.20it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 191/290 [00:00\u003C00:00, 6076.90it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 192/290 [00:00\u003C00:00, 6045.62it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  \rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 192/290 [00:00\u003C00:00, 6038.77it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 193/290 [00:00\u003C00:00, 6056.42it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 193/290 [00:00\u003C00:00, 6050.08it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 194/290 [00:00\u003C00:00, 6040.66it/s, Materializing param=model.layers.16.input_layernorm.weight] \rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 194/290 [00:00\u003C00:00, 6033.94it/s, Materializing param=model.layers.16.input_layernorm.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 195/290 [00:00\u003C00:00, 6051.85it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  \rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 195/290 [00:00\u003C00:00, 6045.77it/s, Materializing param=model.layers.16.mlp.down_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 196/290 [00:00\u003C00:00, 6054.66it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 196/290 [00:00\u003C00:00, 6048.33it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 197/290 [00:00\u003C00:00, 6066.21it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  \rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 197/290 [00:00\u003C00:00, 6060.20it/s, Materializing param=model.layers.16.mlp.up_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 198/290 [00:00\u003C00:00, 6005.77it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 198/290 [00:00\u003C00:00, 5997.31it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 199/290 [00:00\u003C00:00, 6014.10it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          \rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 199/290 [00:00\u003C00:00, 6006.48it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 200/290 [00:00\u003C00:00, 6023.31it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 200/290 [00:00\u003C00:00, 6017.26it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 201/290 [00:00\u003C00:00, 6035.14it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 201/290 [00:00\u003C00:00, 6029.36it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 202/290 [00:00\u003C00:00, 6046.60it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  \rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 202/290 [00:00\u003C00:00, 6040.82it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 203/290 [00:00\u003C00:00, 6058.55it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 203/290 [00:00\u003C00:00, 6052.64it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 204/290 [00:00\u003C00:00, 5996.10it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  \rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 204/290 [00:00\u003C00:00, 5989.77it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 205/290 [00:00\u003C00:00, 6004.88it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 205/290 [00:00\u003C00:00, 5999.10it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 206/290 [00:00\u003C00:00, 6016.73it/s, Materializing param=model.layers.17.input_layernorm.weight] \rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 206/290 [00:00\u003C00:00, 6011.00it/s, Materializing param=model.layers.17.input_layernorm.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 207/290 [00:00\u003C00:00, 6020.99it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  \rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 207/290 [00:00\u003C00:00, 6015.23it/s, Materializing param=model.layers.17.mlp.down_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 208/290 [00:00\u003C00:00, 6032.26it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 208/290 [00:00\u003C00:00, 6026.55it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 209/290 [00:00\u003C00:00, 6044.42it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  \rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 209/290 [00:00\u003C00:00, 6038.88it/s, Materializing param=model.layers.17.mlp.up_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 210/290 [00:00\u003C00:00, 5967.67it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 210/290 [00:00\u003C00:00, 5959.27it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 211/290 [00:00\u003C00:00, 5974.07it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          \rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 211/290 [00:00\u003C00:00, 5967.82it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 212/290 [00:00\u003C00:00, 5972.67it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 212/290 [00:00\u003C00:00, 5966.21it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 213/290 [00:00\u003C00:00, 5982.23it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 213/290 [00:00\u003C00:00, 5976.83it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 214/290 [00:00\u003C00:00, 5992.78it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  \rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 214/290 [00:00\u003C00:00, 5987.31it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 215/290 [00:00\u003C00:00, 6003.67it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 215/290 [00:00\u003C00:00, 5998.32it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 216/290 [00:00\u003C00:00, 6014.86it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  \rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 216/290 [00:00\u003C00:00, 6009.43it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 217/290 [00:00\u003C00:00, 6024.62it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 217/290 [00:00\u003C00:00, 6019.40it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 218/290 [00:00\u003C00:00, 6034.85it/s, Materializing param=model.layers.18.input_layernorm.weight] \rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 218/290 [00:00\u003C00:00, 6029.32it/s, Materializing param=model.layers.18.input_layernorm.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 219/290 [00:00\u003C00:00, 6043.94it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  \rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 219/290 [00:00\u003C00:00, 6037.47it/s, Materializing param=model.layers.18.mlp.down_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 220/290 [00:00\u003C00:00, 6007.35it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 220/290 [00:00\u003C00:00, 6001.61it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 221/290 [00:00\u003C00:00, 6017.81it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  \rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 221/290 [00:00\u003C00:00, 6012.54it/s, Materializing param=model.layers.18.mlp.up_proj.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 222/290 [00:00\u003C00:00, 6028.56it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 222/290 [00:00\u003C00:00, 6022.95it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 223/290 [00:00\u003C00:00, 6038.71it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          \rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 223/290 [00:00\u003C00:00, 6033.18it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 224/290 [00:00\u003C00:00, 6048.18it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 224/290 [00:00\u003C00:00, 6042.97it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 225/290 [00:00\u003C00:00, 6042.47it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 225/290 [00:00\u003C00:00, 6036.59it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 226/290 [00:00\u003C00:00, 6040.16it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  \rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 226/290 [00:00\u003C00:00, 6034.66it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 227/290 [00:00\u003C00:00, 6028.93it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 227/290 [00:00\u003C00:00, 6023.17it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 228/290 [00:00\u003C00:00, 5936.18it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  \rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 228/290 [00:00\u003C00:00, 5930.62it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 229/290 [00:00\u003C00:00, 5945.57it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 229/290 [00:00\u003C00:00, 5940.28it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 230/290 [00:00\u003C00:00, 5955.50it/s, Materializing param=model.layers.19.input_layernorm.weight] \rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 230/290 [00:00\u003C00:00, 5950.54it/s, Materializing param=model.layers.19.input_layernorm.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 231/290 [00:00\u003C00:00, 5965.56it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  \rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 231/290 [00:00\u003C00:00, 5960.38it/s, Materializing param=model.layers.19.mlp.down_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 232/290 [00:00\u003C00:00, 5974.76it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 232/290 [00:00\u003C00:00, 5969.40it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 233/290 [00:00\u003C00:00, 5984.89it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  \rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 233/290 [00:00\u003C00:00, 5980.09it/s, Materializing param=model.layers.19.mlp.up_proj.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 234/290 [00:00\u003C00:00, 5994.97it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 234/290 [00:00\u003C00:00, 5989.63it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 235/290 [00:00\u003C00:00, 5996.57it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          \rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 235/290 [00:00\u003C00:00, 5991.35it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 236/290 [00:00\u003C00:00, 5990.09it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 236/290 [00:00\u003C00:00, 5984.69it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 237/290 [00:00\u003C00:00, 5964.29it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 237/290 [00:00\u003C00:00, 5958.78it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 238/290 [00:00\u003C00:00, 5973.11it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  \rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 238/290 [00:00\u003C00:00, 5968.08it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 239/290 [00:00\u003C00:00, 5982.64it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 239/290 [00:00\u003C00:00, 5977.50it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 240/290 [00:00\u003C00:00, 5992.40it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  \rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 240/290 [00:00\u003C00:00, 5987.55it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 241/290 [00:00\u003C00:00, 5988.46it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 241/290 [00:00\u003C00:00, 5983.32it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 242/290 [00:00\u003C00:00, 5998.17it/s, Materializing param=model.layers.20.input_layernorm.weight] \rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 242/290 [00:00\u003C00:00, 5993.56it/s, Materializing param=model.layers.20.input_layernorm.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 243/290 [00:00\u003C00:00, 5998.17it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  \rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 243/290 [00:00\u003C00:00, 5993.10it/s, Materializing param=model.layers.20.mlp.down_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 244/290 [00:00\u003C00:00, 6006.74it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 244/290 [00:00\u003C00:00, 6002.05it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 245/290 [00:00\u003C00:00, 6016.07it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  \rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 245/290 [00:00\u003C00:00, 6011.53it/s, Materializing param=model.layers.20.mlp.up_proj.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 246/290 [00:00\u003C00:00, 6026.37it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 246/290 [00:00\u003C00:00, 6021.31it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 247/290 [00:00\u003C00:00, 5984.46it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          \rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 247/290 [00:00\u003C00:00, 5978.86it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 248/290 [00:00\u003C00:00, 5991.97it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 248/290 [00:00\u003C00:00, 5986.86it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 249/290 [00:00\u003C00:00, 6000.81it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 249/290 [00:00\u003C00:00, 5996.16it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 250/290 [00:00\u003C00:00, 6010.31it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  \rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 250/290 [00:00\u003C00:00, 6005.49it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 251/290 [00:00\u003C00:00, 6019.89it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 251/290 [00:00\u003C00:00, 6014.80it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 252/290 [00:00\u003C00:00, 6011.22it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  \rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 252/290 [00:00\u003C00:00, 6005.48it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 253/290 [00:00\u003C00:00, 6019.46it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 253/290 [00:00\u003C00:00, 6014.58it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 254/290 [00:00\u003C00:00, 5949.60it/s, Materializing param=model.layers.21.input_layernorm.weight] \rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 254/290 [00:00\u003C00:00, 5944.52it/s, Materializing param=model.layers.21.input_layernorm.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 255/290 [00:00\u003C00:00, 5958.25it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  \rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 255/290 [00:00\u003C00:00, 5953.84it/s, Materializing param=model.layers.21.mlp.down_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 256/290 [00:00\u003C00:00, 5967.69it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 256/290 [00:00\u003C00:00, 5963.31it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 257/290 [00:00\u003C00:00, 5976.75it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  \rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 257/290 [00:00\u003C00:00, 5971.98it/s, Materializing param=model.layers.21.mlp.up_proj.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 258/290 [00:00\u003C00:00, 5984.34it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 258/290 [00:00\u003C00:00, 5979.48it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 259/290 [00:00\u003C00:00, 5993.35it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          \rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 259/290 [00:00\u003C00:00, 5988.86it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 260/290 [00:00\u003C00:00, 6002.25it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 260/290 [00:00\u003C00:00, 5996.90it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 261/290 [00:00\u003C00:00, 5998.86it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 261/290 [00:00\u003C00:00, 5994.06it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 262/290 [00:00\u003C00:00, 6007.62it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  \rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 262/290 [00:00\u003C00:00, 6003.02it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 263/290 [00:00\u003C00:00, 6016.57it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 263/290 [00:00\u003C00:00, 6011.95it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 264/290 [00:00\u003C00:00, 6024.82it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  \rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 264/290 [00:00\u003C00:00, 6020.43it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 265/290 [00:00\u003C00:00, 5970.88it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 265/290 [00:00\u003C00:00, 5965.65it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 266/290 [00:00\u003C00:00, 5978.47it/s, Materializing param=model.layers.22.input_layernorm.weight] \rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 266/290 [00:00\u003C00:00, 5974.12it/s, Materializing param=model.layers.22.input_layernorm.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 267/290 [00:00\u003C00:00, 5987.03it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  \rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 267/290 [00:00\u003C00:00, 5981.01it/s, Materializing param=model.layers.22.mlp.down_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 268/290 [00:00\u003C00:00, 5991.26it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 268/290 [00:00\u003C00:00, 5985.83it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 269/290 [00:00\u003C00:00, 5998.43it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  \rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 269/290 [00:00\u003C00:00, 5994.12it/s, Materializing param=model.layers.22.mlp.up_proj.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 270/290 [00:00\u003C00:00, 6007.02it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 270/290 [00:00\u003C00:00, 6002.60it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 271/290 [00:00\u003C00:00, 6015.90it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          \rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 271/290 [00:00\u003C00:00, 6011.51it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 272/290 [00:00\u003C00:00, 6006.53it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 272/290 [00:00\u003C00:00, 6001.63it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 273/290 [00:00\u003C00:00, 6013.89it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 273/290 [00:00\u003C00:00, 6009.63it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 274/290 [00:00\u003C00:00, 5964.34it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  \rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 274/290 [00:00\u003C00:00, 5959.73it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 275/290 [00:00\u003C00:00, 5962.96it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 275/290 [00:00\u003C00:00, 5958.68it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 276/290 [00:00\u003C00:00, 5971.96it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  \rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 276/290 [00:00\u003C00:00, 5967.74it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 277/290 [00:00\u003C00:00, 5979.40it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 277/290 [00:00\u003C00:00, 5973.35it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 278/290 [00:00\u003C00:00, 5984.70it/s, Materializing param=model.layers.23.input_layernorm.weight] \rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 278/290 [00:00\u003C00:00, 5980.31it/s, Materializing param=model.layers.23.input_layernorm.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 279/290 [00:00\u003C00:00, 5992.35it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  \rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 279/290 [00:00\u003C00:00, 5988.09it/s, Materializing param=model.layers.23.mlp.down_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 280/290 [00:00\u003C00:00, 5984.14it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 280/290 [00:00\u003C00:00, 5979.75it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 281/290 [00:00\u003C00:00, 5990.34it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  \rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 281/290 [00:00\u003C00:00, 5984.74it/s, Materializing param=model.layers.23.mlp.up_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 282/290 [00:00\u003C00:00, 5995.23it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 282/290 [00:00\u003C00:00, 5990.41it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 283/290 [00:00\u003C00:00, 5907.82it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          \rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 283/290 [00:00\u003C00:00, 5894.53it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 284/290 [00:00\u003C00:00, 5899.19it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 284/290 [00:00\u003C00:00, 5894.37it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 285/290 [00:00\u003C00:00, 5906.01it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 285/290 [00:00\u003C00:00, 5900.01it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 286/290 [00:00\u003C00:00, 5908.58it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  \rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 286/290 [00:00\u003C00:00, 5903.72it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 287/290 [00:00\u003C00:00, 5916.50it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 287/290 [00:00\u003C00:00, 5912.81it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 288/290 [00:00\u003C00:00, 5925.06it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  \rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 288/290 [00:00\u003C00:00, 5920.33it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 289/290 [00:00\u003C00:00, 5930.65it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 289/290 [00:00\u003C00:00, 5925.66it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 5939.08it/s, Materializing param=model.norm.weight]                      \rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 5935.23it/s, Materializing param=model.norm.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 5923.41it/s, Materializing param=model.norm.weight]\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "[GeneratorWorker:1] Ready on GPU 2!\n[GeneratorWorker:0] Ready on GPU 1!\n[SETUP] Setting up trainer...\n[Trainer:0] Loading model Qwen/Qwen2.5-0.5B-Instruct on GPU 0...\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stderr", "text": "[actor=\u003Croot\u003E.\u003C__main__.TrainerActor trainer{'procs': 0/1}\u003E] `torch_dtype` is deprecated! Use `dtype` instead!\n\rLoading weights:   0%|          | 0/290 [00:00\u003C?, ?it/s]\rLoading weights:   0%|          | 1/290 [00:00\u003C00:00, 13842.59it/s, Materializing param=model.embed_tokens.weight]\rLoading weights:   0%|          | 1/290 [00:00\u003C00:00, 7061.12it/s, Materializing param=model.embed_tokens.weight] \rLoading weights:   1%|          | 2/290 [00:00\u003C00:00, 6831.11it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|          | 2/290 [00:00\u003C00:00, 5928.34it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|          | 3/290 [00:00\u003C00:00, 6864.65it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  \rLoading weights:   1%|          | 3/290 [00:00\u003C00:00, 6272.64it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   1%|\u258f         | 4/290 [00:00\u003C00:00, 7064.09it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   1%|\u258f         | 4/290 [00:00\u003C00:00, 6594.82it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   2%|\u258f         | 5/290 [00:00\u003C00:00, 7296.98it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \rLoading weights:   2%|\u258f         | 5/290 [00:00\u003C00:00, 6905.34it/s, Materializing param=model.layers.0.mlp.up_proj.weight]\rLoading weights:   2%|\u258f         | 6/290 [00:00\u003C00:00, 7516.67it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   2%|\u258f         | 6/290 [00:00\u003C00:00, 7163.63it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   2%|\u258f         | 7/290 [00:00\u003C00:00, 7679.87it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          \rLoading weights:   2%|\u258f         | 7/290 [00:00\u003C00:00, 7384.34it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]\rLoading weights:   3%|\u258e         | 8/290 [00:00\u003C00:00, 7754.66it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|\u258e         | 8/290 [00:00\u003C00:00, 7498.20it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|\u258e         | 9/290 [00:00\u003C00:00, 7932.07it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   3%|\u258e         | 9/290 [00:00\u003C00:00, 7711.69it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   3%|\u258e         | 10/290 [00:00\u003C00:00, 8115.91it/s, Materializing param=model.layers.0.self_attn.q_proj.bias] \rLoading weights:   3%|\u258e         | 10/290 [00:00\u003C00:00, 7906.32it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]\rLoading weights:   4%|\u258d         | 11/290 [00:00\u003C00:00, 8278.73it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   4%|\u258d         | 11/290 [00:00\u003C00:00, 8078.68it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   4%|\u258d         | 12/290 [00:00\u003C00:00, 8378.83it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  \rLoading weights:   4%|\u258d         | 12/290 [00:00\u003C00:00, 8197.34it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]\rLoading weights:   4%|\u258d         | 13/290 [00:00\u003C00:00, 8495.79it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   4%|\u258d         | 13/290 [00:00\u003C00:00, 8316.95it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   5%|\u258d         | 14/290 [00:00\u003C00:00, 8577.31it/s, Materializing param=model.layers.1.input_layernorm.weight] \rLoading weights:   5%|\u258d         | 14/290 [00:00\u003C00:00, 8405.42it/s, Materializing param=model.layers.1.input_layernorm.weight]\rLoading weights:   5%|\u258c         | 15/290 [00:00\u003C00:00, 8658.76it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  \rLoading weights:   5%|\u258c         | 15/290 [00:00\u003C00:00, 8497.37it/s, Materializing param=model.layers.1.mlp.down_proj.weight]\rLoading weights:   6%|\u258c         | 16/290 [00:00\u003C00:00, 8750.67it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   6%|\u258c         | 16/290 [00:00\u003C00:00, 8594.89it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   6%|\u258c         | 17/290 [00:00\u003C00:00, 8827.93it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  \rLoading weights:   6%|\u258c         | 17/290 [00:00\u003C00:00, 8679.63it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\rLoading weights:   6%|\u258c         | 18/290 [00:00\u003C00:00, 7917.93it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:   6%|\u258c         | 18/290 [00:00\u003C00:00, 7790.47it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:   7%|\u258b         | 19/290 [00:00\u003C00:00, 7947.72it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          \rLoading weights:   7%|\u258b         | 19/290 [00:00\u003C00:00, 7835.20it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]\rLoading weights:   7%|\u258b         | 20/290 [00:00\u003C00:00, 8014.34it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:   7%|\u258b         | 20/290 [00:00\u003C00:00, 7909.30it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:   7%|\u258b         | 21/290 [00:00\u003C00:00, 8100.84it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:   7%|\u258b         | 21/290 [00:00\u003C00:00, 7999.31it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:   8%|\u258a         | 22/290 [00:00\u003C00:00, 8178.21it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  \rLoading weights:   8%|\u258a         | 22/290 [00:00\u003C00:00, 8080.80it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]\rLoading weights:   8%|\u258a         | 23/290 [00:00\u003C00:00, 8250.85it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:   8%|\u258a         | 23/290 [00:00\u003C00:00, 8152.54it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:   8%|\u258a         | 24/290 [00:00\u003C00:00, 8317.22it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  \rLoading weights:   8%|\u258a         | 24/290 [00:00\u003C00:00, 8219.42it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]\rLoading weights:   9%|\u258a         | 25/290 [00:00\u003C00:00, 8371.20it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:   9%|\u258a         | 25/290 [00:00\u003C00:00, 8252.61it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:   9%|\u2589         | 26/290 [00:00\u003C00:00, 8402.83it/s, Materializing param=model.layers.2.input_layernorm.weight] \rLoading weights:   9%|\u2589         | 26/290 [00:00\u003C00:00, 8313.15it/s, Materializing param=model.layers.2.input_layernorm.weight]\rLoading weights:   9%|\u2589         | 27/290 [00:00\u003C00:00, 8455.00it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  \rLoading weights:   9%|\u2589         | 27/290 [00:00\u003C00:00, 8370.01it/s, Materializing param=model.layers.2.mlp.down_proj.weight]\rLoading weights:  10%|\u2589         | 28/290 [00:00\u003C00:00, 7952.36it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  10%|\u2589         | 28/290 [00:00\u003C00:00, 7677.36it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  10%|\u2588         | 29/290 [00:00\u003C00:00, 7726.77it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  \rLoading weights:  10%|\u2588         | 29/290 [00:00\u003C00:00, 7636.54it/s, Materializing param=model.layers.2.mlp.up_proj.weight]\rLoading weights:  10%|\u2588         | 30/290 [00:00\u003C00:00, 7734.76it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  10%|\u2588         | 30/290 [00:00\u003C00:00, 7656.17it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  11%|\u2588         | 31/290 [00:00\u003C00:00, 7760.74it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          \rLoading weights:  11%|\u2588         | 31/290 [00:00\u003C00:00, 7662.86it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]\rLoading weights:  11%|\u2588         | 32/290 [00:00\u003C00:00, 7770.83it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  11%|\u2588         | 32/290 [00:00\u003C00:00, 7707.90it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  11%|\u2588\u258f        | 33/290 [00:00\u003C00:00, 7826.96it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  11%|\u2588\u258f        | 33/290 [00:00\u003C00:00, 7765.49it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 34/290 [00:00\u003C00:00, 7855.37it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  \rLoading weights:  12%|\u2588\u258f        | 34/290 [00:00\u003C00:00, 7796.53it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]\rLoading weights:  12%|\u2588\u258f        | 35/290 [00:00\u003C00:00, 7909.52it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 35/290 [00:00\u003C00:00, 7851.14it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 36/290 [00:00\u003C00:00, 7745.71it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  \rLoading weights:  12%|\u2588\u258f        | 36/290 [00:00\u003C00:00, 7685.79it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]\rLoading weights:  13%|\u2588\u258e        | 37/290 [00:00\u003C00:00, 7791.02it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  13%|\u2588\u258e        | 37/290 [00:00\u003C00:00, 7739.73it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  13%|\u2588\u258e        | 38/290 [00:00\u003C00:00, 7749.10it/s, Materializing param=model.layers.3.input_layernorm.weight] \rLoading weights:  13%|\u2588\u258e        | 38/290 [00:00\u003C00:00, 7690.77it/s, Materializing param=model.layers.3.input_layernorm.weight]\rLoading weights:  13%|\u2588\u258e        | 39/290 [00:00\u003C00:00, 7247.58it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \rLoading weights:  13%|\u2588\u258e        | 39/290 [00:00\u003C00:00, 7196.56it/s, Materializing param=model.layers.3.mlp.down_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 40/290 [00:00\u003C00:00, 7287.15it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 40/290 [00:00\u003C00:00, 7242.17it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 41/290 [00:00\u003C00:00, 7336.77it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  \rLoading weights:  14%|\u2588\u258d        | 41/290 [00:00\u003C00:00, 7291.66it/s, Materializing param=model.layers.3.mlp.up_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 42/290 [00:00\u003C00:00, 7386.82it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  14%|\u2588\u258d        | 42/290 [00:00\u003C00:00, 7341.87it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  15%|\u2588\u258d        | 43/290 [00:00\u003C00:00, 7429.97it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          \rLoading weights:  15%|\u2588\u258d        | 43/290 [00:00\u003C00:00, 7388.27it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]\rLoading weights:  15%|\u2588\u258c        | 44/290 [00:00\u003C00:00, 6970.18it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  15%|\u2588\u258c        | 44/290 [00:00\u003C00:00, 6927.53it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 45/290 [00:00\u003C00:00, 7011.80it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 45/290 [00:00\u003C00:00, 6974.75it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 46/290 [00:00\u003C00:00, 7058.53it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  \rLoading weights:  16%|\u2588\u258c        | 46/290 [00:00\u003C00:00, 7021.03it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]\rLoading weights:  16%|\u2588\u258c        | 47/290 [00:00\u003C00:00, 7102.33it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 47/290 [00:00\u003C00:00, 7068.46it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 48/290 [00:00\u003C00:00, 7148.37it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  \rLoading weights:  17%|\u2588\u258b        | 48/290 [00:00\u003C00:00, 7112.51it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]\rLoading weights:  17%|\u2588\u258b        | 49/290 [00:00\u003C00:00, 7194.09it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 49/290 [00:00\u003C00:00, 7161.26it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 50/290 [00:00\u003C00:00, 6923.58it/s, Materializing param=model.layers.4.input_layernorm.weight] \rLoading weights:  17%|\u2588\u258b        | 50/290 [00:00\u003C00:00, 6889.69it/s, Materializing param=model.layers.4.input_layernorm.weight]\rLoading weights:  18%|\u2588\u258a        | 51/290 [00:00\u003C00:00, 6960.25it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  \rLoading weights:  18%|\u2588\u258a        | 51/290 [00:00\u003C00:00, 6929.37it/s, Materializing param=model.layers.4.mlp.down_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 52/290 [00:00\u003C00:00, 6999.48it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 52/290 [00:00\u003C00:00, 6968.62it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 53/290 [00:00\u003C00:00, 7040.99it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  \rLoading weights:  18%|\u2588\u258a        | 53/290 [00:00\u003C00:00, 7010.13it/s, Materializing param=model.layers.4.mlp.up_proj.weight]\rLoading weights:  19%|\u2588\u258a        | 54/290 [00:00\u003C00:00, 7078.55it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  19%|\u2588\u258a        | 54/290 [00:00\u003C00:00, 7045.96it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  19%|\u2588\u2589        | 55/290 [00:00\u003C00:00, 6952.79it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          \rLoading weights:  19%|\u2588\u2589        | 55/290 [00:00\u003C00:00, 6920.26it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]\rLoading weights:  19%|\u2588\u2589        | 56/290 [00:00\u003C00:00, 6964.80it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  19%|\u2588\u2589        | 56/290 [00:00\u003C00:00, 6934.78it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  20%|\u2588\u2589        | 57/290 [00:00\u003C00:00, 7003.00it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  20%|\u2588\u2589        | 57/290 [00:00\u003C00:00, 6974.60it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  20%|\u2588\u2588        | 58/290 [00:00\u003C00:00, 6898.33it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  \rLoading weights:  20%|\u2588\u2588        | 58/290 [00:00\u003C00:00, 6869.11it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]\rLoading weights:  20%|\u2588\u2588        | 59/290 [00:00\u003C00:00, 6933.90it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  20%|\u2588\u2588        | 59/290 [00:00\u003C00:00, 6907.19it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  21%|\u2588\u2588        | 60/290 [00:00\u003C00:00, 6972.88it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  \rLoading weights:  21%|\u2588\u2588        | 60/290 [00:00\u003C00:00, 6947.09it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]\rLoading weights:  21%|\u2588\u2588        | 61/290 [00:00\u003C00:00, 7007.55it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  21%|\u2588\u2588        | 61/290 [00:00\u003C00:00, 6980.59it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  21%|\u2588\u2588\u258f       | 62/290 [00:00\u003C00:00, 6785.13it/s, Materializing param=model.layers.5.input_layernorm.weight] \rLoading weights:  21%|\u2588\u2588\u258f       | 62/290 [00:00\u003C00:00, 6759.20it/s, Materializing param=model.layers.5.input_layernorm.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 63/290 [00:00\u003C00:00, 6817.89it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  \rLoading weights:  22%|\u2588\u2588\u258f       | 63/290 [00:00\u003C00:00, 6792.83it/s, Materializing param=model.layers.5.mlp.down_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 64/290 [00:00\u003C00:00, 6850.99it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 64/290 [00:00\u003C00:00, 6826.60it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 65/290 [00:00\u003C00:00, 6884.42it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \rLoading weights:  22%|\u2588\u2588\u258f       | 65/290 [00:00\u003C00:00, 6860.68it/s, Materializing param=model.layers.5.mlp.up_proj.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 66/290 [00:00\u003C00:00, 6919.04it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 66/290 [00:00\u003C00:00, 6895.43it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 67/290 [00:00\u003C00:00, 6954.18it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          \rLoading weights:  23%|\u2588\u2588\u258e       | 67/290 [00:00\u003C00:00, 6930.34it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]\rLoading weights:  23%|\u2588\u2588\u258e       | 68/290 [00:00\u003C00:00, 6929.70it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 68/290 [00:00\u003C00:00, 6907.21it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 69/290 [00:00\u003C00:00, 6793.91it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 69/290 [00:00\u003C00:00, 6771.18it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 70/290 [00:00\u003C00:00, 6825.87it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  \rLoading weights:  24%|\u2588\u2588\u258d       | 70/290 [00:00\u003C00:00, 6805.46it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]\rLoading weights:  24%|\u2588\u2588\u258d       | 71/290 [00:00\u003C00:00, 6858.96it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 71/290 [00:00\u003C00:00, 6838.80it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  25%|\u2588\u2588\u258d       | 72/290 [00:00\u003C00:00, 6821.55it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  \rLoading weights:  25%|\u2588\u2588\u258d       | 72/290 [00:00\u003C00:00, 6800.04it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]\rLoading weights:  25%|\u2588\u2588\u258c       | 73/290 [00:00\u003C00:00, 6852.67it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  25%|\u2588\u2588\u258c       | 73/290 [00:00\u003C00:00, 6831.27it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 74/290 [00:00\u003C00:00, 6883.07it/s, Materializing param=model.layers.6.input_layernorm.weight] \rLoading weights:  26%|\u2588\u2588\u258c       | 74/290 [00:00\u003C00:00, 6861.77it/s, Materializing param=model.layers.6.input_layernorm.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 75/290 [00:00\u003C00:00, 6912.93it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  \rLoading weights:  26%|\u2588\u2588\u258c       | 75/290 [00:00\u003C00:00, 6891.27it/s, Materializing param=model.layers.6.mlp.down_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 76/290 [00:00\u003C00:00, 6875.02it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 76/290 [00:00\u003C00:00, 6852.11it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 77/290 [00:00\u003C00:00, 6900.74it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  \rLoading weights:  27%|\u2588\u2588\u258b       | 77/290 [00:00\u003C00:00, 6880.16it/s, Materializing param=model.layers.6.mlp.up_proj.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 78/290 [00:00\u003C00:00, 6762.77it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 78/290 [00:00\u003C00:00, 6740.89it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 79/290 [00:00\u003C00:00, 6787.59it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          \rLoading weights:  27%|\u2588\u2588\u258b       | 79/290 [00:00\u003C00:00, 6767.63it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]\rLoading weights:  28%|\u2588\u2588\u258a       | 80/290 [00:00\u003C00:00, 6812.25it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 80/290 [00:00\u003C00:00, 6793.50it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 81/290 [00:00\u003C00:00, 6840.19it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 81/290 [00:00\u003C00:00, 6820.96it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 82/290 [00:00\u003C00:00, 6867.40it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  \rLoading weights:  28%|\u2588\u2588\u258a       | 82/290 [00:00\u003C00:00, 6848.80it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]\rLoading weights:  29%|\u2588\u2588\u258a       | 83/290 [00:00\u003C00:00, 6891.29it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  29%|\u2588\u2588\u258a       | 83/290 [00:00\u003C00:00, 6872.92it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  29%|\u2588\u2588\u2589       | 84/290 [00:00\u003C00:00, 6919.53it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  \rLoading weights:  29%|\u2588\u2588\u2589       | 84/290 [00:00\u003C00:00, 6901.50it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]\rLoading weights:  29%|\u2588\u2588\u2589       | 85/290 [00:00\u003C00:00, 6853.17it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  29%|\u2588\u2588\u2589       | 85/290 [00:00\u003C00:00, 6834.91it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2589       | 86/290 [00:00\u003C00:00, 6463.76it/s, Materializing param=model.layers.7.input_layernorm.weight] \rLoading weights:  30%|\u2588\u2588\u2589       | 86/290 [00:00\u003C00:00, 6415.02it/s, Materializing param=model.layers.7.input_layernorm.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 87/290 [00:00\u003C00:00, 6390.62it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  \rLoading weights:  30%|\u2588\u2588\u2588       | 87/290 [00:00\u003C00:00, 6370.43it/s, Materializing param=model.layers.7.mlp.down_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 88/290 [00:00\u003C00:00, 6404.52it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 88/290 [00:00\u003C00:00, 6387.89it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 89/290 [00:00\u003C00:00, 6426.67it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  \rLoading weights:  31%|\u2588\u2588\u2588       | 89/290 [00:00\u003C00:00, 6410.01it/s, Materializing param=model.layers.7.mlp.up_proj.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 90/290 [00:00\u003C00:00, 6449.14it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 90/290 [00:00\u003C00:00, 6432.65it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  31%|\u2588\u2588\u2588\u258f      | 91/290 [00:00\u003C00:00, 6465.45it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          \rLoading weights:  31%|\u2588\u2588\u2588\u258f      | 91/290 [00:00\u003C00:00, 6449.07it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 92/290 [00:00\u003C00:00, 6489.13it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 92/290 [00:00\u003C00:00, 6473.89it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 93/290 [00:00\u003C00:00, 6513.98it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 93/290 [00:00\u003C00:00, 6498.68it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 94/290 [00:00\u003C00:00, 6539.14it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  \rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 94/290 [00:00\u003C00:00, 6524.10it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 95/290 [00:00\u003C00:00, 6564.18it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 95/290 [00:00\u003C00:00, 6550.04it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 96/290 [00:00\u003C00:00, 6587.70it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  \rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 96/290 [00:00\u003C00:00, 6573.07it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 97/290 [00:00\u003C00:00, 6611.00it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 97/290 [00:00\u003C00:00, 6596.53it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 98/290 [00:00\u003C00:00, 6635.70it/s, Materializing param=model.layers.8.input_layernorm.weight] \rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 98/290 [00:00\u003C00:00, 6620.52it/s, Materializing param=model.layers.8.input_layernorm.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 99/290 [00:00\u003C00:00, 6659.23it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  \rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 99/290 [00:00\u003C00:00, 6644.63it/s, Materializing param=model.layers.8.mlp.down_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 100/290 [00:00\u003C00:00, 6681.81it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 100/290 [00:00\u003C00:00, 6668.00it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258d      | 101/290 [00:00\u003C00:00, 6529.76it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  \rLoading weights:  35%|\u2588\u2588\u2588\u258d      | 101/290 [00:00\u003C00:00, 6515.70it/s, Materializing param=model.layers.8.mlp.up_proj.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258c      | 102/290 [00:00\u003C00:00, 6552.50it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258c      | 102/290 [00:00\u003C00:00, 6537.98it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 103/290 [00:00\u003C00:00, 6574.84it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          \rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 103/290 [00:00\u003C00:00, 6560.37it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 104/290 [00:00\u003C00:00, 6451.34it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 104/290 [00:00\u003C00:00, 6437.06it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 105/290 [00:00\u003C00:00, 6472.22it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 105/290 [00:00\u003C00:00, 6458.74it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 106/290 [00:00\u003C00:00, 6493.87it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  \rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 106/290 [00:00\u003C00:00, 6480.71it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 107/290 [00:00\u003C00:00, 6515.54it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 107/290 [00:00\u003C00:00, 6503.36it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 108/290 [00:00\u003C00:00, 6539.31it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  \rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 108/290 [00:00\u003C00:00, 6526.50it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 109/290 [00:00\u003C00:00, 6562.44it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 109/290 [00:00\u003C00:00, 6549.56it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 110/290 [00:00\u003C00:00, 6583.99it/s, Materializing param=model.layers.9.input_layernorm.weight] \rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 110/290 [00:00\u003C00:00, 6571.43it/s, Materializing param=model.layers.9.input_layernorm.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 111/290 [00:00\u003C00:00, 6606.23it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  \rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 111/290 [00:00\u003C00:00, 6593.70it/s, Materializing param=model.layers.9.mlp.down_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u258a      | 112/290 [00:00\u003C00:00, 6626.07it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u258a      | 112/290 [00:00\u003C00:00, 6613.11it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 113/290 [00:00\u003C00:00, 6645.49it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  \rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 113/290 [00:00\u003C00:00, 6628.85it/s, Materializing param=model.layers.9.mlp.up_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 114/290 [00:00\u003C00:00, 6598.18it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 114/290 [00:00\u003C00:00, 6583.29it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2589      | 115/290 [00:00\u003C00:00, 6570.38it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          \rLoading weights:  40%|\u2588\u2588\u2588\u2589      | 115/290 [00:00\u003C00:00, 6556.54it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 116/290 [00:00\u003C00:00, 6507.23it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 116/290 [00:00\u003C00:00, 6494.29it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 117/290 [00:00\u003C00:00, 6525.97it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 117/290 [00:00\u003C00:00, 6514.10it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 118/290 [00:00\u003C00:00, 6536.38it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  \rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 118/290 [00:00\u003C00:00, 6524.57it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 119/290 [00:00\u003C00:00, 6556.10it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 119/290 [00:00\u003C00:00, 6544.58it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588\u258f     | 120/290 [00:00\u003C00:00, 6576.81it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  \rLoading weights:  41%|\u2588\u2588\u2588\u2588\u258f     | 120/290 [00:00\u003C00:00, 6564.88it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 121/290 [00:00\u003C00:00, 6596.27it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 121/290 [00:00\u003C00:00, 6584.72it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 122/290 [00:00\u003C00:00, 6568.32it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 122/290 [00:00\u003C00:00, 6555.36it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 123/290 [00:00\u003C00:00, 6585.56it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  \rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 123/290 [00:00\u003C00:00, 6574.14it/s, Materializing param=model.layers.10.mlp.down_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 124/290 [00:00\u003C00:00, 6604.20it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 124/290 [00:00\u003C00:00, 6591.14it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 125/290 [00:00\u003C00:00, 6578.85it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  \rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 125/290 [00:00\u003C00:00, 6561.39it/s, Materializing param=model.layers.10.mlp.up_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 126/290 [00:00\u003C00:00, 6591.45it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 126/290 [00:00\u003C00:00, 6579.30it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 127/290 [00:00\u003C00:00, 6502.96it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          \rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 127/290 [00:00\u003C00:00, 6490.28it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 128/290 [00:00\u003C00:00, 6518.59it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 128/290 [00:00\u003C00:00, 6507.61it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 129/290 [00:00\u003C00:00, 6536.34it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 129/290 [00:00\u003C00:00, 6525.30it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258d     | 130/290 [00:00\u003C00:00, 6554.47it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  \rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258d     | 130/290 [00:00\u003C00:00, 6543.69it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258c     | 131/290 [00:00\u003C00:00, 6572.73it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258c     | 131/290 [00:00\u003C00:00, 6562.21it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 132/290 [00:00\u003C00:00, 6428.95it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  \rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 132/290 [00:00\u003C00:00, 6417.40it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 133/290 [00:00\u003C00:00, 6443.61it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 133/290 [00:00\u003C00:00, 6432.68it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 134/290 [00:00\u003C00:00, 6460.94it/s, Materializing param=model.layers.11.input_layernorm.weight] \rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 134/290 [00:00\u003C00:00, 6450.78it/s, Materializing param=model.layers.11.input_layernorm.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 135/290 [00:00\u003C00:00, 6478.39it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  \rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 135/290 [00:00\u003C00:00, 6468.55it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 136/290 [00:00\u003C00:00, 6496.50it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 136/290 [00:00\u003C00:00, 6486.46it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 137/290 [00:00\u003C00:00, 6511.57it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 137/290 [00:00\u003C00:00, 6501.47it/s, Materializing param=model.layers.11.mlp.up_proj.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 138/290 [00:00\u003C00:00, 6529.57it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 138/290 [00:00\u003C00:00, 6518.98it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 139/290 [00:00\u003C00:00, 6546.02it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          \rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 139/290 [00:00\u003C00:00, 6536.04it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 140/290 [00:00\u003C00:00, 6562.68it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 140/290 [00:00\u003C00:00, 6552.72it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u258a     | 141/290 [00:00\u003C00:00, 6470.99it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u258a     | 141/290 [00:00\u003C00:00, 6460.67it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 142/290 [00:00\u003C00:00, 6487.00it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  \rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 142/290 [00:00\u003C00:00, 6477.27it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 143/290 [00:00\u003C00:00, 6504.28it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 143/290 [00:00\u003C00:00, 6490.69it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2589     | 144/290 [00:00\u003C00:00, 6517.39it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  \rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2589     | 144/290 [00:00\u003C00:00, 6507.84it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 145/290 [00:00\u003C00:00, 6533.82it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 145/290 [00:00\u003C00:00, 6524.42it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 146/290 [00:00\u003C00:00, 6550.17it/s, Materializing param=model.layers.12.input_layernorm.weight] \rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 146/290 [00:00\u003C00:00, 6540.65it/s, Materializing param=model.layers.12.input_layernorm.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 147/290 [00:00\u003C00:00, 6564.62it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  \rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 147/290 [00:00\u003C00:00, 6555.55it/s, Materializing param=model.layers.12.mlp.down_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 148/290 [00:00\u003C00:00, 6581.46it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 148/290 [00:00\u003C00:00, 6572.26it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 149/290 [00:00\u003C00:00, 6533.87it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  \rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 149/290 [00:00\u003C00:00, 6524.04it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 150/290 [00:00\u003C00:00, 6480.69it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 150/290 [00:00\u003C00:00, 6470.49it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 151/290 [00:00\u003C00:00, 6404.04it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          \rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 151/290 [00:00\u003C00:00, 6394.47it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 152/290 [00:00\u003C00:00, 6418.28it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 152/290 [00:00\u003C00:00, 6409.44it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 153/290 [00:00\u003C00:00, 6433.76it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 153/290 [00:00\u003C00:00, 6424.54it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 154/290 [00:00\u003C00:00, 6449.30it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  \rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 154/290 [00:00\u003C00:00, 6440.36it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 155/290 [00:00\u003C00:00, 6465.22it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 155/290 [00:00\u003C00:00, 6451.75it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 156/290 [00:00\u003C00:00, 6476.21it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  \rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 156/290 [00:00\u003C00:00, 6467.51it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 157/290 [00:00\u003C00:00, 6492.15it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 157/290 [00:00\u003C00:00, 6483.65it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 158/290 [00:00\u003C00:00, 6463.22it/s, Materializing param=model.layers.13.input_layernorm.weight] \rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 158/290 [00:00\u003C00:00, 6454.60it/s, Materializing param=model.layers.13.input_layernorm.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 159/290 [00:00\u003C00:00, 6477.47it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  \rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 159/290 [00:00\u003C00:00, 6468.74it/s, Materializing param=model.layers.13.mlp.down_proj.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 160/290 [00:00\u003C00:00, 6491.73it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 160/290 [00:00\u003C00:00, 6483.20it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 161/290 [00:00\u003C00:00, 6506.74it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  \rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 161/290 [00:00\u003C00:00, 6498.29it/s, Materializing param=model.layers.13.mlp.up_proj.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 162/290 [00:00\u003C00:00, 6473.12it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 162/290 [00:00\u003C00:00, 6463.33it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 163/290 [00:00\u003C00:00, 6485.22it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          \rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 163/290 [00:00\u003C00:00, 6476.43it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 164/290 [00:00\u003C00:00, 6476.10it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 164/290 [00:00\u003C00:00, 6466.73it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 165/290 [00:00\u003C00:00, 6376.67it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 165/290 [00:00\u003C00:00, 6368.28it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 166/290 [00:00\u003C00:00, 6390.18it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  \rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 166/290 [00:00\u003C00:00, 6379.87it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 167/290 [00:00\u003C00:00, 6401.94it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 167/290 [00:00\u003C00:00, 6393.76it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 168/290 [00:00\u003C00:00, 6416.05it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  \rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 168/290 [00:00\u003C00:00, 6408.00it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 169/290 [00:00\u003C00:00, 6429.83it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 169/290 [00:00\u003C00:00, 6422.14it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 170/290 [00:00\u003C00:00, 6444.44it/s, Materializing param=model.layers.14.input_layernorm.weight] \rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 170/290 [00:00\u003C00:00, 6436.87it/s, Materializing param=model.layers.14.input_layernorm.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 171/290 [00:00\u003C00:00, 6401.46it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  \rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 171/290 [00:00\u003C00:00, 6393.02it/s, Materializing param=model.layers.14.mlp.down_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 172/290 [00:00\u003C00:00, 6414.79it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 172/290 [00:00\u003C00:00, 6403.92it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 173/290 [00:00\u003C00:00, 6389.03it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  \rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 173/290 [00:00\u003C00:00, 6380.66it/s, Materializing param=model.layers.14.mlp.up_proj.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 174/290 [00:00\u003C00:00, 6401.72it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 174/290 [00:00\u003C00:00, 6393.70it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 175/290 [00:00\u003C00:00, 6414.65it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          \rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 175/290 [00:00\u003C00:00, 6406.65it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 176/290 [00:00\u003C00:00, 6428.28it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 176/290 [00:00\u003C00:00, 6420.62it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 177/290 [00:00\u003C00:00, 6441.97it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 177/290 [00:00\u003C00:00, 6434.26it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 178/290 [00:00\u003C00:00, 6430.16it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  \rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 178/290 [00:00\u003C00:00, 6422.14it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 179/290 [00:00\u003C00:00, 6442.59it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 179/290 [00:00\u003C00:00, 6434.86it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 180/290 [00:00\u003C00:00, 6455.59it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  \rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 180/290 [00:00\u003C00:00, 6447.32it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 181/290 [00:00\u003C00:00, 6246.92it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 181/290 [00:00\u003C00:00, 6224.74it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 182/290 [00:00\u003C00:00, 6237.34it/s, Materializing param=model.layers.15.input_layernorm.weight] \rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 182/290 [00:00\u003C00:00, 6227.37it/s, Materializing param=model.layers.15.input_layernorm.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 183/290 [00:00\u003C00:00, 6244.27it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 183/290 [00:00\u003C00:00, 6236.45it/s, Materializing param=model.layers.15.mlp.down_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 184/290 [00:00\u003C00:00, 6254.78it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 184/290 [00:00\u003C00:00, 6247.33it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 185/290 [00:00\u003C00:00, 6266.42it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  \rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 185/290 [00:00\u003C00:00, 6258.84it/s, Materializing param=model.layers.15.mlp.up_proj.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 186/290 [00:00\u003C00:00, 6277.64it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 186/290 [00:00\u003C00:00, 6269.51it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 187/290 [00:00\u003C00:00, 6288.72it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          \rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 187/290 [00:00\u003C00:00, 6281.51it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188/290 [00:00\u003C00:00, 6300.87it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188/290 [00:00\u003C00:00, 6293.93it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 189/290 [00:00\u003C00:00, 6313.55it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 189/290 [00:00\u003C00:00, 6306.62it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 190/290 [00:00\u003C00:00, 6324.04it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  \rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 190/290 [00:00\u003C00:00, 6316.62it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 191/290 [00:00\u003C00:00, 6332.70it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 191/290 [00:00\u003C00:00, 6326.00it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 192/290 [00:00\u003C00:00, 6345.29it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  \rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 192/290 [00:00\u003C00:00, 6338.75it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 193/290 [00:00\u003C00:00, 6357.90it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 193/290 [00:00\u003C00:00, 6350.97it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 194/290 [00:00\u003C00:00, 6369.98it/s, Materializing param=model.layers.16.input_layernorm.weight] \rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 194/290 [00:00\u003C00:00, 6363.21it/s, Materializing param=model.layers.16.input_layernorm.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 195/290 [00:00\u003C00:00, 6382.08it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  \rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 195/290 [00:00\u003C00:00, 6374.32it/s, Materializing param=model.layers.16.mlp.down_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 196/290 [00:00\u003C00:00, 6390.58it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 196/290 [00:00\u003C00:00, 6383.68it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 197/290 [00:00\u003C00:00, 6402.48it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  \rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 197/290 [00:00\u003C00:00, 6395.54it/s, Materializing param=model.layers.16.mlp.up_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 198/290 [00:00\u003C00:00, 6396.91it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 198/290 [00:00\u003C00:00, 6389.28it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 199/290 [00:00\u003C00:00, 6405.48it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          \rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 199/290 [00:00\u003C00:00, 6398.71it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 200/290 [00:00\u003C00:00, 6346.64it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 200/290 [00:00\u003C00:00, 6339.21it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 201/290 [00:00\u003C00:00, 6356.25it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 201/290 [00:00\u003C00:00, 6349.60it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 202/290 [00:00\u003C00:00, 6367.62it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  \rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 202/290 [00:00\u003C00:00, 6360.87it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 203/290 [00:00\u003C00:00, 6378.72it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 203/290 [00:00\u003C00:00, 6371.94it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 204/290 [00:00\u003C00:00, 6389.70it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  \rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 204/290 [00:00\u003C00:00, 6383.17it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 205/290 [00:00\u003C00:00, 6399.66it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 205/290 [00:00\u003C00:00, 6392.81it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 206/290 [00:00\u003C00:00, 6376.53it/s, Materializing param=model.layers.17.input_layernorm.weight] \rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 206/290 [00:00\u003C00:00, 6369.72it/s, Materializing param=model.layers.17.input_layernorm.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 207/290 [00:00\u003C00:00, 6387.12it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  \rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 207/290 [00:00\u003C00:00, 6380.83it/s, Materializing param=model.layers.17.mlp.down_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 208/290 [00:00\u003C00:00, 6378.38it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 208/290 [00:00\u003C00:00, 6371.85it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 209/290 [00:00\u003C00:00, 6389.28it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  \rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 209/290 [00:00\u003C00:00, 6383.23it/s, Materializing param=model.layers.17.mlp.up_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 210/290 [00:00\u003C00:00, 6360.10it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 210/290 [00:00\u003C00:00, 6352.85it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 211/290 [00:00\u003C00:00, 6369.78it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          \rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 211/290 [00:00\u003C00:00, 6363.00it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 212/290 [00:00\u003C00:00, 6315.60it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 212/290 [00:00\u003C00:00, 6308.66it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 213/290 [00:00\u003C00:00, 6325.13it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 213/290 [00:00\u003C00:00, 6318.73it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 214/290 [00:00\u003C00:00, 6334.96it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  \rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 214/290 [00:00\u003C00:00, 6328.66it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 215/290 [00:00\u003C00:00, 6345.26it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 215/290 [00:00\u003C00:00, 6339.01it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 216/290 [00:00\u003C00:00, 6356.48it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  \rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 216/290 [00:00\u003C00:00, 6348.99it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 217/290 [00:00\u003C00:00, 6331.31it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 217/290 [00:00\u003C00:00, 6324.76it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 218/290 [00:00\u003C00:00, 6341.21it/s, Materializing param=model.layers.18.input_layernorm.weight] \rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 218/290 [00:00\u003C00:00, 6335.32it/s, Materializing param=model.layers.18.input_layernorm.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 219/290 [00:00\u003C00:00, 6352.28it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  \rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 219/290 [00:00\u003C00:00, 6346.05it/s, Materializing param=model.layers.18.mlp.down_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 220/290 [00:00\u003C00:00, 6362.89it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 220/290 [00:00\u003C00:00, 6356.54it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 221/290 [00:00\u003C00:00, 6373.23it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  \rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 221/290 [00:00\u003C00:00, 6367.23it/s, Materializing param=model.layers.18.mlp.up_proj.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 222/290 [00:00\u003C00:00, 6383.50it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 222/290 [00:00\u003C00:00, 6377.20it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 223/290 [00:00\u003C00:00, 6375.80it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          \rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 223/290 [00:00\u003C00:00, 6368.29it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 224/290 [00:00\u003C00:00, 6348.09it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 224/290 [00:00\u003C00:00, 6341.24it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 225/290 [00:00\u003C00:00, 6356.93it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 225/290 [00:00\u003C00:00, 6350.73it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 226/290 [00:00\u003C00:00, 6366.92it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  \rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 226/290 [00:00\u003C00:00, 6358.59it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 227/290 [00:00\u003C00:00, 6373.04it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 227/290 [00:00\u003C00:00, 6365.37it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 228/290 [00:00\u003C00:00, 6381.30it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  \rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 228/290 [00:00\u003C00:00, 6375.43it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 229/290 [00:00\u003C00:00, 6326.29it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 229/290 [00:00\u003C00:00, 6319.22it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 230/290 [00:00\u003C00:00, 6334.60it/s, Materializing param=model.layers.19.input_layernorm.weight] \rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 230/290 [00:00\u003C00:00, 6328.66it/s, Materializing param=model.layers.19.input_layernorm.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 231/290 [00:00\u003C00:00, 6344.27it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  \rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 231/290 [00:00\u003C00:00, 6338.63it/s, Materializing param=model.layers.19.mlp.down_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 232/290 [00:00\u003C00:00, 6354.59it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 232/290 [00:00\u003C00:00, 6348.79it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 233/290 [00:00\u003C00:00, 6364.94it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  \rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 233/290 [00:00\u003C00:00, 6359.22it/s, Materializing param=model.layers.19.mlp.up_proj.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 234/290 [00:00\u003C00:00, 6375.36it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 234/290 [00:00\u003C00:00, 6369.36it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 235/290 [00:00\u003C00:00, 6385.14it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          \rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 235/290 [00:00\u003C00:00, 6379.48it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 236/290 [00:00\u003C00:00, 6395.28it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 236/290 [00:00\u003C00:00, 6389.34it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 237/290 [00:00\u003C00:00, 6389.15it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 237/290 [00:00\u003C00:00, 6382.96it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 238/290 [00:00\u003C00:00, 6332.07it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  \rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 238/290 [00:00\u003C00:00, 6326.01it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 239/290 [00:00\u003C00:00, 6341.10it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 239/290 [00:00\u003C00:00, 6335.41it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 240/290 [00:00\u003C00:00, 6351.20it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  \rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 240/290 [00:00\u003C00:00, 6345.43it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 241/290 [00:00\u003C00:00, 6360.76it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 241/290 [00:00\u003C00:00, 6355.17it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 242/290 [00:00\u003C00:00, 6370.56it/s, Materializing param=model.layers.20.input_layernorm.weight] \rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 242/290 [00:00\u003C00:00, 6363.93it/s, Materializing param=model.layers.20.input_layernorm.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 243/290 [00:00\u003C00:00, 6320.21it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  \rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 243/290 [00:00\u003C00:00, 6314.10it/s, Materializing param=model.layers.20.mlp.down_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 244/290 [00:00\u003C00:00, 6328.79it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 244/290 [00:00\u003C00:00, 6323.40it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 245/290 [00:00\u003C00:00, 6338.62it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  \rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 245/290 [00:00\u003C00:00, 6333.11it/s, Materializing param=model.layers.20.mlp.up_proj.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 246/290 [00:00\u003C00:00, 6348.16it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 246/290 [00:00\u003C00:00, 6342.39it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 247/290 [00:00\u003C00:00, 6356.99it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          \rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 247/290 [00:00\u003C00:00, 6351.58it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 248/290 [00:00\u003C00:00, 6366.13it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 248/290 [00:00\u003C00:00, 6360.68it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 249/290 [00:00\u003C00:00, 6374.98it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 249/290 [00:00\u003C00:00, 6369.54it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 250/290 [00:00\u003C00:00, 6320.49it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  \rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 250/290 [00:00\u003C00:00, 6314.40it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 251/290 [00:00\u003C00:00, 6328.49it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 251/290 [00:00\u003C00:00, 6323.21it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 252/290 [00:00\u003C00:00, 6336.83it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  \rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 252/290 [00:00\u003C00:00, 6331.52it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 253/290 [00:00\u003C00:00, 6346.42it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 253/290 [00:00\u003C00:00, 6341.26it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 254/290 [00:00\u003C00:00, 6356.07it/s, Materializing param=model.layers.21.input_layernorm.weight] \rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 254/290 [00:00\u003C00:00, 6350.91it/s, Materializing param=model.layers.21.input_layernorm.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 255/290 [00:00\u003C00:00, 6355.23it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  \rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 255/290 [00:00\u003C00:00, 6349.87it/s, Materializing param=model.layers.21.mlp.down_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 256/290 [00:00\u003C00:00, 6363.82it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 256/290 [00:00\u003C00:00, 6358.32it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 257/290 [00:00\u003C00:00, 6372.74it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  \rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 257/290 [00:00\u003C00:00, 6367.24it/s, Materializing param=model.layers.21.mlp.up_proj.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 258/290 [00:00\u003C00:00, 6381.31it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 258/290 [00:00\u003C00:00, 6376.01it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 259/290 [00:00\u003C00:00, 6390.18it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          \rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 259/290 [00:00\u003C00:00, 6385.11it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 260/290 [00:00\u003C00:00, 6374.47it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 260/290 [00:00\u003C00:00, 6369.04it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 261/290 [00:00\u003C00:00, 6376.29it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 261/290 [00:00\u003C00:00, 6370.84it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 262/290 [00:00\u003C00:00, 6368.12it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  \rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 262/290 [00:00\u003C00:00, 6362.81it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 263/290 [00:00\u003C00:00, 6376.50it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 263/290 [00:00\u003C00:00, 6371.30it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 264/290 [00:00\u003C00:00, 6385.31it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  \rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 264/290 [00:00\u003C00:00, 6380.16it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 265/290 [00:00\u003C00:00, 6323.73it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 265/290 [00:00\u003C00:00, 6318.23it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 266/290 [00:00\u003C00:00, 6308.08it/s, Materializing param=model.layers.22.input_layernorm.weight] \rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 266/290 [00:00\u003C00:00, 6302.73it/s, Materializing param=model.layers.22.input_layernorm.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 267/290 [00:00\u003C00:00, 6316.01it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  \rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 267/290 [00:00\u003C00:00, 6310.99it/s, Materializing param=model.layers.22.mlp.down_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 268/290 [00:00\u003C00:00, 6324.58it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 268/290 [00:00\u003C00:00, 6319.67it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 269/290 [00:00\u003C00:00, 6333.32it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  \rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 269/290 [00:00\u003C00:00, 6328.52it/s, Materializing param=model.layers.22.mlp.up_proj.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 270/290 [00:00\u003C00:00, 6342.37it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 270/290 [00:00\u003C00:00, 6337.15it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 271/290 [00:00\u003C00:00, 6324.10it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          \rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 271/290 [00:00\u003C00:00, 6318.94it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 272/290 [00:00\u003C00:00, 6332.04it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 272/290 [00:00\u003C00:00, 6327.23it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 273/290 [00:00\u003C00:00, 6340.30it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 273/290 [00:00\u003C00:00, 6335.32it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 274/290 [00:00\u003C00:00, 6348.65it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  \rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 274/290 [00:00\u003C00:00, 6343.85it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 275/290 [00:00\u003C00:00, 6349.58it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 275/290 [00:00\u003C00:00, 6344.80it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 276/290 [00:00\u003C00:00, 6358.39it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  \rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 276/290 [00:00\u003C00:00, 6353.61it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 277/290 [00:00\u003C00:00, 6367.16it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 277/290 [00:00\u003C00:00, 6362.21it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 278/290 [00:00\u003C00:00, 6373.38it/s, Materializing param=model.layers.23.input_layernorm.weight] \rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 278/290 [00:00\u003C00:00, 6368.54it/s, Materializing param=model.layers.23.input_layernorm.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 279/290 [00:00\u003C00:00, 6381.90it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  \rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 279/290 [00:00\u003C00:00, 6376.68it/s, Materializing param=model.layers.23.mlp.down_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 280/290 [00:00\u003C00:00, 6389.06it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 280/290 [00:00\u003C00:00, 6384.13it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 281/290 [00:00\u003C00:00, 6396.91it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  \rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 281/290 [00:00\u003C00:00, 6392.02it/s, Materializing param=model.layers.23.mlp.up_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 282/290 [00:00\u003C00:00, 6387.37it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 282/290 [00:00\u003C00:00, 6381.82it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 283/290 [00:00\u003C00:00, 6359.40it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          \rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 283/290 [00:00\u003C00:00, 6354.29it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 284/290 [00:00\u003C00:00, 6367.88it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 284/290 [00:00\u003C00:00, 6363.32it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 285/290 [00:00\u003C00:00, 6377.55it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 285/290 [00:00\u003C00:00, 6373.10it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 286/290 [00:00\u003C00:00, 6387.36it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  \rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 286/290 [00:00\u003C00:00, 6382.53it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 287/290 [00:00\u003C00:00, 6396.07it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 287/290 [00:00\u003C00:00, 6391.24it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 288/290 [00:00\u003C00:00, 6405.39it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  \rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 288/290 [00:00\u003C00:00, 6400.90it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 289/290 [00:00\u003C00:00, 6413.58it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 289/290 [00:00\u003C00:00, 6409.10it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 6423.30it/s, Materializing param=model.norm.weight]                      \rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 6418.86it/s, Materializing param=model.norm.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 6405.88it/s, Materializing param=model.norm.weight]\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "[Trainer:0] RDMA handles registered for 3 circular buffer slots\n[Trainer:0] Ready! 494,032,768 params, RDMA=True, buffer_slots=3\n[REGISTRY] ServiceRegistry spawned\n[REGISTRY] Registered 'zorplex'\n[SETUP] All actors ready! 2 generators, 2 zorplex workers\n", "type": "stream"}], "id": "Pvdt", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "476adcec390e4d09fb048f4c447bb22f", "console": [], "id": "ZBYS", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"before-training-zorplex-baseline\"\u003EBefore Training: Zorplex Baseline\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003ELet's evaluate the model \u003Cem\u003Ebefore\u003C/em\u003E any training to establish a baseline.\nWe run 10 compositional Zorplex tasks and record accuracy, average turns,\nand tool usage. This gives us a concrete \"before\" snapshot to compare against.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "33240f80c23ace120bb89a36cc7a1f90", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "Evaluating pre-training baseline...\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stderr", "text": "[actor=\u003Croot\u003E.\u003C__main__.TrainerActor trainer{'procs': 0/1}\u003E] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n", "type": "stream"}], "id": "aLJB", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"pre-training-results\"\u003EPre-Training Results\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EMetrics refresher\u003C/strong\u003E (from \u003Ca href=\"./05_rl_intro.html\"\u003ENB05\u003C/a\u003E): \u003Cem\u003EAccuracy\u003C/em\u003E is how often the model gets the\ncorrect answer. \u003Cem\u003EFormat compliance\u003C/em\u003E tracks whether it emits the \u003Ccode\u003E[ANSWER]\u003C/code\u003E tag\nwe trained it to use. \u003Cem\u003EAvg turns/tool calls\u003C/em\u003E measure how many interaction\nsteps the model takes \u2014 lower is more efficient.\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EMetric\u003C/th\u003E\n\u003Cth\u003EValue\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EAccuracy\u003C/td\u003E\n\u003Ctd\u003E0% (0/10)\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EFormat compliance\u003C/td\u003E\n\u003Ctd\u003E20%\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EAvg turns\u003C/td\u003E\n\u003Ctd\u003E4.6\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EAvg tool calls\u003C/td\u003E\n\u003Ctd\u003E4.4\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EFailure mode breakdown:\u003C/strong\u003E\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EMode\u003C/th\u003E\n\u003Cth\u003ECount\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003ESuccess\u003C/td\u003E\n\u003Ctd\u003E0\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EWrong format\u003C/td\u003E\n\u003Ctd\u003E8\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003ETool spam\u003C/td\u003E\n\u003Ctd\u003E0\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EWrong answer\u003C/td\u003E\n\u003Ctd\u003E2\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003EThis is our starting point. Let's see if training improves it at all...\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "a0dcf4a953ef2722683c73f791068743", "console": [], "id": "nHfw", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"example-trajectory\"\u003EExample Trajectory\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EHere's what a single generation looks like -- this is the data unit flowing\nthrough the pipeline:\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EField\u003C/th\u003E\n\u003Cth\u003EValue\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EQuestion\u003C/td\u003E\n\u003Ctd\u003EWhat is zorplex('umbrella') + zorplex('dog')?...\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003ECorrect answer\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003E125\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EResult\u003C/td\u003E\n\u003Ctd\u003E\u003Cstrong\u003EWrong\u003C/strong\u003E (reward=0.20)\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EFailure mode\u003C/td\u003E\n\u003Ctd\u003E\u003Ccode\u003Ewrong_answer\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EFormat (\u003Ccode\u003E[ANSWER]\u003C/code\u003E tag)\u003C/td\u003E\n\u003Ctd\u003EYes\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003ETurns\u003C/td\u003E\n\u003Ctd\u003E3\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003ETool calls\u003C/td\u003E\n\u003Ctd\u003E2\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EModel response\u003C/strong\u003E (first 500 chars):\n\u003Cdiv class=\"language-tsql codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"n\"\u003ELOOKUP\u003C/span\u003E\u003Cspan class=\"o\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Eumbrella\u003C/span\u003E\u003Cspan class=\"o\"\u003E]\u003C/span\u003E\n\n\u003Cspan class=\"o\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003EResult: 30\u003C/span\u003E\u003Cspan class=\"o\"\u003E]\u003C/span\u003E\n\u003Cspan class=\"n\"\u003ELOOKUP\u003C/span\u003E\u003Cspan class=\"o\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Edog\u003C/span\u003E\u003Cspan class=\"o\"\u003E]\u003C/span\u003E\n\n\n\u003Cspan class=\"o\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003EResult: 95\u003C/span\u003E\u003Cspan class=\"o\"\u003E]\u003C/span\u003E\n\u003Cspan class=\"o\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003EANSWER\u003C/span\u003E\u003Cspan class=\"o\"\u003E]\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mi\"\u003E1\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EEach generator produces trajectories like this, which flow into the replay buffer\nfor the trainer to sample from.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "6e20128924fe44882de16035d62c36fe", "console": [], "id": "xXTn", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "dcdcd02377d0ebb18af0cef7cda831d2", "console": [], "id": "AjVT", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "b77f525d8cb685ba488989c212c381ce", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "\n============================================================\nSYNC MODE: Broadcast Generate -\u003E Train\n============================================================\n[SYNC  1] 0/2 correct 1/2 formatted gen=1002ms train=2369ms\n[SYNC  2] 0/2 correct 1/2 formatted gen=189ms train=713ms\n[SYNC  3] 0/2 correct 2/2 formatted gen=415ms train=720ms\n[SYNC  4] 0/2 correct 2/2 formatted gen=116ms train=761ms\n[SYNC  5] 0/2 correct 2/2 formatted gen=105ms train=722ms\n[SYNC  6] 0/2 correct 2/2 formatted gen=101ms train=701ms\n[SYNC  7] 0/2 correct 2/2 formatted gen=106ms train=696ms\n[SYNC  8] 0/2 correct 2/2 formatted gen=148ms train=693ms\n[SYNC  9] 0/2 correct 2/2 formatted gen=305ms train=712ms\n[SYNC 10] 0/2 correct 2/2 formatted gen=183ms train=714ms\n[SYNC 11] 0/2 correct 2/2 formatted gen=180ms train=708ms\n[SYNC 12] 0/2 correct 2/2 formatted gen=288ms train=766ms\n[SYNC 13] 0/2 correct 2/2 formatted gen=258ms train=723ms\n[SYNC 14] 0/2 correct 2/2 formatted gen=149ms train=693ms\n[SYNC 15] 0/2 correct 2/2 formatted gen=555ms train=725ms\n[SYNC 16] 0/2 correct 2/2 formatted gen=2256ms train=771ms\n[SYNC 17] 0/2 correct 2/2 formatted gen=105ms train=690ms\n[SYNC 18] 0/2 correct 2/2 formatted gen=137ms train=698ms\n[SYNC 19] 0/2 correct 2/2 formatted gen=101ms train=696ms\n[SYNC 20] 0/2 correct 2/2 formatted gen=135ms train=709ms\n\nSync complete: 22.82s, 40 generations, 1.75 gens/s\nEvaluating post-sync performance...\nPost-sync accuracy: 0%\n", "type": "stream"}], "id": "pHFh", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "f3fc1e2cd04479f2764fd72bfa495c9c", "console": [], "id": "NCOB", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"re-initializing-for-async\"\u003ERe-initializing for Async\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003ETo compare fairly, we tear down all actors and re-spawn from scratch so async\ntraining starts from the same untrained baseline. \u003Ccode\u003EProcMesh.stop()\u003C/code\u003E releases\nthe processes and frees GPU memory before we spawn fresh ones.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "bda1a812a94ab293703d0a2366de6aed", "console": [{"mimetype": "text/plain", "name": "stderr", "text": "/home/allencwang/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n/home/allencwang/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n/home/allencwang/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "[TEARDOWN] All actors stopped.\nRe-spawning actors for async run...\n[ReplayBuffer] Initialized with max_size=500\n[SERVICE:zorplex] 2 replicas x 1 procs each\n[SETUP] Setting up generator workers...\n[GeneratorWorker:1] Spawned, waiting for setup()...\n[GeneratorWorker:0] Spawned, waiting for setup()...\n[GeneratorWorker:1] Loading model Qwen/Qwen2.5-0.5B-Instruct on GPU 2...\n[Trainer:0] Spawned, waiting for setup()...\n[GeneratorWorker:0] Loading model Qwen/Qwen2.5-0.5B-Instruct on GPU 1...\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stderr", "text": "[actor=\u003Croot\u003E.\u003C__main__.GeneratorWorker generators{'procs': 1/2}\u003E] `torch_dtype` is deprecated! Use `dtype` instead!\n[actor=\u003Croot\u003E.\u003C__main__.GeneratorWorker generators{'procs': 0/2}\u003E] `torch_dtype` is deprecated! Use `dtype` instead!\n\rLoading weights:   0%|          | 0/290 [00:00\u003C?, ?it/s]\rLoading weights:   0%|          | 1/290 [00:00\u003C00:00, 14463.12it/s, Materializing param=model.embed_tokens.weight]\rLoading weights:   0%|          | 1/290 [00:00\u003C00:00, 7037.42it/s, Materializing param=model.embed_tokens.weight] \rLoading weights:   1%|          | 2/290 [00:00\u003C00:00, 6620.84it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|          | 2/290 [00:00\u003C00:00, 5714.31it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|          | 3/290 [00:00\u003C00:00, 6506.16it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  \rLoading weights:   1%|          | 3/290 [00:00\u003C00:00, 5955.00it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   1%|\u258f         | 4/290 [00:00\u003C00:00, 6668.21it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   1%|\u258f         | 4/290 [00:00\u003C00:00, 6168.09it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   2%|\u258f         | 5/290 [00:00\u003C00:00, 6831.11it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \rLoading weights:   2%|\u258f         | 5/290 [00:00\u003C00:00, 6504.81it/s, Materializing param=model.layers.0.mlp.up_proj.weight]\rLoading weights:   2%|\u258f         | 6/290 [00:00\u003C00:00, 7092.96it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   2%|\u258f         | 6/290 [00:00\u003C00:00, 6765.01it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   2%|\u258f         | 7/290 [00:00\u003C00:00, 7238.69it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          \rLoading weights:   2%|\u258f         | 7/290 [00:00\u003C00:00, 6973.90it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]\rLoading weights:   3%|\u258e         | 8/290 [00:00\u003C00:00, 7339.11it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|\u258e         | 8/290 [00:00\u003C00:00, 7108.99it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|\u258e         | 9/290 [00:00\u003C00:00, 7500.25it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   3%|\u258e         | 9/290 [00:00\u003C00:00, 7297.26it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   3%|\u258e         | 10/290 [00:00\u003C00:00, 7674.85it/s, Materializing param=model.layers.0.self_attn.q_proj.bias] \rLoading weights:   3%|\u258e         | 10/290 [00:00\u003C00:00, 7475.15it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]\rLoading weights:   4%|\u258d         | 11/290 [00:00\u003C00:00, 6260.16it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   4%|\u258d         | 11/290 [00:00\u003C00:00, 6119.82it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   4%|\u258d         | 12/290 [00:00\u003C00:00, 6392.94it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  \rLoading weights:   4%|\u258d         | 12/290 [00:00\u003C00:00, 6279.68it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]\rLoading weights:   4%|\u258d         | 13/290 [00:00\u003C00:00, 6551.24it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   4%|\u258d         | 13/290 [00:00\u003C00:00, 6417.84it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   5%|\u258d         | 14/290 [00:00\u003C00:00, 6604.46it/s, Materializing param=model.layers.1.input_layernorm.weight] \rLoading weights:   5%|\u258d         | 14/290 [00:00\u003C00:00, 6499.20it/s, Materializing param=model.layers.1.input_layernorm.weight]\rLoading weights:   5%|\u258c         | 15/290 [00:00\u003C00:00, 6737.48it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  \rLoading weights:   5%|\u258c         | 15/290 [00:00\u003C00:00, 6628.17it/s, Materializing param=model.layers.1.mlp.down_proj.weight]\rLoading weights:   6%|\u258c         | 16/290 [00:00\u003C00:00, 6861.85it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   6%|\u258c         | 16/290 [00:00\u003C00:00, 6759.56it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   6%|\u258c         | 17/290 [00:00\u003C00:00, 6731.16it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  \rLoading weights:   6%|\u258c         | 17/290 [00:00\u003C00:00, 6637.17it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\rLoading weights:   6%|\u258c         | 18/290 [00:00\u003C00:00, 6455.53it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:   6%|\u258c         | 18/290 [00:00\u003C00:00, 6366.26it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:   7%|\u258b         | 19/290 [00:00\u003C00:00, 6553.06it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          \rLoading weights:   7%|\u258b         | 19/290 [00:00\u003C00:00, 6474.27it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]\rLoading weights:   7%|\u258b         | 20/290 [00:00\u003C00:00, 6636.56it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:   7%|\u258b         | 20/290 [00:00\u003C00:00, 6561.80it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:   7%|\u258b         | 21/290 [00:00\u003C00:00, 6701.19it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:   7%|\u258b         | 21/290 [00:00\u003C00:00, 6629.56it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:   8%|\u258a         | 22/290 [00:00\u003C00:00, 6788.90it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  \rLoading weights:   8%|\u258a         | 22/290 [00:00\u003C00:00, 6720.17it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]\rLoading weights:   8%|\u258a         | 23/290 [00:00\u003C00:00, 6882.78it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:   8%|\u258a         | 23/290 [00:00\u003C00:00, 6813.26it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:   8%|\u258a         | 24/290 [00:00\u003C00:00, 6965.84it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  \rLoading weights:   8%|\u258a         | 24/290 [00:00\u003C00:00, 6901.36it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]\rLoading weights:   9%|\u258a         | 25/290 [00:00\u003C00:00, 7041.68it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:   9%|\u258a         | 25/290 [00:00\u003C00:00, 6976.09it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:   9%|\u2589         | 26/290 [00:00\u003C00:00, 7120.59it/s, Materializing param=model.layers.2.input_layernorm.weight] \rLoading weights:   9%|\u2589         | 26/290 [00:00\u003C00:00, 7056.55it/s, Materializing param=model.layers.2.input_layernorm.weight]\rLoading weights:   9%|\u2589         | 27/290 [00:00\u003C00:00, 7019.97it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  \rLoading weights:   9%|\u2589         | 27/290 [00:00\u003C00:00, 6947.62it/s, Materializing param=model.layers.2.mlp.down_proj.weight]\rLoading weights:  10%|\u2589         | 28/290 [00:00\u003C00:00, 7045.87it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  10%|\u2589         | 28/290 [00:00\u003C00:00, 6975.97it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  10%|\u2588         | 29/290 [00:00\u003C00:00, 7097.79it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  \rLoading weights:  10%|\u2588         | 29/290 [00:00\u003C00:00, 7033.35it/s, Materializing param=model.layers.2.mlp.up_proj.weight]\rLoading weights:  10%|\u2588         | 30/290 [00:00\u003C00:00, 7148.57it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  10%|\u2588         | 30/290 [00:00\u003C00:00, 7089.36it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  11%|\u2588         | 31/290 [00:00\u003C00:00, 7203.91it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          \rLoading weights:  11%|\u2588         | 31/290 [00:00\u003C00:00, 7150.43it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]\rLoading weights:  11%|\u2588         | 32/290 [00:00\u003C00:00, 7266.01it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  11%|\u2588         | 32/290 [00:00\u003C00:00, 7210.58it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  11%|\u2588\u258f        | 33/290 [00:00\u003C00:00, 7083.89it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  11%|\u2588\u258f        | 33/290 [00:00\u003C00:00, 7027.78it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 34/290 [00:00\u003C00:00, 7133.53it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  \rLoading weights:  12%|\u2588\u258f        | 34/290 [00:00\u003C00:00, 7085.32it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]\rLoading weights:  12%|\u2588\u258f        | 35/290 [00:00\u003C00:00, 7187.30it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 35/290 [00:00\u003C00:00, 7141.15it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 36/290 [00:00\u003C00:00, 6966.32it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  \rLoading weights:  12%|\u2588\u258f        | 36/290 [00:00\u003C00:00, 6918.44it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]\rLoading weights:  13%|\u2588\u258e        | 37/290 [00:00\u003C00:00, 7012.30it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  13%|\u2588\u258e        | 37/290 [00:00\u003C00:00, 6968.85it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  13%|\u2588\u258e        | 38/290 [00:00\u003C00:00, 7061.74it/s, Materializing param=model.layers.3.input_layernorm.weight] \rLoading weights:  13%|\u2588\u258e        | 38/290 [00:00\u003C00:00, 7019.14it/s, Materializing param=model.layers.3.input_layernorm.weight]\rLoading weights:  13%|\u2588\u258e        | 39/290 [00:00\u003C00:00, 7112.08it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \rLoading weights:  13%|\u2588\u258e        | 39/290 [00:00\u003C00:00, 7067.83it/s, Materializing param=model.layers.3.mlp.down_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 40/290 [00:00\u003C00:00, 7156.60it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 40/290 [00:00\u003C00:00, 7114.42it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 41/290 [00:00\u003C00:00, 7199.77it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  \rLoading weights:  14%|\u2588\u258d        | 41/290 [00:00\u003C00:00, 7159.30it/s, Materializing param=model.layers.3.mlp.up_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 42/290 [00:00\u003C00:00, 7241.37it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  14%|\u2588\u258d        | 42/290 [00:00\u003C00:00, 7197.29it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  15%|\u2588\u258d        | 43/290 [00:00\u003C00:00, 6489.93it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          \rLoading weights:  15%|\u2588\u258d        | 43/290 [00:00\u003C00:00, 6449.78it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]\rLoading weights:  15%|\u2588\u258c        | 44/290 [00:00\u003C00:00, 6524.64it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  15%|\u2588\u258c        | 44/290 [00:00\u003C00:00, 6491.59it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 45/290 [00:00\u003C00:00, 6570.03it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 45/290 [00:00\u003C00:00, 6537.71it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 46/290 [00:00\u003C00:00, 6619.03it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  \rLoading weights:  16%|\u2588\u258c        | 46/290 [00:00\u003C00:00, 6587.61it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]\rLoading weights:  16%|\u2588\u258c        | 47/290 [00:00\u003C00:00, 6656.50it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 47/290 [00:00\u003C00:00, 6625.18it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 48/290 [00:00\u003C00:00, 6696.82it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  \rLoading weights:  17%|\u2588\u258b        | 48/290 [00:00\u003C00:00, 6666.88it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]\rLoading weights:  17%|\u2588\u258b        | 49/290 [00:00\u003C00:00, 6742.81it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 49/290 [00:00\u003C00:00, 6713.74it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 50/290 [00:00\u003C00:00, 6787.56it/s, Materializing param=model.layers.4.input_layernorm.weight] \rLoading weights:  17%|\u2588\u258b        | 50/290 [00:00\u003C00:00, 6759.12it/s, Materializing param=model.layers.4.input_layernorm.weight]\rLoading weights:  18%|\u2588\u258a        | 51/290 [00:00\u003C00:00, 6775.72it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  \rLoading weights:  18%|\u2588\u258a        | 51/290 [00:00\u003C00:00, 6747.08it/s, Materializing param=model.layers.4.mlp.down_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 52/290 [00:00\u003C00:00, 6815.96it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 52/290 [00:00\u003C00:00, 6786.06it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 53/290 [00:00\u003C00:00, 6858.09it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  \rLoading weights:  18%|\u2588\u258a        | 53/290 [00:00\u003C00:00, 6829.02it/s, Materializing param=model.layers.4.mlp.up_proj.weight]\rLoading weights:  19%|\u2588\u258a        | 54/290 [00:00\u003C00:00, 6564.24it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  19%|\u2588\u258a        | 54/290 [00:00\u003C00:00, 6534.50it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  19%|\u2588\u2589        | 55/290 [00:00\u003C00:00, 6596.51it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          \rLoading weights:  19%|\u2588\u2589        | 55/290 [00:00\u003C00:00, 6570.96it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]\rLoading weights:  19%|\u2588\u2589        | 56/290 [00:00\u003C00:00, 6627.38it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  19%|\u2588\u2589        | 56/290 [00:00\u003C00:00, 6601.68it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  20%|\u2588\u2589        | 57/290 [00:00\u003C00:00, 6664.31it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  20%|\u2588\u2589        | 57/290 [00:00\u003C00:00, 6640.06it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  20%|\u2588\u2588        | 58/290 [00:00\u003C00:00, 6700.90it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  \rLoading weights:  20%|\u2588\u2588        | 58/290 [00:00\u003C00:00, 6675.71it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]\rLoading weights:  20%|\u2588\u2588        | 59/290 [00:00\u003C00:00, 6736.10it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  20%|\u2588\u2588        | 59/290 [00:00\u003C00:00, 6711.25it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  21%|\u2588\u2588        | 60/290 [00:00\u003C00:00, 6556.50it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  \rLoading weights:  21%|\u2588\u2588        | 60/290 [00:00\u003C00:00, 6529.45it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]\rLoading weights:  21%|\u2588\u2588        | 61/290 [00:00\u003C00:00, 6457.82it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  21%|\u2588\u2588        | 61/290 [00:00\u003C00:00, 6432.66it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  21%|\u2588\u2588\u258f       | 62/290 [00:00\u003C00:00, 6486.25it/s, Materializing param=model.layers.5.input_layernorm.weight] \rLoading weights:  21%|\u2588\u2588\u258f       | 62/290 [00:00\u003C00:00, 6464.16it/s, Materializing param=model.layers.5.input_layernorm.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 63/290 [00:00\u003C00:00, 6519.97it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  \rLoading weights:  22%|\u2588\u2588\u258f       | 63/290 [00:00\u003C00:00, 6497.04it/s, Materializing param=model.layers.5.mlp.down_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 64/290 [00:00\u003C00:00, 6540.82it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 64/290 [00:00\u003C00:00, 6518.59it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 65/290 [00:00\u003C00:00, 6572.56it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \rLoading weights:  22%|\u2588\u2588\u258f       | 65/290 [00:00\u003C00:00, 6550.61it/s, Materializing param=model.layers.5.mlp.up_proj.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 66/290 [00:00\u003C00:00, 6603.94it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 66/290 [00:00\u003C00:00, 6575.86it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 67/290 [00:00\u003C00:00, 6573.38it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          \rLoading weights:  23%|\u2588\u2588\u258e       | 67/290 [00:00\u003C00:00, 6550.70it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]\rLoading weights:  23%|\u2588\u2588\u258e       | 68/290 [00:00\u003C00:00, 6336.09it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 68/290 [00:00\u003C00:00, 6314.63it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 69/290 [00:00\u003C00:00, 6197.95it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 69/290 [00:00\u003C00:00, 6178.50it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 70/290 [00:00\u003C00:00, 6227.36it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  \rLoading weights:  24%|\u2588\u2588\u258d       | 70/290 [00:00\u003C00:00, 6209.32it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]\rLoading weights:  24%|\u2588\u2588\u258d       | 71/290 [00:00\u003C00:00, 6256.87it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 71/290 [00:00\u003C00:00, 6238.12it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  25%|\u2588\u2588\u258d       | 72/290 [00:00\u003C00:00, 6287.13it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  \rLoading weights:  25%|\u2588\u2588\u258d       | 72/290 [00:00\u003C00:00, 6268.73it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]\rLoading weights:  25%|\u2588\u2588\u258c       | 73/290 [00:00\u003C00:00, 6316.85it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  25%|\u2588\u2588\u258c       | 73/290 [00:00\u003C00:00, 6299.31it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 74/290 [00:00\u003C00:00, 6347.47it/s, Materializing param=model.layers.6.input_layernorm.weight] \rLoading weights:  26%|\u2588\u2588\u258c       | 74/290 [00:00\u003C00:00, 6329.22it/s, Materializing param=model.layers.6.input_layernorm.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 75/290 [00:00\u003C00:00, 6375.10it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  \rLoading weights:  26%|\u2588\u2588\u258c       | 75/290 [00:00\u003C00:00, 6356.80it/s, Materializing param=model.layers.6.mlp.down_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 76/290 [00:00\u003C00:00, 6402.49it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 76/290 [00:00\u003C00:00, 6384.54it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 77/290 [00:00\u003C00:00, 6430.80it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  \rLoading weights:  27%|\u2588\u2588\u258b       | 77/290 [00:00\u003C00:00, 6407.96it/s, Materializing param=model.layers.6.mlp.up_proj.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 78/290 [00:00\u003C00:00, 6451.88it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 78/290 [00:00\u003C00:00, 6434.37it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 79/290 [00:00\u003C00:00, 6361.35it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          \rLoading weights:  27%|\u2588\u2588\u258b       | 79/290 [00:00\u003C00:00, 6343.33it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]\rLoading weights:  28%|\u2588\u2588\u258a       | 80/290 [00:00\u003C00:00, 6386.33it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 80/290 [00:00\u003C00:00, 6369.12it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 81/290 [00:00\u003C00:00, 6411.86it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 81/290 [00:00\u003C00:00, 6395.20it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 82/290 [00:00\u003C00:00, 6235.98it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  \rLoading weights:  28%|\u2588\u2588\u258a       | 82/290 [00:00\u003C00:00, 6218.61it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]\rLoading weights:  29%|\u2588\u2588\u258a       | 83/290 [00:00\u003C00:00, 6257.79it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  29%|\u2588\u2588\u258a       | 83/290 [00:00\u003C00:00, 6241.75it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  29%|\u2588\u2588\u2589       | 84/290 [00:00\u003C00:00, 6283.49it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  \rLoading weights:  29%|\u2588\u2588\u2589       | 84/290 [00:00\u003C00:00, 6267.17it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]\rLoading weights:  29%|\u2588\u2588\u2589       | 85/290 [00:00\u003C00:00, 6307.78it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  29%|\u2588\u2588\u2589       | 85/290 [00:00\u003C00:00, 6292.42it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2589       | 86/290 [00:00\u003C00:00, 6332.03it/s, Materializing param=model.layers.7.input_layernorm.weight] \rLoading weights:  30%|\u2588\u2588\u2589       | 86/290 [00:00\u003C00:00, 6316.17it/s, Materializing param=model.layers.7.input_layernorm.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 87/290 [00:00\u003C00:00, 6251.68it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  \rLoading weights:  30%|\u2588\u2588\u2588       | 87/290 [00:00\u003C00:00, 6235.76it/s, Materializing param=model.layers.7.mlp.down_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 88/290 [00:00\u003C00:00, 6270.47it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 88/290 [00:00\u003C00:00, 6255.59it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 89/290 [00:00\u003C00:00, 6293.61it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  \rLoading weights:  31%|\u2588\u2588\u2588       | 89/290 [00:00\u003C00:00, 6278.90it/s, Materializing param=model.layers.7.mlp.up_proj.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 90/290 [00:00\u003C00:00, 6187.10it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 90/290 [00:00\u003C00:00, 6171.22it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  31%|\u2588\u2588\u2588\u258f      | 91/290 [00:00\u003C00:00, 6208.02it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          \rLoading weights:  31%|\u2588\u2588\u2588\u258f      | 91/290 [00:00\u003C00:00, 6194.02it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 92/290 [00:00\u003C00:00, 6232.35it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 92/290 [00:00\u003C00:00, 6218.69it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 93/290 [00:00\u003C00:00, 6256.84it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 93/290 [00:00\u003C00:00, 6242.32it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 94/290 [00:00\u003C00:00, 6056.66it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  \rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 94/290 [00:00\u003C00:00, 6040.42it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 95/290 [00:00\u003C00:00, 6067.22it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 95/290 [00:00\u003C00:00, 6053.58it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 96/290 [00:00\u003C00:00, 6088.63it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  \rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 96/290 [00:00\u003C00:00, 6075.49it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 97/290 [00:00\u003C00:00, 6077.79it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 97/290 [00:00\u003C00:00, 6064.30it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 98/290 [00:00\u003C00:00, 6096.10it/s, Materializing param=model.layers.8.input_layernorm.weight] \rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 98/290 [00:00\u003C00:00, 6083.47it/s, Materializing param=model.layers.8.input_layernorm.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 99/290 [00:00\u003C00:00, 6118.38it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  \rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 99/290 [00:00\u003C00:00, 6106.14it/s, Materializing param=model.layers.8.mlp.down_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 100/290 [00:00\u003C00:00, 6141.18it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 100/290 [00:00\u003C00:00, 6129.34it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258d      | 101/290 [00:00\u003C00:00, 6165.13it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  \rLoading weights:  35%|\u2588\u2588\u2588\u258d      | 101/290 [00:00\u003C00:00, 6152.77it/s, Materializing param=model.layers.8.mlp.up_proj.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258c      | 102/290 [00:00\u003C00:00, 6187.99it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258c      | 102/290 [00:00\u003C00:00, 6175.13it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 103/290 [00:00\u003C00:00, 6208.78it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          \rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 103/290 [00:00\u003C00:00, 6196.14it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 104/290 [00:00\u003C00:00, 6046.43it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 104/290 [00:00\u003C00:00, 6033.88it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 105/290 [00:00\u003C00:00, 6066.06it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 105/290 [00:00\u003C00:00, 6054.05it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 106/290 [00:00\u003C00:00, 6087.36it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  \rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 106/290 [00:00\u003C00:00, 6075.71it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 107/290 [00:00\u003C00:00, 6107.41it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 107/290 [00:00\u003C00:00, 6096.21it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 108/290 [00:00\u003C00:00, 6104.59it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  \rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 108/290 [00:00\u003C00:00, 6092.76it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 109/290 [00:00\u003C00:00, 6123.40it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 109/290 [00:00\u003C00:00, 6112.27it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 110/290 [00:00\u003C00:00, 6039.00it/s, Materializing param=model.layers.9.input_layernorm.weight] \rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 110/290 [00:00\u003C00:00, 6026.93it/s, Materializing param=model.layers.9.input_layernorm.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 111/290 [00:00\u003C00:00, 6058.06it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  \rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 111/290 [00:00\u003C00:00, 6047.98it/s, Materializing param=model.layers.9.mlp.down_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u258a      | 112/290 [00:00\u003C00:00, 6079.17it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u258a      | 112/290 [00:00\u003C00:00, 6067.94it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 113/290 [00:00\u003C00:00, 6099.04it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  \rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 113/290 [00:00\u003C00:00, 6088.07it/s, Materializing param=model.layers.9.mlp.up_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 114/290 [00:00\u003C00:00, 6117.90it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 114/290 [00:00\u003C00:00, 6106.81it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2589      | 115/290 [00:00\u003C00:00, 6087.60it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          \rLoading weights:  40%|\u2588\u2588\u2588\u2589      | 115/290 [00:00\u003C00:00, 6076.63it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 116/290 [00:00\u003C00:00, 6049.38it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 116/290 [00:00\u003C00:00, 6038.19it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 117/290 [00:00\u003C00:00, 6068.18it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 117/290 [00:00\u003C00:00, 6057.69it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 118/290 [00:00\u003C00:00, 6002.18it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  \rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 118/290 [00:00\u003C00:00, 5990.99it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 119/290 [00:00\u003C00:00, 6018.16it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 119/290 [00:00\u003C00:00, 6007.80it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588\u258f     | 120/290 [00:00\u003C00:00, 6037.14it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  \rLoading weights:  41%|\u2588\u2588\u2588\u2588\u258f     | 120/290 [00:00\u003C00:00, 6027.38it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 121/290 [00:00\u003C00:00, 6007.25it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 121/290 [00:00\u003C00:00, 5996.82it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 122/290 [00:00\u003C00:00, 6024.81it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 122/290 [00:00\u003C00:00, 6015.18it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 123/290 [00:00\u003C00:00, 5885.43it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  \rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 123/290 [00:00\u003C00:00, 5875.18it/s, Materializing param=model.layers.10.mlp.down_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 124/290 [00:00\u003C00:00, 5820.86it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 124/290 [00:00\u003C00:00, 5809.93it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 125/290 [00:00\u003C00:00, 5835.80it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  \rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 125/290 [00:00\u003C00:00, 5826.85it/s, Materializing param=model.layers.10.mlp.up_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 126/290 [00:00\u003C00:00, 5833.91it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 126/290 [00:00\u003C00:00, 5824.07it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 127/290 [00:00\u003C00:00, 5849.60it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          \rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 127/290 [00:00\u003C00:00, 5841.07it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 128/290 [00:00\u003C00:00, 5867.83it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 128/290 [00:00\u003C00:00, 5859.05it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 129/290 [00:00\u003C00:00, 5885.75it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 129/290 [00:00\u003C00:00, 5877.18it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258d     | 130/290 [00:00\u003C00:00, 5902.87it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  \rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258d     | 130/290 [00:00\u003C00:00, 5894.32it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258c     | 131/290 [00:00\u003C00:00, 5921.09it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258c     | 131/290 [00:00\u003C00:00, 5912.62it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 132/290 [00:00\u003C00:00, 5889.31it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  \rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 132/290 [00:00\u003C00:00, 5880.05it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 133/290 [00:00\u003C00:00, 5905.16it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 133/290 [00:00\u003C00:00, 5896.23it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 134/290 [00:00\u003C00:00, 5922.04it/s, Materializing param=model.layers.11.input_layernorm.weight] \rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 134/290 [00:00\u003C00:00, 5913.44it/s, Materializing param=model.layers.11.input_layernorm.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 135/290 [00:00\u003C00:00, 5939.69it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  \rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 135/290 [00:00\u003C00:00, 5931.17it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 136/290 [00:00\u003C00:00, 5956.51it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 136/290 [00:00\u003C00:00, 5948.13it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 137/290 [00:00\u003C00:00, 5973.43it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 137/290 [00:00\u003C00:00, 5964.93it/s, Materializing param=model.layers.11.mlp.up_proj.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 138/290 [00:00\u003C00:00, 5947.47it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 138/290 [00:00\u003C00:00, 5937.95it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 139/290 [00:00\u003C00:00, 5961.72it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          \rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 139/290 [00:00\u003C00:00, 5953.38it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 140/290 [00:00\u003C00:00, 5977.53it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 140/290 [00:00\u003C00:00, 5969.20it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u258a     | 141/290 [00:00\u003C00:00, 5993.62it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u258a     | 141/290 [00:00\u003C00:00, 5985.31it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 142/290 [00:00\u003C00:00, 5944.85it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  \rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 142/290 [00:00\u003C00:00, 5936.44it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 143/290 [00:00\u003C00:00, 5960.01it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 143/290 [00:00\u003C00:00, 5950.72it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2589     | 144/290 [00:00\u003C00:00, 5972.67it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  \rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2589     | 144/290 [00:00\u003C00:00, 5964.17it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 145/290 [00:00\u003C00:00, 5988.79it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 145/290 [00:00\u003C00:00, 5980.79it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 146/290 [00:00\u003C00:00, 5978.64it/s, Materializing param=model.layers.12.input_layernorm.weight] \rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 146/290 [00:00\u003C00:00, 5970.31it/s, Materializing param=model.layers.12.input_layernorm.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 147/290 [00:00\u003C00:00, 5954.77it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  \rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 147/290 [00:00\u003C00:00, 5946.10it/s, Materializing param=model.layers.12.mlp.down_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 148/290 [00:00\u003C00:00, 5921.50it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 148/290 [00:00\u003C00:00, 5906.35it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 149/290 [00:00\u003C00:00, 5753.98it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  \rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 149/290 [00:00\u003C00:00, 5734.81it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 150/290 [00:00\u003C00:00, 5746.57it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 150/290 [00:00\u003C00:00, 5736.56it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 151/290 [00:00\u003C00:00, 5756.17it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          \rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 151/290 [00:00\u003C00:00, 5747.86it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 152/290 [00:00\u003C00:00, 5751.17it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 152/290 [00:00\u003C00:00, 5743.29it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 153/290 [00:00\u003C00:00, 5764.46it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 153/290 [00:00\u003C00:00, 5756.86it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 154/290 [00:00\u003C00:00, 5778.62it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  \rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 154/290 [00:00\u003C00:00, 5770.83it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 155/290 [00:00\u003C00:00, 5737.66it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 155/290 [00:00\u003C00:00, 5729.67it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 156/290 [00:00\u003C00:00, 5750.72it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  \rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 156/290 [00:00\u003C00:00, 5743.91it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 157/290 [00:00\u003C00:00, 5765.24it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 157/290 [00:00\u003C00:00, 5758.08it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 158/290 [00:00\u003C00:00, 5779.65it/s, Materializing param=model.layers.13.input_layernorm.weight] \rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 158/290 [00:00\u003C00:00, 5772.80it/s, Materializing param=model.layers.13.input_layernorm.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 159/290 [00:00\u003C00:00, 5793.49it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  \rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 159/290 [00:00\u003C00:00, 5786.55it/s, Materializing param=model.layers.13.mlp.down_proj.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 160/290 [00:00\u003C00:00, 5807.73it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 160/290 [00:00\u003C00:00, 5800.90it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 161/290 [00:00\u003C00:00, 5759.74it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  \rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 161/290 [00:00\u003C00:00, 5751.84it/s, Materializing param=model.layers.13.mlp.up_proj.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 162/290 [00:00\u003C00:00, 5771.39it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 162/290 [00:00\u003C00:00, 5763.90it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 163/290 [00:00\u003C00:00, 5784.07it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          \rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 163/290 [00:00\u003C00:00, 5776.45it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 164/290 [00:00\u003C00:00, 5796.90it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 164/290 [00:00\u003C00:00, 5790.17it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 165/290 [00:00\u003C00:00, 5801.35it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 165/290 [00:00\u003C00:00, 5794.21it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 166/290 [00:00\u003C00:00, 5814.77it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  \rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 166/290 [00:00\u003C00:00, 5807.98it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 167/290 [00:00\u003C00:00, 5761.83it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 167/290 [00:00\u003C00:00, 5753.27it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 168/290 [00:00\u003C00:00, 5773.54it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  \rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 168/290 [00:00\u003C00:00, 5766.83it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 169/290 [00:00\u003C00:00, 5787.51it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 169/290 [00:00\u003C00:00, 5781.24it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 170/290 [00:00\u003C00:00, 5774.70it/s, Materializing param=model.layers.14.input_layernorm.weight] \rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 170/290 [00:00\u003C00:00, 5767.56it/s, Materializing param=model.layers.14.input_layernorm.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 171/290 [00:00\u003C00:00, 5786.79it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  \rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 171/290 [00:00\u003C00:00, 5779.93it/s, Materializing param=model.layers.14.mlp.down_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 172/290 [00:00\u003C00:00, 5799.62it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 172/290 [00:00\u003C00:00, 5793.05it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 173/290 [00:00\u003C00:00, 5812.64it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  \rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 173/290 [00:00\u003C00:00, 5806.22it/s, Materializing param=model.layers.14.mlp.up_proj.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 174/290 [00:00\u003C00:00, 5825.70it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 174/290 [00:00\u003C00:00, 5818.69it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 175/290 [00:00\u003C00:00, 5838.44it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          \rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 175/290 [00:00\u003C00:00, 5830.47it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 176/290 [00:00\u003C00:00, 5804.90it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 176/290 [00:00\u003C00:00, 5797.74it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 177/290 [00:00\u003C00:00, 5784.75it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 177/290 [00:00\u003C00:00, 5778.00it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 178/290 [00:00\u003C00:00, 5797.02it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  \rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 178/290 [00:00\u003C00:00, 5790.72it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 179/290 [00:00\u003C00:00, 5807.94it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 179/290 [00:00\u003C00:00, 5801.65it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 180/290 [00:00\u003C00:00, 5819.45it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  \rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 180/290 [00:00\u003C00:00, 5813.40it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 181/290 [00:00\u003C00:00, 5742.71it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 181/290 [00:00\u003C00:00, 5735.42it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 182/290 [00:00\u003C00:00, 5695.76it/s, Materializing param=model.layers.15.input_layernorm.weight] \rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 182/290 [00:00\u003C00:00, 5688.59it/s, Materializing param=model.layers.15.input_layernorm.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 183/290 [00:00\u003C00:00, 5705.26it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 183/290 [00:00\u003C00:00, 5699.25it/s, Materializing param=model.layers.15.mlp.down_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 184/290 [00:00\u003C00:00, 5700.89it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 184/290 [00:00\u003C00:00, 5694.75it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 185/290 [00:00\u003C00:00, 5712.38it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  \rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 185/290 [00:00\u003C00:00, 5706.58it/s, Materializing param=model.layers.15.mlp.up_proj.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 186/290 [00:00\u003C00:00, 5712.80it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 186/290 [00:00\u003C00:00, 5706.12it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 187/290 [00:00\u003C00:00, 5723.65it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          \rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 187/290 [00:00\u003C00:00, 5717.81it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188/290 [00:00\u003C00:00, 5735.68it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188/290 [00:00\u003C00:00, 5729.76it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 189/290 [00:00\u003C00:00, 5747.75it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 189/290 [00:00\u003C00:00, 5742.00it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 190/290 [00:00\u003C00:00, 5760.16it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  \rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 190/290 [00:00\u003C00:00, 5753.84it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 191/290 [00:00\u003C00:00, 5771.16it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 191/290 [00:00\u003C00:00, 5765.55it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 192/290 [00:00\u003C00:00, 5783.21it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  \rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 192/290 [00:00\u003C00:00, 5777.32it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 193/290 [00:00\u003C00:00, 5795.15it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 193/290 [00:00\u003C00:00, 5789.34it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 194/290 [00:00\u003C00:00, 5780.77it/s, Materializing param=model.layers.16.input_layernorm.weight] \rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 194/290 [00:00\u003C00:00, 5774.24it/s, Materializing param=model.layers.16.input_layernorm.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 195/290 [00:00\u003C00:00, 5746.35it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  \rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 195/290 [00:00\u003C00:00, 5740.26it/s, Materializing param=model.layers.16.mlp.down_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 196/290 [00:00\u003C00:00, 5755.96it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 196/290 [00:00\u003C00:00, 5750.16it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 197/290 [00:00\u003C00:00, 5767.32it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  \rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 197/290 [00:00\u003C00:00, 5761.85it/s, Materializing param=model.layers.16.mlp.up_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 198/290 [00:00\u003C00:00, 5736.14it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 198/290 [00:00\u003C00:00, 5729.61it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 199/290 [00:00\u003C00:00, 5746.33it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          \rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 199/290 [00:00\u003C00:00, 5740.92it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 200/290 [00:00\u003C00:00, 5757.22it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 200/290 [00:00\u003C00:00, 5751.37it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 201/290 [00:00\u003C00:00, 5768.34it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 201/290 [00:00\u003C00:00, 5762.55it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 202/290 [00:00\u003C00:00, 5780.00it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  \rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 202/290 [00:00\u003C00:00, 5774.64it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 203/290 [00:00\u003C00:00, 5790.79it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 203/290 [00:00\u003C00:00, 5785.33it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 204/290 [00:00\u003C00:00, 5773.07it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  \rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 204/290 [00:00\u003C00:00, 5767.31it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 205/290 [00:00\u003C00:00, 5779.53it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 205/290 [00:00\u003C00:00, 5774.17it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 206/290 [00:00\u003C00:00, 5791.06it/s, Materializing param=model.layers.17.input_layernorm.weight] \rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 206/290 [00:00\u003C00:00, 5785.79it/s, Materializing param=model.layers.17.input_layernorm.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 207/290 [00:00\u003C00:00, 5802.49it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  \rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 207/290 [00:00\u003C00:00, 5796.99it/s, Materializing param=model.layers.17.mlp.down_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 208/290 [00:00\u003C00:00, 5732.90it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 208/290 [00:00\u003C00:00, 5727.18it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 209/290 [00:00\u003C00:00, 5742.23it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  \rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 209/290 [00:00\u003C00:00, 5737.27it/s, Materializing param=model.layers.17.mlp.up_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 210/290 [00:00\u003C00:00, 5753.05it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 210/290 [00:00\u003C00:00, 5747.38it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 211/290 [00:00\u003C00:00, 5763.62it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          \rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 211/290 [00:00\u003C00:00, 5758.37it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 212/290 [00:00\u003C00:00, 5753.24it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 212/290 [00:00\u003C00:00, 5747.55it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 213/290 [00:00\u003C00:00, 5762.86it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 213/290 [00:00\u003C00:00, 5754.65it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 214/290 [00:00\u003C00:00, 5746.69it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  \rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 214/290 [00:00\u003C00:00, 5741.03it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 215/290 [00:00\u003C00:00, 5756.33it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 215/290 [00:00\u003C00:00, 5751.05it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 216/290 [00:00\u003C00:00, 5766.58it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  \rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 216/290 [00:00\u003C00:00, 5761.44it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 217/290 [00:00\u003C00:00, 5765.79it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 217/290 [00:00\u003C00:00, 5760.49it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 218/290 [00:00\u003C00:00, 5775.93it/s, Materializing param=model.layers.18.input_layernorm.weight] \rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 218/290 [00:00\u003C00:00, 5770.79it/s, Materializing param=model.layers.18.input_layernorm.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 219/290 [00:00\u003C00:00, 5786.38it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  \rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 219/290 [00:00\u003C00:00, 5781.39it/s, Materializing param=model.layers.18.mlp.down_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 220/290 [00:00\u003C00:00, 5796.99it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 220/290 [00:00\u003C00:00, 5792.00it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 221/290 [00:00\u003C00:00, 5803.32it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  \rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 221/290 [00:00\u003C00:00, 5798.27it/s, Materializing param=model.layers.18.mlp.up_proj.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 222/290 [00:00\u003C00:00, 5813.13it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 222/290 [00:00\u003C00:00, 5807.65it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 223/290 [00:00\u003C00:00, 5822.96it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          \rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 223/290 [00:00\u003C00:00, 5817.89it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 224/290 [00:00\u003C00:00, 5810.04it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 224/290 [00:00\u003C00:00, 5804.69it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 225/290 [00:00\u003C00:00, 5819.71it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 225/290 [00:00\u003C00:00, 5814.58it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 226/290 [00:00\u003C00:00, 5828.75it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  \rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 226/290 [00:00\u003C00:00, 5821.56it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 227/290 [00:00\u003C00:00, 5822.14it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 227/290 [00:00\u003C00:00, 5815.96it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 228/290 [00:00\u003C00:00, 5794.29it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  \rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 228/290 [00:00\u003C00:00, 5789.03it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 229/290 [00:00\u003C00:00, 5803.39it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 229/290 [00:00\u003C00:00, 5798.62it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 230/290 [00:00\u003C00:00, 5813.70it/s, Materializing param=model.layers.19.input_layernorm.weight] \rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 230/290 [00:00\u003C00:00, 5808.69it/s, Materializing param=model.layers.19.input_layernorm.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 231/290 [00:00\u003C00:00, 5823.25it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  \rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 231/290 [00:00\u003C00:00, 5818.43it/s, Materializing param=model.layers.19.mlp.down_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 232/290 [00:00\u003C00:00, 5777.45it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 232/290 [00:00\u003C00:00, 5771.97it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 233/290 [00:00\u003C00:00, 5786.51it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  \rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 233/290 [00:00\u003C00:00, 5781.62it/s, Materializing param=model.layers.19.mlp.up_proj.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 234/290 [00:00\u003C00:00, 5793.10it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 234/290 [00:00\u003C00:00, 5787.94it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 235/290 [00:00\u003C00:00, 5797.33it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          \rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 235/290 [00:00\u003C00:00, 5792.42it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 236/290 [00:00\u003C00:00, 5806.90it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 236/290 [00:00\u003C00:00, 5801.96it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 237/290 [00:00\u003C00:00, 5816.19it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 237/290 [00:00\u003C00:00, 5811.36it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 238/290 [00:00\u003C00:00, 5825.97it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  \rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 238/290 [00:00\u003C00:00, 5821.18it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 239/290 [00:00\u003C00:00, 5803.33it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 239/290 [00:00\u003C00:00, 5798.30it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 240/290 [00:00\u003C00:00, 5812.17it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  \rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 240/290 [00:00\u003C00:00, 5807.34it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 241/290 [00:00\u003C00:00, 5773.32it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 241/290 [00:00\u003C00:00, 5768.21it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 242/290 [00:00\u003C00:00, 5781.89it/s, Materializing param=model.layers.20.input_layernorm.weight] \rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 242/290 [00:00\u003C00:00, 5777.34it/s, Materializing param=model.layers.20.input_layernorm.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 243/290 [00:00\u003C00:00, 5791.62it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  \rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 243/290 [00:00\u003C00:00, 5787.12it/s, Materializing param=model.layers.20.mlp.down_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 244/290 [00:00\u003C00:00, 5800.07it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 244/290 [00:00\u003C00:00, 5795.40it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 245/290 [00:00\u003C00:00, 5799.29it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  \rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 245/290 [00:00\u003C00:00, 5794.45it/s, Materializing param=model.layers.20.mlp.up_proj.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 246/290 [00:00\u003C00:00, 5808.70it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 246/290 [00:00\u003C00:00, 5803.99it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 247/290 [00:00\u003C00:00, 5817.18it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          \rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 247/290 [00:00\u003C00:00, 5812.61it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 248/290 [00:00\u003C00:00, 5814.55it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 248/290 [00:00\u003C00:00, 5809.64it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 249/290 [00:00\u003C00:00, 5802.70it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 249/290 [00:00\u003C00:00, 5797.90it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 250/290 [00:00\u003C00:00, 5796.47it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  \rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 250/290 [00:00\u003C00:00, 5791.77it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 251/290 [00:00\u003C00:00, 5805.15it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 251/290 [00:00\u003C00:00, 5800.67it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 252/290 [00:00\u003C00:00, 5813.95it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  \rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 252/290 [00:00\u003C00:00, 5809.57it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 253/290 [00:00\u003C00:00, 5769.49it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 253/290 [00:00\u003C00:00, 5764.82it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 254/290 [00:00\u003C00:00, 5778.12it/s, Materializing param=model.layers.21.input_layernorm.weight] \rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 254/290 [00:00\u003C00:00, 5773.62it/s, Materializing param=model.layers.21.input_layernorm.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 255/290 [00:00\u003C00:00, 5786.50it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  \rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 255/290 [00:00\u003C00:00, 5782.24it/s, Materializing param=model.layers.21.mlp.down_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 256/290 [00:00\u003C00:00, 5789.58it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 256/290 [00:00\u003C00:00, 5784.65it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 257/290 [00:00\u003C00:00, 5798.32it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  \rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 257/290 [00:00\u003C00:00, 5793.86it/s, Materializing param=model.layers.21.mlp.up_proj.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 258/290 [00:00\u003C00:00, 5806.70it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 258/290 [00:00\u003C00:00, 5802.25it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 259/290 [00:00\u003C00:00, 5810.84it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          \rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 259/290 [00:00\u003C00:00, 5806.34it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 260/290 [00:00\u003C00:00, 5819.21it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 260/290 [00:00\u003C00:00, 5814.83it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 261/290 [00:00\u003C00:00, 5803.80it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 261/290 [00:00\u003C00:00, 5799.34it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 262/290 [00:00\u003C00:00, 5785.67it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  \rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 262/290 [00:00\u003C00:00, 5781.11it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 263/290 [00:00\u003C00:00, 5780.15it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 263/290 [00:00\u003C00:00, 5775.49it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 264/290 [00:00\u003C00:00, 5787.70it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  \rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 264/290 [00:00\u003C00:00, 5783.52it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 265/290 [00:00\u003C00:00, 5792.15it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 265/290 [00:00\u003C00:00, 5787.78it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 266/290 [00:00\u003C00:00, 5800.01it/s, Materializing param=model.layers.22.input_layernorm.weight] \rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 266/290 [00:00\u003C00:00, 5795.07it/s, Materializing param=model.layers.22.input_layernorm.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 267/290 [00:00\u003C00:00, 5782.44it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  \rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 267/290 [00:00\u003C00:00, 5777.81it/s, Materializing param=model.layers.22.mlp.down_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 268/290 [00:00\u003C00:00, 5790.28it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 268/290 [00:00\u003C00:00, 5786.14it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 269/290 [00:00\u003C00:00, 5798.75it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  \rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 269/290 [00:00\u003C00:00, 5794.78it/s, Materializing param=model.layers.22.mlp.up_proj.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 270/290 [00:00\u003C00:00, 5806.28it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 270/290 [00:00\u003C00:00, 5801.76it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 271/290 [00:00\u003C00:00, 5813.77it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          \rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 271/290 [00:00\u003C00:00, 5809.67it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 272/290 [00:00\u003C00:00, 5760.53it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 272/290 [00:00\u003C00:00, 5756.15it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 273/290 [00:00\u003C00:00, 5768.11it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 273/290 [00:00\u003C00:00, 5764.07it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 274/290 [00:00\u003C00:00, 5776.52it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  \rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 274/290 [00:00\u003C00:00, 5772.52it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 275/290 [00:00\u003C00:00, 5781.10it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 275/290 [00:00\u003C00:00, 5776.84it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 276/290 [00:00\u003C00:00, 5789.53it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  \rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 276/290 [00:00\u003C00:00, 5785.45it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 277/290 [00:00\u003C00:00, 5797.57it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 277/290 [00:00\u003C00:00, 5793.50it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 278/290 [00:00\u003C00:00, 5805.73it/s, Materializing param=model.layers.23.input_layernorm.weight] \rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 278/290 [00:00\u003C00:00, 5801.68it/s, Materializing param=model.layers.23.input_layernorm.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 279/290 [00:00\u003C00:00, 5813.96it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  \rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 279/290 [00:00\u003C00:00, 5809.31it/s, Materializing param=model.layers.23.mlp.down_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 280/290 [00:00\u003C00:00, 5818.70it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 280/290 [00:00\u003C00:00, 5814.58it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 281/290 [00:00\u003C00:00, 5804.82it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  \rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 281/290 [00:00\u003C00:00, 5800.54it/s, Materializing param=model.layers.23.mlp.up_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 282/290 [00:00\u003C00:00, 5812.28it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 282/290 [00:00\u003C00:00, 5808.03it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 283/290 [00:00\u003C00:00, 5789.96it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          \rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 283/290 [00:00\u003C00:00, 5785.56it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 284/290 [00:00\u003C00:00, 5797.16it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 284/290 [00:00\u003C00:00, 5792.65it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 285/290 [00:00\u003C00:00, 5804.43it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 285/290 [00:00\u003C00:00, 5800.35it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 286/290 [00:00\u003C00:00, 5811.57it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  \rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 286/290 [00:00\u003C00:00, 5807.63it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 287/290 [00:00\u003C00:00, 5808.22it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 287/290 [00:00\u003C00:00, 5804.02it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 288/290 [00:00\u003C00:00, 5816.14it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  \rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 288/290 [00:00\u003C00:00, 5812.14it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 289/290 [00:00\u003C00:00, 5824.58it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 289/290 [00:00\u003C00:00, 5820.36it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 5832.82it/s, Materializing param=model.norm.weight]                      \rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 5828.80it/s, Materializing param=model.norm.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 5813.23it/s, Materializing param=model.norm.weight]\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "[ZorplexWorker:0] Initialized with difficulty=easy\n[ZorplexWorker:0] Initialized with difficulty=easy\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stderr", "text": "\rLoading weights:   0%|          | 0/290 [00:00\u003C?, ?it/s]\rLoading weights:   0%|          | 1/290 [00:00\u003C00:00, 16384.00it/s, Materializing param=model.embed_tokens.weight]\rLoading weights:   0%|          | 1/290 [00:00\u003C00:00, 7503.23it/s, Materializing param=model.embed_tokens.weight] \rLoading weights:   1%|          | 2/290 [00:00\u003C00:00, 6961.50it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|          | 2/290 [00:00\u003C00:00, 6000.43it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|          | 3/290 [00:00\u003C00:00, 6750.49it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  \rLoading weights:   1%|          | 3/290 [00:00\u003C00:00, 6150.01it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   1%|\u258f         | 4/290 [00:00\u003C00:00, 6901.36it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   1%|\u258f         | 4/290 [00:00\u003C00:00, 6442.86it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   2%|\u258f         | 5/290 [00:00\u003C00:00, 6992.84it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \rLoading weights:   2%|\u258f         | 5/290 [00:00\u003C00:00, 6649.18it/s, Materializing param=model.layers.0.mlp.up_proj.weight]\rLoading weights:   2%|\u258f         | 6/290 [00:00\u003C00:00, 7202.58it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   2%|\u258f         | 6/290 [00:00\u003C00:00, 6847.84it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   2%|\u258f         | 7/290 [00:00\u003C00:00, 7289.01it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          \rLoading weights:   2%|\u258f         | 7/290 [00:00\u003C00:00, 7029.00it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]\rLoading weights:   3%|\u258e         | 8/290 [00:00\u003C00:00, 7451.57it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|\u258e         | 8/290 [00:00\u003C00:00, 7209.80it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|\u258e         | 9/290 [00:00\u003C00:00, 7589.21it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   3%|\u258e         | 9/290 [00:00\u003C00:00, 7387.23it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   3%|\u258e         | 10/290 [00:00\u003C00:00, 7745.71it/s, Materializing param=model.layers.0.self_attn.q_proj.bias] \rLoading weights:   3%|\u258e         | 10/290 [00:00\u003C00:00, 7539.64it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]\rLoading weights:   4%|\u258d         | 11/290 [00:00\u003C00:00, 7797.42it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   4%|\u258d         | 11/290 [00:00\u003C00:00, 7598.38it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   4%|\u258d         | 12/290 [00:00\u003C00:00, 7842.26it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  \rLoading weights:   4%|\u258d         | 12/290 [00:00\u003C00:00, 7660.83it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]\rLoading weights:   4%|\u258d         | 13/290 [00:00\u003C00:00, 7887.45it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   4%|\u258d         | 13/290 [00:00\u003C00:00, 7722.13it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   5%|\u258d         | 14/290 [00:00\u003C00:00, 7948.06it/s, Materializing param=model.layers.1.input_layernorm.weight] \rLoading weights:   5%|\u258d         | 14/290 [00:00\u003C00:00, 7790.93it/s, Materializing param=model.layers.1.input_layernorm.weight]\rLoading weights:   5%|\u258c         | 15/290 [00:00\u003C00:00, 7979.02it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  \rLoading weights:   5%|\u258c         | 15/290 [00:00\u003C00:00, 7830.06it/s, Materializing param=model.layers.1.mlp.down_proj.weight]\rLoading weights:   6%|\u258c         | 16/290 [00:00\u003C00:00, 8034.10it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   6%|\u258c         | 16/290 [00:00\u003C00:00, 7905.39it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   6%|\u258c         | 17/290 [00:00\u003C00:00, 8122.01it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  \rLoading weights:   6%|\u258c         | 17/290 [00:00\u003C00:00, 7989.15it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\rLoading weights:   6%|\u258c         | 18/290 [00:00\u003C00:00, 8132.00it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:   6%|\u258c         | 18/290 [00:00\u003C00:00, 8009.49it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:   7%|\u258b         | 19/290 [00:00\u003C00:00, 6699.60it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          \rLoading weights:   7%|\u258b         | 19/290 [00:00\u003C00:00, 6607.94it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]\rLoading weights:   7%|\u258b         | 20/290 [00:00\u003C00:00, 6780.86it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:   7%|\u258b         | 20/290 [00:00\u003C00:00, 6708.20it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:   7%|\u258b         | 21/290 [00:00\u003C00:00, 6880.20it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:   7%|\u258b         | 21/290 [00:00\u003C00:00, 6808.41it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:   8%|\u258a         | 22/290 [00:00\u003C00:00, 6966.76it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  \rLoading weights:   8%|\u258a         | 22/290 [00:00\u003C00:00, 6898.01it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]\rLoading weights:   8%|\u258a         | 23/290 [00:00\u003C00:00, 7055.95it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:   8%|\u258a         | 23/290 [00:00\u003C00:00, 6988.99it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:   8%|\u258a         | 24/290 [00:00\u003C00:00, 7127.61it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  \rLoading weights:   8%|\u258a         | 24/290 [00:00\u003C00:00, 7056.66it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]\rLoading weights:   9%|\u258a         | 25/290 [00:00\u003C00:00, 7191.88it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:   9%|\u258a         | 25/290 [00:00\u003C00:00, 7119.61it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:   9%|\u2589         | 26/290 [00:00\u003C00:00, 7258.51it/s, Materializing param=model.layers.2.input_layernorm.weight] \rLoading weights:   9%|\u2589         | 26/290 [00:00\u003C00:00, 7192.92it/s, Materializing param=model.layers.2.input_layernorm.weight]\rLoading weights:   9%|\u2589         | 27/290 [00:00\u003C00:00, 7303.38it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  \rLoading weights:   9%|\u2589         | 27/290 [00:00\u003C00:00, 7234.79it/s, Materializing param=model.layers.2.mlp.down_proj.weight]\rLoading weights:  10%|\u2589         | 28/290 [00:00\u003C00:00, 7359.81it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  10%|\u2589         | 28/290 [00:00\u003C00:00, 7296.71it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  10%|\u2588         | 29/290 [00:00\u003C00:00, 7426.72it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  \rLoading weights:  10%|\u2588         | 29/290 [00:00\u003C00:00, 7357.54it/s, Materializing param=model.layers.2.mlp.up_proj.weight]\rLoading weights:  10%|\u2588         | 30/290 [00:00\u003C00:00, 7472.04it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  10%|\u2588         | 30/290 [00:00\u003C00:00, 7409.99it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  11%|\u2588         | 31/290 [00:00\u003C00:00, 7530.17it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          \rLoading weights:  11%|\u2588         | 31/290 [00:00\u003C00:00, 7467.46it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]\rLoading weights:  11%|\u2588         | 32/290 [00:00\u003C00:00, 7577.36it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  11%|\u2588         | 32/290 [00:00\u003C00:00, 7518.78it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  11%|\u2588\u258f        | 33/290 [00:00\u003C00:00, 7625.17it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  11%|\u2588\u258f        | 33/290 [00:00\u003C00:00, 7565.98it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 34/290 [00:00\u003C00:00, 7667.83it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  \rLoading weights:  12%|\u2588\u258f        | 34/290 [00:00\u003C00:00, 7609.73it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]\rLoading weights:  12%|\u2588\u258f        | 35/290 [00:00\u003C00:00, 7709.31it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 35/290 [00:00\u003C00:00, 7652.64it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 36/290 [00:00\u003C00:00, 7740.95it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  \rLoading weights:  12%|\u2588\u258f        | 36/290 [00:00\u003C00:00, 7685.39it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]\rLoading weights:  13%|\u2588\u258e        | 37/290 [00:00\u003C00:00, 7780.86it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  13%|\u2588\u258e        | 37/290 [00:00\u003C00:00, 7725.47it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  13%|\u2588\u258e        | 38/290 [00:00\u003C00:00, 7814.07it/s, Materializing param=model.layers.3.input_layernorm.weight] \rLoading weights:  13%|\u2588\u258e        | 38/290 [00:00\u003C00:00, 7761.18it/s, Materializing param=model.layers.3.input_layernorm.weight]\rLoading weights:  13%|\u2588\u258e        | 39/290 [00:00\u003C00:00, 7848.10it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \rLoading weights:  13%|\u2588\u258e        | 39/290 [00:00\u003C00:00, 7792.39it/s, Materializing param=model.layers.3.mlp.down_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 40/290 [00:00\u003C00:00, 7514.32it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 40/290 [00:00\u003C00:00, 7463.17it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 41/290 [00:00\u003C00:00, 7246.19it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  \rLoading weights:  14%|\u2588\u258d        | 41/290 [00:00\u003C00:00, 7197.96it/s, Materializing param=model.layers.3.mlp.up_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 42/290 [00:00\u003C00:00, 7280.27it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  14%|\u2588\u258d        | 42/290 [00:00\u003C00:00, 7236.61it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  15%|\u2588\u258d        | 43/290 [00:00\u003C00:00, 7192.05it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          \rLoading weights:  15%|\u2588\u258d        | 43/290 [00:00\u003C00:00, 7148.72it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]\rLoading weights:  15%|\u2588\u258c        | 44/290 [00:00\u003C00:00, 7227.59it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  15%|\u2588\u258c        | 44/290 [00:00\u003C00:00, 7187.06it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 45/290 [00:00\u003C00:00, 7267.20it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 45/290 [00:00\u003C00:00, 7226.30it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 46/290 [00:00\u003C00:00, 6703.43it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  \rLoading weights:  16%|\u2588\u258c        | 46/290 [00:00\u003C00:00, 6664.52it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]\rLoading weights:  16%|\u2588\u258c        | 47/290 [00:00\u003C00:00, 6734.27it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 47/290 [00:00\u003C00:00, 6700.85it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 48/290 [00:00\u003C00:00, 6631.97it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  \rLoading weights:  17%|\u2588\u258b        | 48/290 [00:00\u003C00:00, 6598.28it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]\rLoading weights:  17%|\u2588\u258b        | 49/290 [00:00\u003C00:00, 6666.05it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 49/290 [00:00\u003C00:00, 6634.84it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 50/290 [00:00\u003C00:00, 6599.38it/s, Materializing param=model.layers.4.input_layernorm.weight] \rLoading weights:  17%|\u2588\u258b        | 50/290 [00:00\u003C00:00, 6566.94it/s, Materializing param=model.layers.4.input_layernorm.weight]\rLoading weights:  18%|\u2588\u258a        | 51/290 [00:00\u003C00:00, 6628.54it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  \rLoading weights:  18%|\u2588\u258a        | 51/290 [00:00\u003C00:00, 6600.11it/s, Materializing param=model.layers.4.mlp.down_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 52/290 [00:00\u003C00:00, 6667.19it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 52/290 [00:00\u003C00:00, 6638.58it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 53/290 [00:00\u003C00:00, 6707.44it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  \rLoading weights:  18%|\u2588\u258a        | 53/290 [00:00\u003C00:00, 6679.43it/s, Materializing param=model.layers.4.mlp.up_proj.weight]\rLoading weights:  19%|\u2588\u258a        | 54/290 [00:00\u003C00:00, 6744.26it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  19%|\u2588\u258a        | 54/290 [00:00\u003C00:00, 6715.66it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  19%|\u2588\u2589        | 55/290 [00:00\u003C00:00, 6781.91it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          \rLoading weights:  19%|\u2588\u2589        | 55/290 [00:00\u003C00:00, 6755.50it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]\rLoading weights:  19%|\u2588\u2589        | 56/290 [00:00\u003C00:00, 6816.44it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  19%|\u2588\u2589        | 56/290 [00:00\u003C00:00, 6789.45it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  20%|\u2588\u2589        | 57/290 [00:00\u003C00:00, 6852.85it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  20%|\u2588\u2589        | 57/290 [00:00\u003C00:00, 6825.07it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  20%|\u2588\u2588        | 58/290 [00:00\u003C00:00, 6884.66it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  \rLoading weights:  20%|\u2588\u2588        | 58/290 [00:00\u003C00:00, 6856.53it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]\rLoading weights:  20%|\u2588\u2588        | 59/290 [00:00\u003C00:00, 6916.46it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  20%|\u2588\u2588        | 59/290 [00:00\u003C00:00, 6888.73it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  21%|\u2588\u2588        | 60/290 [00:00\u003C00:00, 6732.97it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  \rLoading weights:  21%|\u2588\u2588        | 60/290 [00:00\u003C00:00, 6703.91it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]\rLoading weights:  21%|\u2588\u2588        | 61/290 [00:00\u003C00:00, 6756.07it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  21%|\u2588\u2588        | 61/290 [00:00\u003C00:00, 6729.60it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  21%|\u2588\u2588\u258f       | 62/290 [00:00\u003C00:00, 6785.84it/s, Materializing param=model.layers.5.input_layernorm.weight] \rLoading weights:  21%|\u2588\u2588\u258f       | 62/290 [00:00\u003C00:00, 6760.61it/s, Materializing param=model.layers.5.input_layernorm.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 63/290 [00:00\u003C00:00, 6563.37it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  \rLoading weights:  22%|\u2588\u2588\u258f       | 63/290 [00:00\u003C00:00, 6536.58it/s, Materializing param=model.layers.5.mlp.down_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 64/290 [00:00\u003C00:00, 6585.11it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 64/290 [00:00\u003C00:00, 6561.45it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 65/290 [00:00\u003C00:00, 6614.50it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \rLoading weights:  22%|\u2588\u2588\u258f       | 65/290 [00:00\u003C00:00, 6590.51it/s, Materializing param=model.layers.5.mlp.up_proj.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 66/290 [00:00\u003C00:00, 6643.56it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 66/290 [00:00\u003C00:00, 6621.16it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 67/290 [00:00\u003C00:00, 6675.97it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          \rLoading weights:  23%|\u2588\u2588\u258e       | 67/290 [00:00\u003C00:00, 6654.31it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]\rLoading weights:  23%|\u2588\u2588\u258e       | 68/290 [00:00\u003C00:00, 6629.00it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 68/290 [00:00\u003C00:00, 6605.82it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 69/290 [00:00\u003C00:00, 6658.39it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 69/290 [00:00\u003C00:00, 6637.17it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 70/290 [00:00\u003C00:00, 6565.03it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  \rLoading weights:  24%|\u2588\u2588\u258d       | 70/290 [00:00\u003C00:00, 6543.52it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]\rLoading weights:  24%|\u2588\u2588\u258d       | 71/290 [00:00\u003C00:00, 6593.80it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 71/290 [00:00\u003C00:00, 6572.69it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  25%|\u2588\u2588\u258d       | 72/290 [00:00\u003C00:00, 6600.15it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  \rLoading weights:  25%|\u2588\u2588\u258d       | 72/290 [00:00\u003C00:00, 6579.01it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]\rLoading weights:  25%|\u2588\u2588\u258c       | 73/290 [00:00\u003C00:00, 6561.75it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  25%|\u2588\u2588\u258c       | 73/290 [00:00\u003C00:00, 6539.74it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 74/290 [00:00\u003C00:00, 6407.22it/s, Materializing param=model.layers.6.input_layernorm.weight] \rLoading weights:  26%|\u2588\u2588\u258c       | 74/290 [00:00\u003C00:00, 6387.97it/s, Materializing param=model.layers.6.input_layernorm.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 75/290 [00:00\u003C00:00, 6428.64it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  \rLoading weights:  26%|\u2588\u2588\u258c       | 75/290 [00:00\u003C00:00, 6409.52it/s, Materializing param=model.layers.6.mlp.down_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 76/290 [00:00\u003C00:00, 6456.17it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 76/290 [00:00\u003C00:00, 6438.31it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 77/290 [00:00\u003C00:00, 6482.83it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  \rLoading weights:  27%|\u2588\u2588\u258b       | 77/290 [00:00\u003C00:00, 6464.27it/s, Materializing param=model.layers.6.mlp.up_proj.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 78/290 [00:00\u003C00:00, 6511.73it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 78/290 [00:00\u003C00:00, 6493.76it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 79/290 [00:00\u003C00:00, 6539.11it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          \rLoading weights:  27%|\u2588\u2588\u258b       | 79/290 [00:00\u003C00:00, 6521.48it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]\rLoading weights:  28%|\u2588\u2588\u258a       | 80/290 [00:00\u003C00:00, 6562.96it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 80/290 [00:00\u003C00:00, 6545.67it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 81/290 [00:00\u003C00:00, 6357.15it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 81/290 [00:00\u003C00:00, 6339.00it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 82/290 [00:00\u003C00:00, 6380.71it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  \rLoading weights:  28%|\u2588\u2588\u258a       | 82/290 [00:00\u003C00:00, 6364.06it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]\rLoading weights:  29%|\u2588\u2588\u258a       | 83/290 [00:00\u003C00:00, 6406.94it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  29%|\u2588\u2588\u258a       | 83/290 [00:00\u003C00:00, 6390.35it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  29%|\u2588\u2588\u2589       | 84/290 [00:00\u003C00:00, 6420.91it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  \rLoading weights:  29%|\u2588\u2588\u2589       | 84/290 [00:00\u003C00:00, 6404.45it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]\rLoading weights:  29%|\u2588\u2588\u2589       | 85/290 [00:00\u003C00:00, 6446.13it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  29%|\u2588\u2588\u2589       | 85/290 [00:00\u003C00:00, 6429.97it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2589       | 86/290 [00:00\u003C00:00, 6470.95it/s, Materializing param=model.layers.7.input_layernorm.weight] \rLoading weights:  30%|\u2588\u2588\u2589       | 86/290 [00:00\u003C00:00, 6454.74it/s, Materializing param=model.layers.7.input_layernorm.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 87/290 [00:00\u003C00:00, 6493.65it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  \rLoading weights:  30%|\u2588\u2588\u2588       | 87/290 [00:00\u003C00:00, 6477.17it/s, Materializing param=model.layers.7.mlp.down_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 88/290 [00:00\u003C00:00, 6517.49it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 88/290 [00:00\u003C00:00, 6501.65it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 89/290 [00:00\u003C00:00, 6388.50it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  \rLoading weights:  31%|\u2588\u2588\u2588       | 89/290 [00:00\u003C00:00, 6369.97it/s, Materializing param=model.layers.7.mlp.up_proj.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 90/290 [00:00\u003C00:00, 6406.34it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 90/290 [00:00\u003C00:00, 6390.73it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  31%|\u2588\u2588\u2588\u258f      | 91/290 [00:00\u003C00:00, 6428.21it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          \rLoading weights:  31%|\u2588\u2588\u2588\u258f      | 91/290 [00:00\u003C00:00, 6412.02it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 92/290 [00:00\u003C00:00, 6449.97it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 92/290 [00:00\u003C00:00, 6435.45it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 93/290 [00:00\u003C00:00, 6474.95it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 93/290 [00:00\u003C00:00, 6460.58it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 94/290 [00:00\u003C00:00, 6498.62it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  \rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 94/290 [00:00\u003C00:00, 6483.65it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 95/290 [00:00\u003C00:00, 6413.31it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 95/290 [00:00\u003C00:00, 6396.84it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 96/290 [00:00\u003C00:00, 6429.49it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  \rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 96/290 [00:00\u003C00:00, 6415.25it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 97/290 [00:00\u003C00:00, 6451.14it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 97/290 [00:00\u003C00:00, 6437.15it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 98/290 [00:00\u003C00:00, 6473.10it/s, Materializing param=model.layers.8.input_layernorm.weight] \rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 98/290 [00:00\u003C00:00, 6459.27it/s, Materializing param=model.layers.8.input_layernorm.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 99/290 [00:00\u003C00:00, 6373.93it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  \rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 99/290 [00:00\u003C00:00, 6359.19it/s, Materializing param=model.layers.8.mlp.down_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 100/290 [00:00\u003C00:00, 6393.66it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 100/290 [00:00\u003C00:00, 6378.01it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258d      | 101/290 [00:00\u003C00:00, 6412.14it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  \rLoading weights:  35%|\u2588\u2588\u2588\u258d      | 101/290 [00:00\u003C00:00, 6398.39it/s, Materializing param=model.layers.8.mlp.up_proj.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258c      | 102/290 [00:00\u003C00:00, 6433.08it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258c      | 102/290 [00:00\u003C00:00, 6418.99it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 103/290 [00:00\u003C00:00, 6453.06it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          \rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 103/290 [00:00\u003C00:00, 6436.14it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 104/290 [00:00\u003C00:00, 6423.89it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 104/290 [00:00\u003C00:00, 6404.08it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 105/290 [00:00\u003C00:00, 6425.66it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 105/290 [00:00\u003C00:00, 6412.38it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 106/290 [00:00\u003C00:00, 6445.29it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  \rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 106/290 [00:00\u003C00:00, 6432.70it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 107/290 [00:00\u003C00:00, 6404.61it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 107/290 [00:00\u003C00:00, 6391.02it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 108/290 [00:00\u003C00:00, 6423.86it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  \rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 108/290 [00:00\u003C00:00, 6411.95it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 109/290 [00:00\u003C00:00, 6444.68it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 109/290 [00:00\u003C00:00, 6432.08it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 110/290 [00:00\u003C00:00, 6465.43it/s, Materializing param=model.layers.9.input_layernorm.weight] \rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 110/290 [00:00\u003C00:00, 6452.96it/s, Materializing param=model.layers.9.input_layernorm.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 111/290 [00:00\u003C00:00, 6408.81it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  \rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 111/290 [00:00\u003C00:00, 6395.25it/s, Materializing param=model.layers.9.mlp.down_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u258a      | 112/290 [00:00\u003C00:00, 6425.68it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u258a      | 112/290 [00:00\u003C00:00, 6413.66it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 113/290 [00:00\u003C00:00, 6362.00it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  \rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 113/290 [00:00\u003C00:00, 6349.30it/s, Materializing param=model.layers.9.mlp.up_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 114/290 [00:00\u003C00:00, 6379.94it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 114/290 [00:00\u003C00:00, 6367.70it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2589      | 115/290 [00:00\u003C00:00, 6398.68it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          \rLoading weights:  40%|\u2588\u2588\u2588\u2589      | 115/290 [00:00\u003C00:00, 6386.90it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 116/290 [00:00\u003C00:00, 6417.37it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 116/290 [00:00\u003C00:00, 6405.46it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 117/290 [00:00\u003C00:00, 6434.92it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 117/290 [00:00\u003C00:00, 6423.63it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 118/290 [00:00\u003C00:00, 6437.00it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  \rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 118/290 [00:00\u003C00:00, 6424.71it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 119/290 [00:00\u003C00:00, 6352.50it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 119/290 [00:00\u003C00:00, 6336.05it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588\u258f     | 120/290 [00:00\u003C00:00, 6364.01it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  \rLoading weights:  41%|\u2588\u2588\u2588\u2588\u258f     | 120/290 [00:00\u003C00:00, 6352.20it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 121/290 [00:00\u003C00:00, 6241.60it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 121/290 [00:00\u003C00:00, 6227.36it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 122/290 [00:00\u003C00:00, 6206.47it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 122/290 [00:00\u003C00:00, 6194.98it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 123/290 [00:00\u003C00:00, 6223.60it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  \rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 123/290 [00:00\u003C00:00, 6213.86it/s, Materializing param=model.layers.10.mlp.down_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 124/290 [00:00\u003C00:00, 6241.52it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 124/290 [00:00\u003C00:00, 6230.01it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 125/290 [00:00\u003C00:00, 6256.57it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  \rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 125/290 [00:00\u003C00:00, 6246.51it/s, Materializing param=model.layers.10.mlp.up_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 126/290 [00:00\u003C00:00, 6273.53it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 126/290 [00:00\u003C00:00, 6262.23it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 127/290 [00:00\u003C00:00, 6288.61it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          \rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 127/290 [00:00\u003C00:00, 6278.45it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 128/290 [00:00\u003C00:00, 6290.82it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 128/290 [00:00\u003C00:00, 6279.49it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 129/290 [00:00\u003C00:00, 6304.14it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 129/290 [00:00\u003C00:00, 6290.94it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258d     | 130/290 [00:00\u003C00:00, 6257.21it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  \rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258d     | 130/290 [00:00\u003C00:00, 6247.10it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258c     | 131/290 [00:00\u003C00:00, 6273.23it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258c     | 131/290 [00:00\u003C00:00, 6263.30it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 132/290 [00:00\u003C00:00, 6290.38it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  \rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 132/290 [00:00\u003C00:00, 6280.39it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 133/290 [00:00\u003C00:00, 6306.08it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 133/290 [00:00\u003C00:00, 6296.12it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 134/290 [00:00\u003C00:00, 6322.05it/s, Materializing param=model.layers.11.input_layernorm.weight] \rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 134/290 [00:00\u003C00:00, 6311.69it/s, Materializing param=model.layers.11.input_layernorm.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 135/290 [00:00\u003C00:00, 6325.19it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  \rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 135/290 [00:00\u003C00:00, 6314.89it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 136/290 [00:00\u003C00:00, 6338.62it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 136/290 [00:00\u003C00:00, 6328.85it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 137/290 [00:00\u003C00:00, 6243.69it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 137/290 [00:00\u003C00:00, 6232.99it/s, Materializing param=model.layers.11.mlp.up_proj.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 138/290 [00:00\u003C00:00, 6255.76it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 138/290 [00:00\u003C00:00, 6243.34it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 139/290 [00:00\u003C00:00, 6243.80it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          \rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 139/290 [00:00\u003C00:00, 6233.72it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 140/290 [00:00\u003C00:00, 6256.82it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 140/290 [00:00\u003C00:00, 6247.30it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u258a     | 141/290 [00:00\u003C00:00, 6272.37it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u258a     | 141/290 [00:00\u003C00:00, 6262.94it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 142/290 [00:00\u003C00:00, 6286.59it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  \rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 142/290 [00:00\u003C00:00, 6277.84it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 143/290 [00:00\u003C00:00, 6301.33it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 143/290 [00:00\u003C00:00, 6292.47it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2589     | 144/290 [00:00\u003C00:00, 6317.32it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  \rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2589     | 144/290 [00:00\u003C00:00, 6308.15it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 145/290 [00:00\u003C00:00, 6331.85it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 145/290 [00:00\u003C00:00, 6322.83it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 146/290 [00:00\u003C00:00, 6200.63it/s, Materializing param=model.layers.12.input_layernorm.weight] \rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 146/290 [00:00\u003C00:00, 6191.17it/s, Materializing param=model.layers.12.input_layernorm.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 147/290 [00:00\u003C00:00, 6207.78it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  \rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 147/290 [00:00\u003C00:00, 6198.98it/s, Materializing param=model.layers.12.mlp.down_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 148/290 [00:00\u003C00:00, 6222.13it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 148/290 [00:00\u003C00:00, 6213.41it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 149/290 [00:00\u003C00:00, 6235.98it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  \rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 149/290 [00:00\u003C00:00, 6227.59it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 150/290 [00:00\u003C00:00, 6250.64it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 150/290 [00:00\u003C00:00, 6242.08it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 151/290 [00:00\u003C00:00, 6161.07it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          \rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 151/290 [00:00\u003C00:00, 6152.04it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 152/290 [00:00\u003C00:00, 6174.55it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 152/290 [00:00\u003C00:00, 6164.52it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 153/290 [00:00\u003C00:00, 6134.54it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 153/290 [00:00\u003C00:00, 6125.88it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 154/290 [00:00\u003C00:00, 6148.02it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  \rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 154/290 [00:00\u003C00:00, 6139.60it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 155/290 [00:00\u003C00:00, 6161.84it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 155/290 [00:00\u003C00:00, 6153.44it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 156/290 [00:00\u003C00:00, 6175.66it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  \rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 156/290 [00:00\u003C00:00, 6167.28it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 157/290 [00:00\u003C00:00, 6187.80it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 157/290 [00:00\u003C00:00, 6179.61it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 158/290 [00:00\u003C00:00, 6201.69it/s, Materializing param=model.layers.13.input_layernorm.weight] \rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 158/290 [00:00\u003C00:00, 6193.86it/s, Materializing param=model.layers.13.input_layernorm.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 159/290 [00:00\u003C00:00, 6215.52it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  \rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 159/290 [00:00\u003C00:00, 6207.25it/s, Materializing param=model.layers.13.mlp.down_proj.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 160/290 [00:00\u003C00:00, 6137.01it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 160/290 [00:00\u003C00:00, 6127.66it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 161/290 [00:00\u003C00:00, 6148.83it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  \rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 161/290 [00:00\u003C00:00, 6141.11it/s, Materializing param=model.layers.13.mlp.up_proj.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 162/290 [00:00\u003C00:00, 6161.83it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 162/290 [00:00\u003C00:00, 6154.29it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 163/290 [00:00\u003C00:00, 6175.11it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          \rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 163/290 [00:00\u003C00:00, 6167.20it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 164/290 [00:00\u003C00:00, 6188.01it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 164/290 [00:00\u003C00:00, 6180.29it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 165/290 [00:00\u003C00:00, 6143.24it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 165/290 [00:00\u003C00:00, 6135.07it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 166/290 [00:00\u003C00:00, 6004.42it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  \rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 166/290 [00:00\u003C00:00, 5995.99it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 167/290 [00:00\u003C00:00, 6015.59it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 167/290 [00:00\u003C00:00, 6008.26it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 168/290 [00:00\u003C00:00, 6027.90it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  \rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 168/290 [00:00\u003C00:00, 6020.69it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 169/290 [00:00\u003C00:00, 6040.88it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 169/290 [00:00\u003C00:00, 6033.89it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 170/290 [00:00\u003C00:00, 6054.08it/s, Materializing param=model.layers.14.input_layernorm.weight] \rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 170/290 [00:00\u003C00:00, 6047.05it/s, Materializing param=model.layers.14.input_layernorm.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 171/290 [00:00\u003C00:00, 6067.23it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  \rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 171/290 [00:00\u003C00:00, 6060.16it/s, Materializing param=model.layers.14.mlp.down_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 172/290 [00:00\u003C00:00, 6080.14it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 172/290 [00:00\u003C00:00, 6073.02it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 173/290 [00:00\u003C00:00, 6090.69it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  \rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 173/290 [00:00\u003C00:00, 6083.49it/s, Materializing param=model.layers.14.mlp.up_proj.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 174/290 [00:00\u003C00:00, 6103.71it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 174/290 [00:00\u003C00:00, 6096.12it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 175/290 [00:00\u003C00:00, 6097.44it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          \rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 175/290 [00:00\u003C00:00, 6090.00it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 176/290 [00:00\u003C00:00, 6012.07it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 176/290 [00:00\u003C00:00, 6002.39it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 177/290 [00:00\u003C00:00, 6021.17it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 177/290 [00:00\u003C00:00, 6014.14it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 178/290 [00:00\u003C00:00, 6016.34it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  \rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 178/290 [00:00\u003C00:00, 6009.18it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 179/290 [00:00\u003C00:00, 6027.46it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 179/290 [00:00\u003C00:00, 6020.69it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 180/290 [00:00\u003C00:00, 6039.80it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  \rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 180/290 [00:00\u003C00:00, 6032.99it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 181/290 [00:00\u003C00:00, 6005.47it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 181/290 [00:00\u003C00:00, 5998.30it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 182/290 [00:00\u003C00:00, 6016.32it/s, Materializing param=model.layers.15.input_layernorm.weight] \rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 182/290 [00:00\u003C00:00, 6009.65it/s, Materializing param=model.layers.15.input_layernorm.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 183/290 [00:00\u003C00:00, 6027.96it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 183/290 [00:00\u003C00:00, 6021.29it/s, Materializing param=model.layers.15.mlp.down_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 184/290 [00:00\u003C00:00, 6039.55it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 184/290 [00:00\u003C00:00, 6032.94it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 185/290 [00:00\u003C00:00, 6021.58it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  \rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 185/290 [00:00\u003C00:00, 6014.99it/s, Materializing param=model.layers.15.mlp.up_proj.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 186/290 [00:00\u003C00:00, 6033.20it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 186/290 [00:00\u003C00:00, 6026.44it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 187/290 [00:00\u003C00:00, 6044.74it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          \rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 187/290 [00:00\u003C00:00, 6038.13it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188/290 [00:00\u003C00:00, 6056.11it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188/290 [00:00\u003C00:00, 6049.42it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 189/290 [00:00\u003C00:00, 6068.19it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 189/290 [00:00\u003C00:00, 6061.50it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 190/290 [00:00\u003C00:00, 6080.09it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  \rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 190/290 [00:00\u003C00:00, 6073.56it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 191/290 [00:00\u003C00:00, 6092.01it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 191/290 [00:00\u003C00:00, 6085.35it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 192/290 [00:00\u003C00:00, 6103.21it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  \rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 192/290 [00:00\u003C00:00, 6096.70it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 193/290 [00:00\u003C00:00, 6055.37it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 193/290 [00:00\u003C00:00, 6048.54it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 194/290 [00:00\u003C00:00, 6034.66it/s, Materializing param=model.layers.16.input_layernorm.weight] \rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 194/290 [00:00\u003C00:00, 6028.00it/s, Materializing param=model.layers.16.input_layernorm.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 195/290 [00:00\u003C00:00, 6045.18it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  \rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 195/290 [00:00\u003C00:00, 6038.71it/s, Materializing param=model.layers.16.mlp.down_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 196/290 [00:00\u003C00:00, 6051.90it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 196/290 [00:00\u003C00:00, 6045.31it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 197/290 [00:00\u003C00:00, 6062.65it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  \rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 197/290 [00:00\u003C00:00, 6056.47it/s, Materializing param=model.layers.16.mlp.up_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 198/290 [00:00\u003C00:00, 6073.81it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 198/290 [00:00\u003C00:00, 6067.51it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 199/290 [00:00\u003C00:00, 6082.38it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          \rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 199/290 [00:00\u003C00:00, 6075.87it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 200/290 [00:00\u003C00:00, 6047.85it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 200/290 [00:00\u003C00:00, 6041.01it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 201/290 [00:00\u003C00:00, 6038.34it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 201/290 [00:00\u003C00:00, 6031.99it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 202/290 [00:00\u003C00:00, 6049.32it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  \rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 202/290 [00:00\u003C00:00, 6043.19it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 203/290 [00:00\u003C00:00, 6059.80it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 203/290 [00:00\u003C00:00, 6053.68it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 204/290 [00:00\u003C00:00, 6070.85it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  \rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 204/290 [00:00\u003C00:00, 6065.04it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 205/290 [00:00\u003C00:00, 6011.80it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 205/290 [00:00\u003C00:00, 6005.59it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 206/290 [00:00\u003C00:00, 6022.14it/s, Materializing param=model.layers.17.input_layernorm.weight] \rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 206/290 [00:00\u003C00:00, 6016.44it/s, Materializing param=model.layers.17.input_layernorm.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 207/290 [00:00\u003C00:00, 6032.91it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  \rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 207/290 [00:00\u003C00:00, 6027.30it/s, Materializing param=model.layers.17.mlp.down_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 208/290 [00:00\u003C00:00, 6043.96it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 208/290 [00:00\u003C00:00, 6038.14it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 209/290 [00:00\u003C00:00, 5959.56it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  \rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 209/290 [00:00\u003C00:00, 5951.83it/s, Materializing param=model.layers.17.mlp.up_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 210/290 [00:00\u003C00:00, 5968.03it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 210/290 [00:00\u003C00:00, 5962.17it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 211/290 [00:00\u003C00:00, 5978.14it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          \rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 211/290 [00:00\u003C00:00, 5972.25it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 212/290 [00:00\u003C00:00, 5987.99it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 212/290 [00:00\u003C00:00, 5981.83it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 213/290 [00:00\u003C00:00, 5982.67it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 213/290 [00:00\u003C00:00, 5976.55it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 214/290 [00:00\u003C00:00, 5990.98it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  \rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 214/290 [00:00\u003C00:00, 5985.27it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 215/290 [00:00\u003C00:00, 6000.67it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 215/290 [00:00\u003C00:00, 5994.69it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 216/290 [00:00\u003C00:00, 5998.37it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  \rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 216/290 [00:00\u003C00:00, 5992.22it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 217/290 [00:00\u003C00:00, 6007.60it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 217/290 [00:00\u003C00:00, 6001.66it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 218/290 [00:00\u003C00:00, 6017.42it/s, Materializing param=model.layers.18.input_layernorm.weight] \rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 218/290 [00:00\u003C00:00, 6011.72it/s, Materializing param=model.layers.18.input_layernorm.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 219/290 [00:00\u003C00:00, 6027.17it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  \rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 219/290 [00:00\u003C00:00, 6020.73it/s, Materializing param=model.layers.18.mlp.down_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 220/290 [00:00\u003C00:00, 6035.84it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 220/290 [00:00\u003C00:00, 6030.32it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 221/290 [00:00\u003C00:00, 5985.48it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  \rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 221/290 [00:00\u003C00:00, 5978.61it/s, Materializing param=model.layers.18.mlp.up_proj.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 222/290 [00:00\u003C00:00, 5987.32it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 222/290 [00:00\u003C00:00, 5981.55it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 223/290 [00:00\u003C00:00, 5996.47it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          \rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 223/290 [00:00\u003C00:00, 5990.90it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 224/290 [00:00\u003C00:00, 6006.15it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 224/290 [00:00\u003C00:00, 6000.55it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 225/290 [00:00\u003C00:00, 6015.31it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 225/290 [00:00\u003C00:00, 6009.84it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 226/290 [00:00\u003C00:00, 6024.69it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  \rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 226/290 [00:00\u003C00:00, 6019.60it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 227/290 [00:00\u003C00:00, 6034.63it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 227/290 [00:00\u003C00:00, 6028.32it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 228/290 [00:00\u003C00:00, 6042.94it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  \rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 228/290 [00:00\u003C00:00, 6037.75it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 229/290 [00:00\u003C00:00, 6052.73it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 229/290 [00:00\u003C00:00, 6047.17it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 230/290 [00:00\u003C00:00, 6035.95it/s, Materializing param=model.layers.19.input_layernorm.weight] \rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 230/290 [00:00\u003C00:00, 6029.73it/s, Materializing param=model.layers.19.input_layernorm.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 231/290 [00:00\u003C00:00, 6019.00it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  \rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 231/290 [00:00\u003C00:00, 6013.17it/s, Materializing param=model.layers.19.mlp.down_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 232/290 [00:00\u003C00:00, 6027.72it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 232/290 [00:00\u003C00:00, 6022.53it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 233/290 [00:00\u003C00:00, 6037.06it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  \rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 233/290 [00:00\u003C00:00, 6031.77it/s, Materializing param=model.layers.19.mlp.up_proj.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 234/290 [00:00\u003C00:00, 6045.08it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 234/290 [00:00\u003C00:00, 6039.39it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 235/290 [00:00\u003C00:00, 6035.60it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          \rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 235/290 [00:00\u003C00:00, 6030.32it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 236/290 [00:00\u003C00:00, 6038.76it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 236/290 [00:00\u003C00:00, 6033.57it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 237/290 [00:00\u003C00:00, 6048.12it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 237/290 [00:00\u003C00:00, 6043.26it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 238/290 [00:00\u003C00:00, 5996.04it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  \rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 238/290 [00:00\u003C00:00, 5988.16it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 239/290 [00:00\u003C00:00, 6000.72it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 239/290 [00:00\u003C00:00, 5995.41it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 240/290 [00:00\u003C00:00, 6009.21it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  \rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 240/290 [00:00\u003C00:00, 6002.69it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 241/290 [00:00\u003C00:00, 6016.69it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 241/290 [00:00\u003C00:00, 6011.50it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 242/290 [00:00\u003C00:00, 6025.30it/s, Materializing param=model.layers.20.input_layernorm.weight] \rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 242/290 [00:00\u003C00:00, 6020.37it/s, Materializing param=model.layers.20.input_layernorm.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 243/290 [00:00\u003C00:00, 6034.72it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  \rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 243/290 [00:00\u003C00:00, 6029.94it/s, Materializing param=model.layers.20.mlp.down_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 244/290 [00:00\u003C00:00, 6044.09it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 244/290 [00:00\u003C00:00, 6039.17it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 245/290 [00:00\u003C00:00, 6053.53it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  \rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 245/290 [00:00\u003C00:00, 6048.72it/s, Materializing param=model.layers.20.mlp.up_proj.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 246/290 [00:00\u003C00:00, 6063.13it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 246/290 [00:00\u003C00:00, 6058.07it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 247/290 [00:00\u003C00:00, 6044.69it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          \rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 247/290 [00:00\u003C00:00, 6039.54it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 248/290 [00:00\u003C00:00, 6052.35it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 248/290 [00:00\u003C00:00, 6047.53it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 249/290 [00:00\u003C00:00, 6024.91it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 249/290 [00:00\u003C00:00, 6019.73it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 250/290 [00:00\u003C00:00, 5975.71it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  \rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 250/290 [00:00\u003C00:00, 5970.44it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 251/290 [00:00\u003C00:00, 5983.72it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 251/290 [00:00\u003C00:00, 5978.90it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 252/290 [00:00\u003C00:00, 5991.93it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  \rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 252/290 [00:00\u003C00:00, 5987.21it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 253/290 [00:00\u003C00:00, 6000.54it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 253/290 [00:00\u003C00:00, 5995.82it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 254/290 [00:00\u003C00:00, 6009.95it/s, Materializing param=model.layers.21.input_layernorm.weight] \rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 254/290 [00:00\u003C00:00, 6005.31it/s, Materializing param=model.layers.21.input_layernorm.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 255/290 [00:00\u003C00:00, 6018.63it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  \rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 255/290 [00:00\u003C00:00, 6014.03it/s, Materializing param=model.layers.21.mlp.down_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 256/290 [00:00\u003C00:00, 6027.45it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 256/290 [00:00\u003C00:00, 6023.05it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 257/290 [00:00\u003C00:00, 6036.32it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  \rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 257/290 [00:00\u003C00:00, 6031.32it/s, Materializing param=model.layers.21.mlp.up_proj.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 258/290 [00:00\u003C00:00, 6043.43it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 258/290 [00:00\u003C00:00, 6038.71it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 259/290 [00:00\u003C00:00, 6000.07it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          \rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 259/290 [00:00\u003C00:00, 5994.94it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 260/290 [00:00\u003C00:00, 6007.61it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 260/290 [00:00\u003C00:00, 6002.95it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 261/290 [00:00\u003C00:00, 6016.16it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 261/290 [00:00\u003C00:00, 6011.64it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 262/290 [00:00\u003C00:00, 6024.12it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  \rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 262/290 [00:00\u003C00:00, 6019.50it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 263/290 [00:00\u003C00:00, 6032.53it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 263/290 [00:00\u003C00:00, 6028.14it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 264/290 [00:00\u003C00:00, 6038.79it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  \rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 264/290 [00:00\u003C00:00, 6034.11it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 265/290 [00:00\u003C00:00, 5996.49it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 265/290 [00:00\u003C00:00, 5991.51it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 266/290 [00:00\u003C00:00, 6004.05it/s, Materializing param=model.layers.22.input_layernorm.weight] \rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 266/290 [00:00\u003C00:00, 5999.56it/s, Materializing param=model.layers.22.input_layernorm.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 267/290 [00:00\u003C00:00, 6012.55it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  \rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 267/290 [00:00\u003C00:00, 6007.94it/s, Materializing param=model.layers.22.mlp.down_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 268/290 [00:00\u003C00:00, 6020.52it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 268/290 [00:00\u003C00:00, 6015.95it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 269/290 [00:00\u003C00:00, 6028.49it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  \rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 269/290 [00:00\u003C00:00, 6023.60it/s, Materializing param=model.layers.22.mlp.up_proj.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 270/290 [00:00\u003C00:00, 5985.62it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 270/290 [00:00\u003C00:00, 5980.53it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 271/290 [00:00\u003C00:00, 5993.00it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          \rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 271/290 [00:00\u003C00:00, 5988.49it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 272/290 [00:00\u003C00:00, 5987.90it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 272/290 [00:00\u003C00:00, 5982.94it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 273/290 [00:00\u003C00:00, 5994.28it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 273/290 [00:00\u003C00:00, 5989.98it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 274/290 [00:00\u003C00:00, 6001.91it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  \rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 274/290 [00:00\u003C00:00, 5997.59it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 275/290 [00:00\u003C00:00, 6000.50it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 275/290 [00:00\u003C00:00, 5995.73it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 276/290 [00:00\u003C00:00, 5974.33it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  \rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 276/290 [00:00\u003C00:00, 5969.68it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 277/290 [00:00\u003C00:00, 5980.33it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 277/290 [00:00\u003C00:00, 5976.08it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 278/290 [00:00\u003C00:00, 5988.60it/s, Materializing param=model.layers.23.input_layernorm.weight] \rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 278/290 [00:00\u003C00:00, 5984.42it/s, Materializing param=model.layers.23.input_layernorm.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 279/290 [00:00\u003C00:00, 5996.68it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  \rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 279/290 [00:00\u003C00:00, 5992.32it/s, Materializing param=model.layers.23.mlp.down_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 280/290 [00:00\u003C00:00, 6004.30it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 280/290 [00:00\u003C00:00, 5999.36it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 281/290 [00:00\u003C00:00, 6011.73it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  \rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 281/290 [00:00\u003C00:00, 6007.59it/s, Materializing param=model.layers.23.mlp.up_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 282/290 [00:00\u003C00:00, 5991.10it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 282/290 [00:00\u003C00:00, 5986.25it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 283/290 [00:00\u003C00:00, 5998.19it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          \rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 283/290 [00:00\u003C00:00, 5993.98it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 284/290 [00:00\u003C00:00, 6003.07it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 284/290 [00:00\u003C00:00, 5998.77it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 285/290 [00:00\u003C00:00, 6010.63it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 285/290 [00:00\u003C00:00, 6006.56it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 286/290 [00:00\u003C00:00, 6018.38it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  \rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 286/290 [00:00\u003C00:00, 6013.85it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 287/290 [00:00\u003C00:00, 6006.60it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 287/290 [00:00\u003C00:00, 6002.32it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 288/290 [00:00\u003C00:00, 6015.53it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  \rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 288/290 [00:00\u003C00:00, 6011.66it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 289/290 [00:00\u003C00:00, 6025.28it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 289/290 [00:00\u003C00:00, 6021.51it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 6035.33it/s, Materializing param=model.norm.weight]                      \rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 6031.59it/s, Materializing param=model.norm.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 6020.90it/s, Materializing param=model.norm.weight]\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "[GeneratorWorker:1] Ready on GPU 2!\n[GeneratorWorker:0] Ready on GPU 1!\n[SETUP] Setting up trainer...\n[Trainer:0] Loading model Qwen/Qwen2.5-0.5B-Instruct on GPU 0...\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stderr", "text": "[actor=\u003Croot\u003E.\u003C__main__.TrainerActor trainer{'procs': 0/1}\u003E] `torch_dtype` is deprecated! Use `dtype` instead!\n\rLoading weights:   0%|          | 0/290 [00:00\u003C?, ?it/s]\rLoading weights:   0%|          | 1/290 [00:00\u003C00:00, 18724.57it/s, Materializing param=model.embed_tokens.weight]\rLoading weights:   0%|          | 1/290 [00:00\u003C00:00, 7943.76it/s, Materializing param=model.embed_tokens.weight] \rLoading weights:   1%|          | 2/290 [00:00\u003C00:00, 7013.89it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|          | 2/290 [00:00\u003C00:00, 6039.31it/s, Materializing param=model.layers.0.input_layernorm.weight]\rLoading weights:   1%|          | 3/290 [00:00\u003C00:00, 6838.54it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  \rLoading weights:   1%|          | 3/290 [00:00\u003C00:00, 6216.85it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\rLoading weights:   1%|\u258f         | 4/290 [00:00\u003C00:00, 6970.18it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   1%|\u258f         | 4/290 [00:00\u003C00:00, 6470.20it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\rLoading weights:   2%|\u258f         | 5/290 [00:00\u003C00:00, 7080.19it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \rLoading weights:   2%|\u258f         | 5/290 [00:00\u003C00:00, 6736.76it/s, Materializing param=model.layers.0.mlp.up_proj.weight]\rLoading weights:   2%|\u258f         | 6/290 [00:00\u003C00:00, 7285.99it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   2%|\u258f         | 6/290 [00:00\u003C00:00, 6930.82it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\rLoading weights:   2%|\u258f         | 7/290 [00:00\u003C00:00, 7352.90it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          \rLoading weights:   2%|\u258f         | 7/290 [00:00\u003C00:00, 7086.68it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]\rLoading weights:   3%|\u258e         | 8/290 [00:00\u003C00:00, 7523.42it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|\u258e         | 8/290 [00:00\u003C00:00, 7283.36it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\rLoading weights:   3%|\u258e         | 9/290 [00:00\u003C00:00, 7664.72it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   3%|\u258e         | 9/290 [00:00\u003C00:00, 7451.39it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\rLoading weights:   3%|\u258e         | 10/290 [00:00\u003C00:00, 7803.36it/s, Materializing param=model.layers.0.self_attn.q_proj.bias] \rLoading weights:   3%|\u258e         | 10/290 [00:00\u003C00:00, 7602.51it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]\rLoading weights:   4%|\u258d         | 11/290 [00:00\u003C00:00, 6221.32it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   4%|\u258d         | 11/290 [00:00\u003C00:00, 6083.51it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\rLoading weights:   4%|\u258d         | 12/290 [00:00\u003C00:00, 6325.46it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  \rLoading weights:   4%|\u258d         | 12/290 [00:00\u003C00:00, 6207.65it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]\rLoading weights:   4%|\u258d         | 13/290 [00:00\u003C00:00, 6483.47it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   4%|\u258d         | 13/290 [00:00\u003C00:00, 6365.39it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\rLoading weights:   5%|\u258d         | 14/290 [00:00\u003C00:00, 6605.20it/s, Materializing param=model.layers.1.input_layernorm.weight] \rLoading weights:   5%|\u258d         | 14/290 [00:00\u003C00:00, 6491.30it/s, Materializing param=model.layers.1.input_layernorm.weight]\rLoading weights:   5%|\u258c         | 15/290 [00:00\u003C00:00, 6704.45it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  \rLoading weights:   5%|\u258c         | 15/290 [00:00\u003C00:00, 6601.74it/s, Materializing param=model.layers.1.mlp.down_proj.weight]\rLoading weights:   6%|\u258c         | 16/290 [00:00\u003C00:00, 6820.01it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   6%|\u258c         | 16/290 [00:00\u003C00:00, 6722.99it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\rLoading weights:   6%|\u258c         | 17/290 [00:00\u003C00:00, 6931.39it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  \rLoading weights:   6%|\u258c         | 17/290 [00:00\u003C00:00, 6837.01it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\rLoading weights:   6%|\u258c         | 18/290 [00:00\u003C00:00, 7039.39it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:   6%|\u258c         | 18/290 [00:00\u003C00:00, 6944.85it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\rLoading weights:   7%|\u258b         | 19/290 [00:00\u003C00:00, 7134.45it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          \rLoading weights:   7%|\u258b         | 19/290 [00:00\u003C00:00, 7041.78it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]\rLoading weights:   7%|\u258b         | 20/290 [00:00\u003C00:00, 7222.22it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:   7%|\u258b         | 20/290 [00:00\u003C00:00, 7130.14it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\rLoading weights:   7%|\u258b         | 21/290 [00:00\u003C00:00, 7304.12it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:   7%|\u258b         | 21/290 [00:00\u003C00:00, 7200.82it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\rLoading weights:   8%|\u258a         | 22/290 [00:00\u003C00:00, 7232.69it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  \rLoading weights:   8%|\u258a         | 22/290 [00:00\u003C00:00, 7149.75it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]\rLoading weights:   8%|\u258a         | 23/290 [00:00\u003C00:00, 7307.15it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:   8%|\u258a         | 23/290 [00:00\u003C00:00, 7224.52it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\rLoading weights:   8%|\u258a         | 24/290 [00:00\u003C00:00, 6618.67it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  \rLoading weights:   8%|\u258a         | 24/290 [00:00\u003C00:00, 6547.63it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]\rLoading weights:   9%|\u258a         | 25/290 [00:00\u003C00:00, 6693.32it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:   9%|\u258a         | 25/290 [00:00\u003C00:00, 6631.10it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\rLoading weights:   9%|\u2589         | 26/290 [00:00\u003C00:00, 6767.53it/s, Materializing param=model.layers.2.input_layernorm.weight] \rLoading weights:   9%|\u2589         | 26/290 [00:00\u003C00:00, 6705.11it/s, Materializing param=model.layers.2.input_layernorm.weight]\rLoading weights:   9%|\u2589         | 27/290 [00:00\u003C00:00, 6646.68it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  \rLoading weights:   9%|\u2589         | 27/290 [00:00\u003C00:00, 6589.06it/s, Materializing param=model.layers.2.mlp.down_proj.weight]\rLoading weights:  10%|\u2589         | 28/290 [00:00\u003C00:00, 6710.12it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  10%|\u2589         | 28/290 [00:00\u003C00:00, 6657.63it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\rLoading weights:  10%|\u2588         | 29/290 [00:00\u003C00:00, 6780.85it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  \rLoading weights:  10%|\u2588         | 29/290 [00:00\u003C00:00, 6724.24it/s, Materializing param=model.layers.2.mlp.up_proj.weight]\rLoading weights:  10%|\u2588         | 30/290 [00:00\u003C00:00, 6841.51it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  10%|\u2588         | 30/290 [00:00\u003C00:00, 6789.46it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\rLoading weights:  11%|\u2588         | 31/290 [00:00\u003C00:00, 6909.16it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          \rLoading weights:  11%|\u2588         | 31/290 [00:00\u003C00:00, 6859.58it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]\rLoading weights:  11%|\u2588         | 32/290 [00:00\u003C00:00, 6973.80it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  11%|\u2588         | 32/290 [00:00\u003C00:00, 6924.51it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\rLoading weights:  11%|\u2588\u258f        | 33/290 [00:00\u003C00:00, 7034.20it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  11%|\u2588\u258f        | 33/290 [00:00\u003C00:00, 6982.40it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 34/290 [00:00\u003C00:00, 6911.57it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  \rLoading weights:  12%|\u2588\u258f        | 34/290 [00:00\u003C00:00, 6861.02it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]\rLoading weights:  12%|\u2588\u258f        | 35/290 [00:00\u003C00:00, 6960.35it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 35/290 [00:00\u003C00:00, 6915.10it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\rLoading weights:  12%|\u2588\u258f        | 36/290 [00:00\u003C00:00, 7016.82it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  \rLoading weights:  12%|\u2588\u258f        | 36/290 [00:00\u003C00:00, 6966.96it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]\rLoading weights:  13%|\u2588\u258e        | 37/290 [00:00\u003C00:00, 7066.91it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  13%|\u2588\u258e        | 37/290 [00:00\u003C00:00, 7022.77it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\rLoading weights:  13%|\u2588\u258e        | 38/290 [00:00\u003C00:00, 7119.15it/s, Materializing param=model.layers.3.input_layernorm.weight] \rLoading weights:  13%|\u2588\u258e        | 38/290 [00:00\u003C00:00, 7075.54it/s, Materializing param=model.layers.3.input_layernorm.weight]\rLoading weights:  13%|\u2588\u258e        | 39/290 [00:00\u003C00:00, 7163.16it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \rLoading weights:  13%|\u2588\u258e        | 39/290 [00:00\u003C00:00, 7120.13it/s, Materializing param=model.layers.3.mlp.down_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 40/290 [00:00\u003C00:00, 7212.28it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 40/290 [00:00\u003C00:00, 7169.14it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 41/290 [00:00\u003C00:00, 6564.36it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  \rLoading weights:  14%|\u2588\u258d        | 41/290 [00:00\u003C00:00, 6523.02it/s, Materializing param=model.layers.3.mlp.up_proj.weight]\rLoading weights:  14%|\u2588\u258d        | 42/290 [00:00\u003C00:00, 6602.23it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  14%|\u2588\u258d        | 42/290 [00:00\u003C00:00, 6564.83it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\rLoading weights:  15%|\u2588\u258d        | 43/290 [00:00\u003C00:00, 6646.34it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          \rLoading weights:  15%|\u2588\u258d        | 43/290 [00:00\u003C00:00, 6602.54it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]\rLoading weights:  15%|\u2588\u258c        | 44/290 [00:00\u003C00:00, 6666.76it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  15%|\u2588\u258c        | 44/290 [00:00\u003C00:00, 6632.50it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 45/290 [00:00\u003C00:00, 6709.22it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 45/290 [00:00\u003C00:00, 6674.81it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 46/290 [00:00\u003C00:00, 6600.46it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  \rLoading weights:  16%|\u2588\u258c        | 46/290 [00:00\u003C00:00, 6564.75it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]\rLoading weights:  16%|\u2588\u258c        | 47/290 [00:00\u003C00:00, 6639.24it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  16%|\u2588\u258c        | 47/290 [00:00\u003C00:00, 6605.87it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 48/290 [00:00\u003C00:00, 6661.81it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  \rLoading weights:  17%|\u2588\u258b        | 48/290 [00:00\u003C00:00, 6619.75it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]\rLoading weights:  17%|\u2588\u258b        | 49/290 [00:00\u003C00:00, 6672.54it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 49/290 [00:00\u003C00:00, 6632.70it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\rLoading weights:  17%|\u2588\u258b        | 50/290 [00:00\u003C00:00, 6699.10it/s, Materializing param=model.layers.4.input_layernorm.weight] \rLoading weights:  17%|\u2588\u258b        | 50/290 [00:00\u003C00:00, 6669.69it/s, Materializing param=model.layers.4.input_layernorm.weight]\rLoading weights:  18%|\u2588\u258a        | 51/290 [00:00\u003C00:00, 6627.92it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  \rLoading weights:  18%|\u2588\u258a        | 51/290 [00:00\u003C00:00, 6598.28it/s, Materializing param=model.layers.4.mlp.down_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 52/290 [00:00\u003C00:00, 6667.60it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 52/290 [00:00\u003C00:00, 6639.79it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\rLoading weights:  18%|\u2588\u258a        | 53/290 [00:00\u003C00:00, 6493.68it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  \rLoading weights:  18%|\u2588\u258a        | 53/290 [00:00\u003C00:00, 6463.28it/s, Materializing param=model.layers.4.mlp.up_proj.weight]\rLoading weights:  19%|\u2588\u258a        | 54/290 [00:00\u003C00:00, 6524.71it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  19%|\u2588\u258a        | 54/290 [00:00\u003C00:00, 6497.01it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\rLoading weights:  19%|\u2588\u2589        | 55/290 [00:00\u003C00:00, 6562.55it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          \rLoading weights:  19%|\u2588\u2589        | 55/290 [00:00\u003C00:00, 6536.52it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]\rLoading weights:  19%|\u2588\u2589        | 56/290 [00:00\u003C00:00, 6365.17it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  19%|\u2588\u2589        | 56/290 [00:00\u003C00:00, 6337.86it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\rLoading weights:  20%|\u2588\u2589        | 57/290 [00:00\u003C00:00, 6395.98it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  20%|\u2588\u2589        | 57/290 [00:00\u003C00:00, 6373.30it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\rLoading weights:  20%|\u2588\u2588        | 58/290 [00:00\u003C00:00, 6299.87it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  \rLoading weights:  20%|\u2588\u2588        | 58/290 [00:00\u003C00:00, 6274.85it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]\rLoading weights:  20%|\u2588\u2588        | 59/290 [00:00\u003C00:00, 6332.56it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  20%|\u2588\u2588        | 59/290 [00:00\u003C00:00, 6309.96it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\rLoading weights:  21%|\u2588\u2588        | 60/290 [00:00\u003C00:00, 6367.55it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  \rLoading weights:  21%|\u2588\u2588        | 60/290 [00:00\u003C00:00, 6344.11it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]\rLoading weights:  21%|\u2588\u2588        | 61/290 [00:00\u003C00:00, 6389.44it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  21%|\u2588\u2588        | 61/290 [00:00\u003C00:00, 6366.07it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\rLoading weights:  21%|\u2588\u2588\u258f       | 62/290 [00:00\u003C00:00, 6210.96it/s, Materializing param=model.layers.5.input_layernorm.weight] \rLoading weights:  21%|\u2588\u2588\u258f       | 62/290 [00:00\u003C00:00, 6188.35it/s, Materializing param=model.layers.5.input_layernorm.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 63/290 [00:00\u003C00:00, 6241.97it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  \rLoading weights:  22%|\u2588\u2588\u258f       | 63/290 [00:00\u003C00:00, 6221.10it/s, Materializing param=model.layers.5.mlp.down_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 64/290 [00:00\u003C00:00, 6095.82it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 64/290 [00:00\u003C00:00, 6074.16it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\rLoading weights:  22%|\u2588\u2588\u258f       | 65/290 [00:00\u003C00:00, 6091.88it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \rLoading weights:  22%|\u2588\u2588\u258f       | 65/290 [00:00\u003C00:00, 6073.01it/s, Materializing param=model.layers.5.mlp.up_proj.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 66/290 [00:00\u003C00:00, 6116.58it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 66/290 [00:00\u003C00:00, 6096.51it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 67/290 [00:00\u003C00:00, 6077.39it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          \rLoading weights:  23%|\u2588\u2588\u258e       | 67/290 [00:00\u003C00:00, 6057.08it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]\rLoading weights:  23%|\u2588\u2588\u258e       | 68/290 [00:00\u003C00:00, 6107.21it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  23%|\u2588\u2588\u258e       | 68/290 [00:00\u003C00:00, 6089.08it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 69/290 [00:00\u003C00:00, 6139.18it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 69/290 [00:00\u003C00:00, 6121.00it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 70/290 [00:00\u003C00:00, 5947.56it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  \rLoading weights:  24%|\u2588\u2588\u258d       | 70/290 [00:00\u003C00:00, 5927.87it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]\rLoading weights:  24%|\u2588\u2588\u258d       | 71/290 [00:00\u003C00:00, 5970.96it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  24%|\u2588\u2588\u258d       | 71/290 [00:00\u003C00:00, 5954.60it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\rLoading weights:  25%|\u2588\u2588\u258d       | 72/290 [00:00\u003C00:00, 6002.22it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  \rLoading weights:  25%|\u2588\u2588\u258d       | 72/290 [00:00\u003C00:00, 5985.57it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]\rLoading weights:  25%|\u2588\u2588\u258c       | 73/290 [00:00\u003C00:00, 6034.61it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  25%|\u2588\u2588\u258c       | 73/290 [00:00\u003C00:00, 6017.77it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 74/290 [00:00\u003C00:00, 6064.33it/s, Materializing param=model.layers.6.input_layernorm.weight] \rLoading weights:  26%|\u2588\u2588\u258c       | 74/290 [00:00\u003C00:00, 6048.61it/s, Materializing param=model.layers.6.input_layernorm.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 75/290 [00:00\u003C00:00, 6095.31it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  \rLoading weights:  26%|\u2588\u2588\u258c       | 75/290 [00:00\u003C00:00, 6079.17it/s, Materializing param=model.layers.6.mlp.down_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 76/290 [00:00\u003C00:00, 6126.25it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  26%|\u2588\u2588\u258c       | 76/290 [00:00\u003C00:00, 6110.16it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 77/290 [00:00\u003C00:00, 6155.16it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  \rLoading weights:  27%|\u2588\u2588\u258b       | 77/290 [00:00\u003C00:00, 6136.33it/s, Materializing param=model.layers.6.mlp.up_proj.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 78/290 [00:00\u003C00:00, 6053.73it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 78/290 [00:00\u003C00:00, 6035.08it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\rLoading weights:  27%|\u2588\u2588\u258b       | 79/290 [00:00\u003C00:00, 6078.03it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          \rLoading weights:  27%|\u2588\u2588\u258b       | 79/290 [00:00\u003C00:00, 6062.02it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]\rLoading weights:  28%|\u2588\u2588\u258a       | 80/290 [00:00\u003C00:00, 6103.91it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 80/290 [00:00\u003C00:00, 6088.96it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 81/290 [00:00\u003C00:00, 6057.78it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 81/290 [00:00\u003C00:00, 6041.62it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\rLoading weights:  28%|\u2588\u2588\u258a       | 82/290 [00:00\u003C00:00, 5884.12it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  \rLoading weights:  28%|\u2588\u2588\u258a       | 82/290 [00:00\u003C00:00, 5867.66it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]\rLoading weights:  29%|\u2588\u2588\u258a       | 83/290 [00:00\u003C00:00, 5909.28it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  29%|\u2588\u2588\u258a       | 83/290 [00:00\u003C00:00, 5894.87it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\rLoading weights:  29%|\u2588\u2588\u2589       | 84/290 [00:00\u003C00:00, 5904.30it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  \rLoading weights:  29%|\u2588\u2588\u2589       | 84/290 [00:00\u003C00:00, 5888.61it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]\rLoading weights:  29%|\u2588\u2588\u2589       | 85/290 [00:00\u003C00:00, 5926.72it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  29%|\u2588\u2588\u2589       | 85/290 [00:00\u003C00:00, 5912.96it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2589       | 86/290 [00:00\u003C00:00, 5954.67it/s, Materializing param=model.layers.7.input_layernorm.weight] \rLoading weights:  30%|\u2588\u2588\u2589       | 86/290 [00:00\u003C00:00, 5940.65it/s, Materializing param=model.layers.7.input_layernorm.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 87/290 [00:00\u003C00:00, 5925.70it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  \rLoading weights:  30%|\u2588\u2588\u2588       | 87/290 [00:00\u003C00:00, 5911.30it/s, Materializing param=model.layers.7.mlp.down_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 88/290 [00:00\u003C00:00, 5836.57it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  30%|\u2588\u2588\u2588       | 88/290 [00:00\u003C00:00, 5822.94it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 89/290 [00:00\u003C00:00, 5861.37it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  \rLoading weights:  31%|\u2588\u2588\u2588       | 89/290 [00:00\u003C00:00, 5848.51it/s, Materializing param=model.layers.7.mlp.up_proj.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 90/290 [00:00\u003C00:00, 5887.66it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  31%|\u2588\u2588\u2588       | 90/290 [00:00\u003C00:00, 5873.46it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\rLoading weights:  31%|\u2588\u2588\u2588\u258f      | 91/290 [00:00\u003C00:00, 5911.13it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          \rLoading weights:  31%|\u2588\u2588\u2588\u258f      | 91/290 [00:00\u003C00:00, 5898.34it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 92/290 [00:00\u003C00:00, 5934.82it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 92/290 [00:00\u003C00:00, 5921.70it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 93/290 [00:00\u003C00:00, 5960.00it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 93/290 [00:00\u003C00:00, 5947.01it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 94/290 [00:00\u003C00:00, 5984.68it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  \rLoading weights:  32%|\u2588\u2588\u2588\u258f      | 94/290 [00:00\u003C00:00, 5971.26it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 95/290 [00:00\u003C00:00, 6007.40it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 95/290 [00:00\u003C00:00, 5994.48it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 96/290 [00:00\u003C00:00, 5958.52it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  \rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 96/290 [00:00\u003C00:00, 5944.19it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 97/290 [00:00\u003C00:00, 5977.78it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  33%|\u2588\u2588\u2588\u258e      | 97/290 [00:00\u003C00:00, 5965.24it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 98/290 [00:00\u003C00:00, 6000.70it/s, Materializing param=model.layers.8.input_layernorm.weight] \rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 98/290 [00:00\u003C00:00, 5988.11it/s, Materializing param=model.layers.8.input_layernorm.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 99/290 [00:00\u003C00:00, 6022.80it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  \rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 99/290 [00:00\u003C00:00, 6010.25it/s, Materializing param=model.layers.8.mlp.down_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 100/290 [00:00\u003C00:00, 5905.72it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  34%|\u2588\u2588\u2588\u258d      | 100/290 [00:00\u003C00:00, 5893.28it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258d      | 101/290 [00:00\u003C00:00, 5925.24it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  \rLoading weights:  35%|\u2588\u2588\u2588\u258d      | 101/290 [00:00\u003C00:00, 5914.15it/s, Materializing param=model.layers.8.mlp.up_proj.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258c      | 102/290 [00:00\u003C00:00, 5947.55it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  35%|\u2588\u2588\u2588\u258c      | 102/290 [00:00\u003C00:00, 5935.50it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 103/290 [00:00\u003C00:00, 5970.00it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          \rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 103/290 [00:00\u003C00:00, 5957.98it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 104/290 [00:00\u003C00:00, 5665.70it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 104/290 [00:00\u003C00:00, 5654.09it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 105/290 [00:00\u003C00:00, 5684.81it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  36%|\u2588\u2588\u2588\u258c      | 105/290 [00:00\u003C00:00, 5674.62it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 106/290 [00:00\u003C00:00, 5706.10it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  \rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 106/290 [00:00\u003C00:00, 5695.72it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 107/290 [00:00\u003C00:00, 5728.02it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 107/290 [00:00\u003C00:00, 5717.37it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 108/290 [00:00\u003C00:00, 5748.39it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  \rLoading weights:  37%|\u2588\u2588\u2588\u258b      | 108/290 [00:00\u003C00:00, 5738.63it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 109/290 [00:00\u003C00:00, 5767.66it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 109/290 [00:00\u003C00:00, 5757.49it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 110/290 [00:00\u003C00:00, 5787.79it/s, Materializing param=model.layers.9.input_layernorm.weight] \rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 110/290 [00:00\u003C00:00, 5777.35it/s, Materializing param=model.layers.9.input_layernorm.weight]\rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 111/290 [00:00\u003C00:00, 5808.05it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  \rLoading weights:  38%|\u2588\u2588\u2588\u258a      | 111/290 [00:00\u003C00:00, 5797.21it/s, Materializing param=model.layers.9.mlp.down_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u258a      | 112/290 [00:00\u003C00:00, 5827.37it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u258a      | 112/290 [00:00\u003C00:00, 5816.84it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 113/290 [00:00\u003C00:00, 5846.98it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  \rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 113/290 [00:00\u003C00:00, 5836.83it/s, Materializing param=model.layers.9.mlp.up_proj.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 114/290 [00:00\u003C00:00, 5866.52it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  39%|\u2588\u2588\u2588\u2589      | 114/290 [00:00\u003C00:00, 5855.53it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2589      | 115/290 [00:00\u003C00:00, 5871.37it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          \rLoading weights:  40%|\u2588\u2588\u2588\u2589      | 115/290 [00:00\u003C00:00, 5861.03it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 116/290 [00:00\u003C00:00, 5890.95it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 116/290 [00:00\u003C00:00, 5880.62it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 117/290 [00:00\u003C00:00, 5770.08it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  40%|\u2588\u2588\u2588\u2588      | 117/290 [00:00\u003C00:00, 5758.84it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 118/290 [00:00\u003C00:00, 5782.54it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  \rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 118/290 [00:00\u003C00:00, 5773.03it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 119/290 [00:00\u003C00:00, 5801.86it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588      | 119/290 [00:00\u003C00:00, 5792.43it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\rLoading weights:  41%|\u2588\u2588\u2588\u2588\u258f     | 120/290 [00:00\u003C00:00, 5820.91it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  \rLoading weights:  41%|\u2588\u2588\u2588\u2588\u258f     | 120/290 [00:00\u003C00:00, 5811.16it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 121/290 [00:00\u003C00:00, 5838.96it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 121/290 [00:00\u003C00:00, 5829.50it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 122/290 [00:00\u003C00:00, 5814.90it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 122/290 [00:00\u003C00:00, 5804.80it/s, Materializing param=model.layers.10.input_layernorm.weight]\rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 123/290 [00:00\u003C00:00, 5741.47it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  \rLoading weights:  42%|\u2588\u2588\u2588\u2588\u258f     | 123/290 [00:00\u003C00:00, 5731.26it/s, Materializing param=model.layers.10.mlp.down_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 124/290 [00:00\u003C00:00, 5756.05it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 124/290 [00:00\u003C00:00, 5746.00it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 125/290 [00:00\u003C00:00, 5772.38it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  \rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 125/290 [00:00\u003C00:00, 5763.81it/s, Materializing param=model.layers.10.mlp.up_proj.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 126/290 [00:00\u003C00:00, 5791.33it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  43%|\u2588\u2588\u2588\u2588\u258e     | 126/290 [00:00\u003C00:00, 5782.08it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 127/290 [00:00\u003C00:00, 5801.76it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          \rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 127/290 [00:00\u003C00:00, 5792.80it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 128/290 [00:00\u003C00:00, 5819.74it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 128/290 [00:00\u003C00:00, 5811.11it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 129/290 [00:00\u003C00:00, 5837.62it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  44%|\u2588\u2588\u2588\u2588\u258d     | 129/290 [00:00\u003C00:00, 5828.75it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258d     | 130/290 [00:00\u003C00:00, 5855.20it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  \rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258d     | 130/290 [00:00\u003C00:00, 5846.41it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258c     | 131/290 [00:00\u003C00:00, 5821.10it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  45%|\u2588\u2588\u2588\u2588\u258c     | 131/290 [00:00\u003C00:00, 5811.25it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 132/290 [00:00\u003C00:00, 5786.58it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  \rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 132/290 [00:00\u003C00:00, 5777.04it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 133/290 [00:00\u003C00:00, 5799.20it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 133/290 [00:00\u003C00:00, 5790.59it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 134/290 [00:00\u003C00:00, 5814.75it/s, Materializing param=model.layers.11.input_layernorm.weight] \rLoading weights:  46%|\u2588\u2588\u2588\u2588\u258c     | 134/290 [00:00\u003C00:00, 5806.70it/s, Materializing param=model.layers.11.input_layernorm.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 135/290 [00:00\u003C00:00, 5830.34it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  \rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 135/290 [00:00\u003C00:00, 5822.01it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 136/290 [00:00\u003C00:00, 5846.86it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 136/290 [00:00\u003C00:00, 5838.54it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 137/290 [00:00\u003C00:00, 5859.76it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \rLoading weights:  47%|\u2588\u2588\u2588\u2588\u258b     | 137/290 [00:00\u003C00:00, 5851.40it/s, Materializing param=model.layers.11.mlp.up_proj.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 138/290 [00:00\u003C00:00, 5713.13it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 138/290 [00:00\u003C00:00, 5702.71it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 139/290 [00:00\u003C00:00, 5725.48it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          \rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 139/290 [00:00\u003C00:00, 5717.17it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 140/290 [00:00\u003C00:00, 5740.57it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  48%|\u2588\u2588\u2588\u2588\u258a     | 140/290 [00:00\u003C00:00, 5732.55it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u258a     | 141/290 [00:00\u003C00:00, 5701.53it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u258a     | 141/290 [00:00\u003C00:00, 5693.73it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 142/290 [00:00\u003C00:00, 5717.55it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  \rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 142/290 [00:00\u003C00:00, 5709.54it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 143/290 [00:00\u003C00:00, 5731.57it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  49%|\u2588\u2588\u2588\u2588\u2589     | 143/290 [00:00\u003C00:00, 5720.31it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2589     | 144/290 [00:00\u003C00:00, 5738.14it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  \rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2589     | 144/290 [00:00\u003C00:00, 5727.75it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 145/290 [00:00\u003C00:00, 5745.89it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 145/290 [00:00\u003C00:00, 5735.00it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 146/290 [00:00\u003C00:00, 5753.07it/s, Materializing param=model.layers.12.input_layernorm.weight] \rLoading weights:  50%|\u2588\u2588\u2588\u2588\u2588     | 146/290 [00:00\u003C00:00, 5741.37it/s, Materializing param=model.layers.12.input_layernorm.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 147/290 [00:00\u003C00:00, 5757.69it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  \rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 147/290 [00:00\u003C00:00, 5746.32it/s, Materializing param=model.layers.12.mlp.down_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 148/290 [00:00\u003C00:00, 5741.05it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588     | 148/290 [00:00\u003C00:00, 5732.83it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 149/290 [00:00\u003C00:00, 5689.29it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  \rLoading weights:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 149/290 [00:00\u003C00:00, 5680.96it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 150/290 [00:00\u003C00:00, 5703.38it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 150/290 [00:00\u003C00:00, 5695.64it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 151/290 [00:00\u003C00:00, 5679.11it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          \rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 151/290 [00:00\u003C00:00, 5671.43it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 152/290 [00:00\u003C00:00, 5693.34it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 152/290 [00:00\u003C00:00, 5686.33it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 153/290 [00:00\u003C00:00, 5708.47it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 153/290 [00:00\u003C00:00, 5701.16it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 154/290 [00:00\u003C00:00, 5666.69it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  \rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 154/290 [00:00\u003C00:00, 5659.34it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 155/290 [00:00\u003C00:00, 5674.76it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 155/290 [00:00\u003C00:00, 5667.88it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 156/290 [00:00\u003C00:00, 5689.62it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  \rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 156/290 [00:00\u003C00:00, 5682.70it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 157/290 [00:00\u003C00:00, 5704.36it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 157/290 [00:00\u003C00:00, 5697.30it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 158/290 [00:00\u003C00:00, 5718.21it/s, Materializing param=model.layers.13.input_layernorm.weight] \rLoading weights:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 158/290 [00:00\u003C00:00, 5711.55it/s, Materializing param=model.layers.13.input_layernorm.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 159/290 [00:00\u003C00:00, 5689.45it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  \rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 159/290 [00:00\u003C00:00, 5681.50it/s, Materializing param=model.layers.13.mlp.down_proj.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 160/290 [00:00\u003C00:00, 5646.52it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 160/290 [00:00\u003C00:00, 5638.74it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 161/290 [00:00\u003C00:00, 5659.43it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  \rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 161/290 [00:00\u003C00:00, 5652.80it/s, Materializing param=model.layers.13.mlp.up_proj.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 162/290 [00:00\u003C00:00, 5673.75it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 162/290 [00:00\u003C00:00, 5666.80it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 163/290 [00:00\u003C00:00, 5685.28it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          \rLoading weights:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 163/290 [00:00\u003C00:00, 5678.71it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 164/290 [00:00\u003C00:00, 5699.63it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 164/290 [00:00\u003C00:00, 5693.36it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 165/290 [00:00\u003C00:00, 5713.70it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 165/290 [00:00\u003C00:00, 5704.98it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 166/290 [00:00\u003C00:00, 5725.07it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  \rLoading weights:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 166/290 [00:00\u003C00:00, 5718.39it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 167/290 [00:00\u003C00:00, 5704.17it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 167/290 [00:00\u003C00:00, 5696.79it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 168/290 [00:00\u003C00:00, 5648.58it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  \rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 168/290 [00:00\u003C00:00, 5641.12it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 169/290 [00:00\u003C00:00, 5651.84it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 169/290 [00:00\u003C00:00, 5645.41it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 170/290 [00:00\u003C00:00, 5608.06it/s, Materializing param=model.layers.14.input_layernorm.weight] \rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 170/290 [00:00\u003C00:00, 5601.50it/s, Materializing param=model.layers.14.input_layernorm.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 171/290 [00:00\u003C00:00, 5620.67it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  \rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 171/290 [00:00\u003C00:00, 5614.29it/s, Materializing param=model.layers.14.mlp.down_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 172/290 [00:00\u003C00:00, 5633.94it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 172/290 [00:00\u003C00:00, 5627.83it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 173/290 [00:00\u003C00:00, 5646.81it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  \rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 173/290 [00:00\u003C00:00, 5640.70it/s, Materializing param=model.layers.14.mlp.up_proj.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 174/290 [00:00\u003C00:00, 5660.33it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 174/290 [00:00\u003C00:00, 5653.88it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 175/290 [00:00\u003C00:00, 5673.28it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          \rLoading weights:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 175/290 [00:00\u003C00:00, 5664.48it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 176/290 [00:00\u003C00:00, 5663.72it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 176/290 [00:00\u003C00:00, 5656.95it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 177/290 [00:00\u003C00:00, 5674.87it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 177/290 [00:00\u003C00:00, 5668.19it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 178/290 [00:00\u003C00:00, 5686.89it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  \rLoading weights:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 178/290 [00:00\u003C00:00, 5680.31it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 179/290 [00:00\u003C00:00, 5603.01it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 179/290 [00:00\u003C00:00, 5596.45it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 180/290 [00:00\u003C00:00, 5614.20it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  \rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 180/290 [00:00\u003C00:00, 5608.07it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 181/290 [00:00\u003C00:00, 5626.06it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 181/290 [00:00\u003C00:00, 5619.98it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 182/290 [00:00\u003C00:00, 5638.13it/s, Materializing param=model.layers.15.input_layernorm.weight] \rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 182/290 [00:00\u003C00:00, 5631.93it/s, Materializing param=model.layers.15.input_layernorm.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 183/290 [00:00\u003C00:00, 5613.59it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 183/290 [00:00\u003C00:00, 5606.87it/s, Materializing param=model.layers.15.mlp.down_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 184/290 [00:00\u003C00:00, 5622.23it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 184/290 [00:00\u003C00:00, 5616.17it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 185/290 [00:00\u003C00:00, 5561.18it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  \rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 185/290 [00:00\u003C00:00, 5554.89it/s, Materializing param=model.layers.15.mlp.up_proj.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 186/290 [00:00\u003C00:00, 5572.39it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 186/290 [00:00\u003C00:00, 5566.31it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 187/290 [00:00\u003C00:00, 5583.85it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          \rLoading weights:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 187/290 [00:00\u003C00:00, 5578.25it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188/290 [00:00\u003C00:00, 5595.82it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188/290 [00:00\u003C00:00, 5588.60it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 189/290 [00:00\u003C00:00, 5582.17it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 189/290 [00:00\u003C00:00, 5575.18it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 190/290 [00:00\u003C00:00, 5592.80it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  \rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 190/290 [00:00\u003C00:00, 5587.46it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 191/290 [00:00\u003C00:00, 5566.49it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 191/290 [00:00\u003C00:00, 5560.31it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 192/290 [00:00\u003C00:00, 5576.41it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  \rLoading weights:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 192/290 [00:00\u003C00:00, 5570.66it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 193/290 [00:00\u003C00:00, 5587.50it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 193/290 [00:00\u003C00:00, 5581.80it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 194/290 [00:00\u003C00:00, 5598.64it/s, Materializing param=model.layers.16.input_layernorm.weight] \rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 194/290 [00:00\u003C00:00, 5593.06it/s, Materializing param=model.layers.16.input_layernorm.weight]\rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 195/290 [00:00\u003C00:00, 5576.96it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  \rLoading weights:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 195/290 [00:00\u003C00:00, 5571.00it/s, Materializing param=model.layers.16.mlp.down_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 196/290 [00:00\u003C00:00, 5587.77it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 196/290 [00:00\u003C00:00, 5582.34it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 197/290 [00:00\u003C00:00, 5599.30it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  \rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 197/290 [00:00\u003C00:00, 5593.88it/s, Materializing param=model.layers.16.mlp.up_proj.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 198/290 [00:00\u003C00:00, 5606.41it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\rLoading weights:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 198/290 [00:00\u003C00:00, 5600.44it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 199/290 [00:00\u003C00:00, 5616.94it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          \rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 199/290 [00:00\u003C00:00, 5611.54it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 200/290 [00:00\u003C00:00, 5628.50it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 200/290 [00:00\u003C00:00, 5623.48it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 201/290 [00:00\u003C00:00, 5639.16it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\rLoading weights:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 201/290 [00:00\u003C00:00, 5633.81it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 202/290 [00:00\u003C00:00, 5650.02it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  \rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 202/290 [00:00\u003C00:00, 5644.90it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 203/290 [00:00\u003C00:00, 5619.35it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 203/290 [00:00\u003C00:00, 5613.75it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 204/290 [00:00\u003C00:00, 5629.60it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  \rLoading weights:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 204/290 [00:00\u003C00:00, 5624.20it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 205/290 [00:00\u003C00:00, 5640.50it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 205/290 [00:00\u003C00:00, 5635.29it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 206/290 [00:00\u003C00:00, 5611.80it/s, Materializing param=model.layers.17.input_layernorm.weight] \rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 206/290 [00:00\u003C00:00, 5606.34it/s, Materializing param=model.layers.17.input_layernorm.weight]\rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 207/290 [00:00\u003C00:00, 5616.46it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  \rLoading weights:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 207/290 [00:00\u003C00:00, 5610.33it/s, Materializing param=model.layers.17.mlp.down_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 208/290 [00:00\u003C00:00, 5626.49it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 208/290 [00:00\u003C00:00, 5621.49it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 209/290 [00:00\u003C00:00, 5616.95it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  \rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 209/290 [00:00\u003C00:00, 5611.88it/s, Materializing param=model.layers.17.mlp.up_proj.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 210/290 [00:00\u003C00:00, 5628.10it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\rLoading weights:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 210/290 [00:00\u003C00:00, 5622.75it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 211/290 [00:00\u003C00:00, 5638.48it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          \rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 211/290 [00:00\u003C00:00, 5633.41it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 212/290 [00:00\u003C00:00, 5599.48it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 212/290 [00:00\u003C00:00, 5592.12it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 213/290 [00:00\u003C00:00, 5571.76it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\rLoading weights:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 213/290 [00:00\u003C00:00, 5564.99it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 214/290 [00:00\u003C00:00, 5574.41it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  \rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 214/290 [00:00\u003C00:00, 5568.12it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 215/290 [00:00\u003C00:00, 5581.64it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 215/290 [00:00\u003C00:00, 5575.50it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 216/290 [00:00\u003C00:00, 5590.96it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  \rLoading weights:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 216/290 [00:00\u003C00:00, 5585.96it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 217/290 [00:00\u003C00:00, 5601.39it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 217/290 [00:00\u003C00:00, 5596.60it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 218/290 [00:00\u003C00:00, 5597.13it/s, Materializing param=model.layers.18.input_layernorm.weight] \rLoading weights:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 218/290 [00:00\u003C00:00, 5592.06it/s, Materializing param=model.layers.18.input_layernorm.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 219/290 [00:00\u003C00:00, 5607.56it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  \rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 219/290 [00:00\u003C00:00, 5602.67it/s, Materializing param=model.layers.18.mlp.down_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 220/290 [00:00\u003C00:00, 5618.32it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 220/290 [00:00\u003C00:00, 5613.43it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 221/290 [00:00\u003C00:00, 5608.55it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  \rLoading weights:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 221/290 [00:00\u003C00:00, 5603.56it/s, Materializing param=model.layers.18.mlp.up_proj.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 222/290 [00:00\u003C00:00, 5601.79it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 222/290 [00:00\u003C00:00, 5595.06it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 223/290 [00:00\u003C00:00, 5607.43it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          \rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 223/290 [00:00\u003C00:00, 5601.18it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 224/290 [00:00\u003C00:00, 5591.04it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\rLoading weights:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 224/290 [00:00\u003C00:00, 5585.36it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 225/290 [00:00\u003C00:00, 5591.31it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 225/290 [00:00\u003C00:00, 5586.38it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 226/290 [00:00\u003C00:00, 5497.12it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  \rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 226/290 [00:00\u003C00:00, 5484.02it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 227/290 [00:00\u003C00:00, 5493.89it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\rLoading weights:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 227/290 [00:00\u003C00:00, 5488.00it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 228/290 [00:00\u003C00:00, 5500.73it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  \rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 228/290 [00:00\u003C00:00, 5495.76it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 229/290 [00:00\u003C00:00, 5509.64it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 229/290 [00:00\u003C00:00, 5504.71it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 230/290 [00:00\u003C00:00, 5517.24it/s, Materializing param=model.layers.19.input_layernorm.weight] \rLoading weights:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 230/290 [00:00\u003C00:00, 5512.26it/s, Materializing param=model.layers.19.input_layernorm.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 231/290 [00:00\u003C00:00, 5512.45it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  \rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 231/290 [00:00\u003C00:00, 5507.37it/s, Materializing param=model.layers.19.mlp.down_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 232/290 [00:00\u003C00:00, 5493.40it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 232/290 [00:00\u003C00:00, 5488.16it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 233/290 [00:00\u003C00:00, 5480.04it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  \rLoading weights:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 233/290 [00:00\u003C00:00, 5474.15it/s, Materializing param=model.layers.19.mlp.up_proj.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 234/290 [00:00\u003C00:00, 5485.63it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 234/290 [00:00\u003C00:00, 5480.39it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 235/290 [00:00\u003C00:00, 5492.10it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          \rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 235/290 [00:00\u003C00:00, 5487.24it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 236/290 [00:00\u003C00:00, 5498.74it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\rLoading weights:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 236/290 [00:00\u003C00:00, 5493.74it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 237/290 [00:00\u003C00:00, 5505.80it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 237/290 [00:00\u003C00:00, 5501.05it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 238/290 [00:00\u003C00:00, 5513.18it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  \rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 238/290 [00:00\u003C00:00, 5508.59it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 239/290 [00:00\u003C00:00, 5496.04it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\rLoading weights:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 239/290 [00:00\u003C00:00, 5491.28it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 240/290 [00:00\u003C00:00, 5503.07it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  \rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 240/290 [00:00\u003C00:00, 5498.56it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 241/290 [00:00\u003C00:00, 5510.34it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 241/290 [00:00\u003C00:00, 5505.87it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 242/290 [00:00\u003C00:00, 5517.83it/s, Materializing param=model.layers.20.input_layernorm.weight] \rLoading weights:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 242/290 [00:00\u003C00:00, 5511.27it/s, Materializing param=model.layers.20.input_layernorm.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 243/290 [00:00\u003C00:00, 5493.15it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  \rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 243/290 [00:00\u003C00:00, 5488.48it/s, Materializing param=model.layers.20.mlp.down_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 244/290 [00:00\u003C00:00, 5495.85it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 244/290 [00:00\u003C00:00, 5491.52it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 245/290 [00:00\u003C00:00, 5503.54it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  \rLoading weights:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 245/290 [00:00\u003C00:00, 5499.39it/s, Materializing param=model.layers.20.mlp.up_proj.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 246/290 [00:00\u003C00:00, 5511.42it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 246/290 [00:00\u003C00:00, 5506.92it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 247/290 [00:00\u003C00:00, 5505.86it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          \rLoading weights:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 247/290 [00:00\u003C00:00, 5499.84it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 248/290 [00:00\u003C00:00, 5477.32it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 248/290 [00:00\u003C00:00, 5472.40it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 249/290 [00:00\u003C00:00, 5483.21it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 249/290 [00:00\u003C00:00, 5478.90it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 250/290 [00:00\u003C00:00, 5480.06it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  \rLoading weights:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 250/290 [00:00\u003C00:00, 5475.54it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 251/290 [00:00\u003C00:00, 5487.24it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 251/290 [00:00\u003C00:00, 5482.84it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 252/290 [00:00\u003C00:00, 5494.29it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  \rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 252/290 [00:00\u003C00:00, 5488.33it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 253/290 [00:00\u003C00:00, 5499.40it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\rLoading weights:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 253/290 [00:00\u003C00:00, 5495.04it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 254/290 [00:00\u003C00:00, 5488.71it/s, Materializing param=model.layers.21.input_layernorm.weight] \rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 254/290 [00:00\u003C00:00, 5484.19it/s, Materializing param=model.layers.21.input_layernorm.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 255/290 [00:00\u003C00:00, 5492.27it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  \rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 255/290 [00:00\u003C00:00, 5488.12it/s, Materializing param=model.layers.21.mlp.down_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 256/290 [00:00\u003C00:00, 5470.93it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\rLoading weights:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 256/290 [00:00\u003C00:00, 5466.25it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 257/290 [00:00\u003C00:00, 5476.23it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  \rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 257/290 [00:00\u003C00:00, 5471.78it/s, Materializing param=model.layers.21.mlp.up_proj.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 258/290 [00:00\u003C00:00, 5482.97it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 258/290 [00:00\u003C00:00, 5478.59it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 259/290 [00:00\u003C00:00, 5489.57it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          \rLoading weights:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 259/290 [00:00\u003C00:00, 5485.35it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 260/290 [00:00\u003C00:00, 5456.22it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 260/290 [00:00\u003C00:00, 5451.86it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 261/290 [00:00\u003C00:00, 5461.71it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 261/290 [00:00\u003C00:00, 5457.66it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 262/290 [00:00\u003C00:00, 5438.49it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  \rLoading weights:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 262/290 [00:00\u003C00:00, 5434.06it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 263/290 [00:00\u003C00:00, 5444.81it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 263/290 [00:00\u003C00:00, 5440.54it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 264/290 [00:00\u003C00:00, 5451.20it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  \rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 264/290 [00:00\u003C00:00, 5447.12it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 265/290 [00:00\u003C00:00, 5458.03it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\rLoading weights:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 265/290 [00:00\u003C00:00, 5453.94it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 266/290 [00:00\u003C00:00, 5465.00it/s, Materializing param=model.layers.22.input_layernorm.weight] \rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 266/290 [00:00\u003C00:00, 5460.88it/s, Materializing param=model.layers.22.input_layernorm.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 267/290 [00:00\u003C00:00, 5471.90it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  \rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 267/290 [00:00\u003C00:00, 5467.95it/s, Materializing param=model.layers.22.mlp.down_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 268/290 [00:00\u003C00:00, 5478.13it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\rLoading weights:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 268/290 [00:00\u003C00:00, 5474.10it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 269/290 [00:00\u003C00:00, 5485.15it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  \rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 269/290 [00:00\u003C00:00, 5481.13it/s, Materializing param=model.layers.22.mlp.up_proj.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 270/290 [00:00\u003C00:00, 5461.54it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 270/290 [00:00\u003C00:00, 5457.12it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 271/290 [00:00\u003C00:00, 5467.59it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          \rLoading weights:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 271/290 [00:00\u003C00:00, 5463.70it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 272/290 [00:00\u003C00:00, 5458.25it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 272/290 [00:00\u003C00:00, 5454.18it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 273/290 [00:00\u003C00:00, 5435.00it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 273/290 [00:00\u003C00:00, 5430.92it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 274/290 [00:00\u003C00:00, 5438.98it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  \rLoading weights:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 274/290 [00:00\u003C00:00, 5435.04it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 275/290 [00:00\u003C00:00, 5445.30it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 275/290 [00:00\u003C00:00, 5441.31it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 276/290 [00:00\u003C00:00, 5451.66it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  \rLoading weights:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 276/290 [00:00\u003C00:00, 5447.99it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 277/290 [00:00\u003C00:00, 5439.19it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 277/290 [00:00\u003C00:00, 5435.22it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 278/290 [00:00\u003C00:00, 5445.34it/s, Materializing param=model.layers.23.input_layernorm.weight] \rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 278/290 [00:00\u003C00:00, 5441.50it/s, Materializing param=model.layers.23.input_layernorm.weight]\rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 279/290 [00:00\u003C00:00, 5451.51it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  \rLoading weights:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 279/290 [00:00\u003C00:00, 5447.50it/s, Materializing param=model.layers.23.mlp.down_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 280/290 [00:00\u003C00:00, 5455.55it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 280/290 [00:00\u003C00:00, 5451.72it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 281/290 [00:00\u003C00:00, 5423.56it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  \rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 281/290 [00:00\u003C00:00, 5419.40it/s, Materializing param=model.layers.23.mlp.up_proj.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 282/290 [00:00\u003C00:00, 5429.17it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\rLoading weights:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 282/290 [00:00\u003C00:00, 5425.34it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 283/290 [00:00\u003C00:00, 5435.30it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          \rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 283/290 [00:00\u003C00:00, 5431.64it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 284/290 [00:00\u003C00:00, 5441.77it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 284/290 [00:00\u003C00:00, 5438.05it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 285/290 [00:00\u003C00:00, 5448.44it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\rLoading weights:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 285/290 [00:00\u003C00:00, 5444.62it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 286/290 [00:00\u003C00:00, 5454.65it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  \rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 286/290 [00:00\u003C00:00, 5450.84it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 287/290 [00:00\u003C00:00, 5460.71it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 287/290 [00:00\u003C00:00, 5456.70it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 288/290 [00:00\u003C00:00, 5448.67it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  \rLoading weights:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 288/290 [00:00\u003C00:00, 5443.74it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 289/290 [00:00\u003C00:00, 5454.62it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 289/290 [00:00\u003C00:00, 5450.94it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 5462.00it/s, Materializing param=model.norm.weight]                      \rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 5458.54it/s, Materializing param=model.norm.weight]\rLoading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 290/290 [00:00\u003C00:00, 5447.68it/s, Materializing param=model.norm.weight]\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "[Trainer:0] RDMA handles registered for 3 circular buffer slots\n[Trainer:0] Ready! 494,032,768 params, RDMA=True, buffer_slots=3\n[REGISTRY] Registered 'zorplex'\n[SETUP] All actors ready! 2 generators, 2 zorplex workers\nActors re-initialized. Starting async loop...\n\n============================================================\nASYNC MODE: 2 Generators + 1 Trainer (Concurrent)\n============================================================\n[GEN0 # 1] wrong_answer gen=820ms\n[GEN1 # 2] wrong_answer gen=977ms\n[GEN0 # 3] wrong_format gen=421ms\n[GEN1 # 4] wrong_format gen=453ms\n[TRAIN  1] time=978ms buffer=2\n[TRAIN  2] time=426ms buffer=4\n[TRAIN  3] time=446ms buffer=4\n[TRAIN  4] time=442ms buffer=4\n[GEN1 # 5] tool_spam gen=2126ms\n[GEN0 # 6] wrong_format gen=2333ms\n[TRAIN  5] time=421ms buffer=4\n[TRAIN  6] time=424ms buffer=6\n[GEN0 # 7] wrong_format gen=634ms\n[TRAIN  7] time=421ms buffer=6\n[GEN1 # 8] wrong_format gen=1192ms\n[TRAIN  8] time=424ms buffer=7\n[TRAIN  9] time=425ms buffer=8\n[TRAIN 10] time=422ms buffer=8\n[TRAIN 11] time=422ms buffer=8\n[GEN1 # 9] wrong_format gen=1822ms\n[TRAIN 12] time=421ms buffer=8\n[GEN0 #10] wrong_format gen=2766ms\n[TRAIN 13] time=469ms buffer=9\n[GEN1 #11] wrong_format gen=959ms\n[TRAIN 14] time=458ms buffer=10\n[TRAIN 15] time=467ms buffer=11\n[TRAIN 16] time=422ms buffer=11\n[TRAIN 17] time=421ms buffer=11\n[TRAIN 18] time=460ms buffer=11\n[GEN0 #12] wrong_format gen=2590ms\n[TRAIN 19] time=435ms buffer=11\n[GEN1 #13] wrong_format gen=2619ms\n[TRAIN 20] time=434ms buffer=12\n[GEN0 #14] wrong_format gen=2648ms\n[GEN1 #15] wrong_format gen=2497ms\n\nAsync complete: 12.65s, 15 generations, 1.19 gens/s\nEvaluating post-async performance...\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stderr", "text": "[actor=\u003Croot\u003E.\u003C__main__.TrainerActor trainer{'procs': 0/1}\u003E] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n", "type": "stream"}, {"mimetype": "text/plain", "name": "stdout", "text": "Post-async accuracy: 0%\n", "type": "stream"}], "id": "aqbW", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "f26daa12018661cf14d95adc3623c9c4", "console": [], "id": "TRpd", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"sync-vs-async-comparison\"\u003ESync vs Async Comparison\u003C/h2\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EMetric\u003C/th\u003E\n\u003Cth\u003ESYNC\u003C/th\u003E\n\u003Cth\u003EASYNC\u003C/th\u003E\n\u003Cth\u003ERatio\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EWall time\u003C/td\u003E\n\u003Ctd\u003E22.82s\u003C/td\u003E\n\u003Ctd\u003E12.65s\u003C/td\u003E\n\u003Ctd\u003E\u003Cstrong\u003E1.80x\u003C/strong\u003E speedup\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EGenerations\u003C/td\u003E\n\u003Ctd\u003E40\u003C/td\u003E\n\u003Ctd\u003E15\u003C/td\u003E\n\u003Ctd\u003E0.4x\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EGens/second\u003C/td\u003E\n\u003Ctd\u003E1.75\u003C/td\u003E\n\u003Ctd\u003E1.19\u003C/td\u003E\n\u003Ctd\u003E\u003Cstrong\u003E0.7x\u003C/strong\u003E throughput\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EAvg gen time\u003C/td\u003E\n\u003Ctd\u003E342ms\u003C/td\u003E\n\u003Ctd\u003E1657ms\u003C/td\u003E\n\u003Ctd\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EAvg train time\u003C/td\u003E\n\u003Ctd\u003E799ms\u003C/td\u003E\n\u003Ctd\u003E462ms\u003C/td\u003E\n\u003Ctd\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EWeight syncs\u003C/td\u003E\n\u003Ctd\u003E40 (every step)\u003C/td\u003E\n\u003Ctd\u003E11 (per-generator)\u003C/td\u003E\n\u003Ctd\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Ch3 id=\"key-observations\"\u003EKey Observations\u003C/h3\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003EData throughput\u003C/strong\u003E: Async collected \u003Cstrong\u003E0.7x\u003C/strong\u003E more trajectories per second.\n  More data means better gradient estimates.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EGPU utilization\u003C/strong\u003E: In sync mode, the trainer GPU sits idle during generation and\n  vice versa. Async keeps both busy.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EGenerators ran in parallel\u003C/strong\u003E: 2 generators each had their own\n  thread, producing data independently.\u003C/li\u003E\n\u003Cli\u003EThe trainer consumed from the replay buffer continuously, never waiting for a specific\n  generator to finish.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cspan class=\"paragraph\"\u003EIn production with more generators, the throughput advantage grows further.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "6ef3a1d608e215c8065f609a97024cb1", "console": [], "id": "TXez", "outputs": [{"data": {"text/html": "\u003Cdiv style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'\u003E\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"timeline-visualization\"\u003ETimeline Visualization\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EThe Gantt charts below show what each actor was doing over time. In sync mode,\nbars are strictly sequential -- notice the gaps between generation and training bars.\nIn async mode, generators and trainer overlap -- that overlap is where the throughput\ngain comes from.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ETry this:\u003C/strong\u003E Look at the sync chart and count the idle gaps. Each gap is wasted GPU\ntime. Then look at the async chart -- the trainer bar starts almost immediately because\ngenerators are pre-filling the buffer concurrently.\u003C/span\u003E\u003C/span\u003E\u003Cmarimo-mime-renderer data-mime='\u0026quot;application/vnd.marimo+mimebundle\u0026quot;' data-data='\u0026quot;{\u0026#92;\u0026quot;image/png\u0026#92;\u0026quot;: \u0026#92;\u0026quot;data:image/png;base64,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\u0026#92;\u0026quot;, \u0026#92;\u0026quot;__metadata__\u0026#92;\u0026quot;: {\u0026#92;\u0026quot;image/png\u0026#92;\u0026quot;: {\u0026#92;\u0026quot;width\u0026#92;\u0026quot;: 1198, \u0026#92;\u0026quot;height\u0026#92;\u0026quot;: 598}}}\u0026quot;'\u003E\u003C/marimo-mime-renderer\u003E\u003C/div\u003E"}, "type": "data"}]}, {"code_hash": "062962b0a775ae3479f416525946cec1", "console": [], "id": "dNNg", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"policy-staleness-the-cost-of-async\"\u003EPolicy Staleness: The Cost of Async\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EAsync mode gives us better hardware utilization, but there's a trade-off:\n\u003Cstrong\u003Epolicy staleness\u003C/strong\u003E. Generators produce trajectories using an older version\nof the policy while the trainer has already moved on. This is \u003Cem\u003Eoff-policy\u003C/em\u003E\ndata -- the log-probabilities computed during training don't match the policy\nthat generated the trajectory.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWe measure staleness as \u003Ccode\u003Etrainer_version - trajectory_version\u003C/code\u003E at each\ntraining step:\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EMetric\u003C/th\u003E\n\u003Cth\u003ESYNC\u003C/th\u003E\n\u003Cth\u003EASYNC\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EAvg staleness\u003C/td\u003E\n\u003Ctd\u003E1.0\u003C/td\u003E\n\u003Ctd\u003E9.2\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EMax staleness\u003C/td\u003E\n\u003Ctd\u003E1\u003C/td\u003E\n\u003Ctd\u003E20\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003ESync mode shows ~0 staleness because we sync weights to generators after\nevery training step. Async mode shows \u0026gt;0 because generators keep producing\nwith older weights while the trainer advances.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWith REINFORCE, this introduces some bias. More sophisticated algorithms\n(PPO, GRPO) address this with importance sampling ratios (\u003Ccode\u003Epi_new / pi_old\u003C/code\u003E)\nand clipping, but that's beyond our scope here. For a small model with few\nsteps, the staleness is mild -- and the throughput gain from async more than\ncompensates.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "cc61e0a627bbd4580b6ed8ba45ecb6a7", "console": [], "id": "yCnT", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"after-training-did-it-improve\"\u003EAfter Training: Did It Improve?\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EWe ran sync and async training \u003Cstrong\u003Eindependently\u003C/strong\u003E -- each started from the same\nuntrained model (we re-spawned actors between runs). This lets us compare\nboth the throughput characteristics (above) and the training outcomes.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003ENote: We're using a small model (0.5B) with few training steps, so dramatic\nimprovement isn't guaranteed. The point is the \u003Cem\u003Einfrastructure\u003C/em\u003E -- showing that\nthe full loop works end to end.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "0d346a7547ae79efa1a5ad23da7002d3", "console": [], "id": "wlCL", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"training-results-baseline-vs-sync-vs-async\"\u003ETraining Results: Baseline vs Sync vs Async\u003C/h3\u003E\n\u003Cspan class=\"paragraph\"\u003EBoth runs started from the same untrained model and ran for the same number\nof training steps.\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EMetric\u003C/th\u003E\n\u003Cth\u003EBaseline\u003C/th\u003E\n\u003Cth\u003EAfter Sync\u003C/th\u003E\n\u003Cth\u003EAfter Async\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EAccuracy\u003C/td\u003E\n\u003Ctd\u003E0%\u003C/td\u003E\n\u003Ctd\u003E0% (+0%)\u003C/td\u003E\n\u003Ctd\u003E0% (+0%)\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EFormat compliance\u003C/td\u003E\n\u003Ctd\u003E20%\u003C/td\u003E\n\u003Ctd\u003E100% (+80%)\u003C/td\u003E\n\u003Ctd\u003E0% (-20%)\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EAvg turns\u003C/td\u003E\n\u003Ctd\u003E4.6\u003C/td\u003E\n\u003Ctd\u003E1.2\u003C/td\u003E\n\u003Ctd\u003E1.0\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EAvg tool calls\u003C/td\u003E\n\u003Ctd\u003E4.4\u003C/td\u003E\n\u003Ctd\u003E0.2\u003C/td\u003E\n\u003Ctd\u003E0.0\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EFailure mode breakdown:\u003C/strong\u003E\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EMode\u003C/th\u003E\n\u003Cth\u003EBaseline\u003C/th\u003E\n\u003Cth\u003EAfter Sync\u003C/th\u003E\n\u003Cth\u003EAfter Async\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003ESuccess\u003C/td\u003E\n\u003Ctd\u003E0\u003C/td\u003E\n\u003Ctd\u003E0\u003C/td\u003E\n\u003Ctd\u003E0\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EWrong format\u003C/td\u003E\n\u003Ctd\u003E8\u003C/td\u003E\n\u003Ctd\u003E0\u003C/td\u003E\n\u003Ctd\u003E10\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003ETool spam\u003C/td\u003E\n\u003Ctd\u003E0\u003C/td\u003E\n\u003Ctd\u003E0\u003C/td\u003E\n\u003Ctd\u003E0\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EWrong answer\u003C/td\u003E\n\u003Ctd\u003E2\u003C/td\u003E\n\u003Ctd\u003E10\u003C/td\u003E\n\u003Ctd\u003E0\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003ESync accuracy unchanged by 0%.\nAsync accuracy unchanged by 0%.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EWith a 0.5B model and only a few training steps, large gains are unlikely.\nThe key result is that the full pipeline works: generation, training,\nweight sync, and evaluation all compose correctly through Monarch actors. The\nfailure mode breakdown shows \u003Cem\u003Ewhere\u003C/em\u003E the model is improving (or not) -- watch\nfor format compliance changes in particular, since that's the easiest RL win.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "c4f4917bb8b869c54595594a0130480b", "console": [], "id": "kqZH", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"whats-happening-under-the-hood\"\u003EWhat's Happening Under the Hood\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EWhen you run the training loop, here's what each layer does:\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EActor isolation\u003C/strong\u003E: Each actor (trainer, generators, buffer, zorplex workers)\nruns in its own process with its own GPU assignment. \u003Ccode\u003ECUDA_VISIBLE_DEVICES\u003C/code\u003E is\nset in \u003Ccode\u003Esetup()\u003C/code\u003E, not at spawn time -- the \u003Ccode\u003Eprocs\u003C/code\u003E dimension in \u003Ccode\u003Espawn_procs\u003C/code\u003E\nis just a dimension name, not a GPU assignment.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EWeight sync data flow\u003C/strong\u003E (circular buffer + CPU staging from \u003Ca href=\"./07_rdma_weight_sync.html\"\u003ENB07\u003C/a\u003E):\n\u003Cdiv class=\"language-ecl codehilite\"\u003E\u003Cpre\u003E\u003Cspan\u003E\u003C/span\u003E\u003Ccode\u003E\u003Cspan class=\"n\"\u003ETrainer\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGPU\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"o\"\u003E--\u003C/span\u003E\u003Cspan class=\"n\"\u003ED2H\u003C/span\u003E\u003Cspan class=\"o\"\u003E--\u0026gt;\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"n\"\u003ECPU\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Eslot\u003C/span\u003E\u003Cspan class=\"p\"\u003E[\u003C/span\u003E\u003Cspan class=\"n\"\u003Ev\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"o\"\u003E%\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"mi\"\u003E3\u003C/span\u003E\u003Cspan class=\"p\"\u003E]\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"o\"\u003E--\u003C/span\u003E\u003Cspan class=\"n\"\u003ERDMA\u003C/span\u003E\u003Cspan class=\"o\"\u003E--\u0026gt;\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"n\"\u003EGenerator\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003ECPU\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003Estaging\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"o\"\u003E--\u003C/span\u003E\u003Cspan class=\"n\"\u003EH2D\u003C/span\u003E\u003Cspan class=\"o\"\u003E--\u0026gt;\u003C/span\u003E\u003Cspan class=\"w\"\u003E  \u003C/span\u003E\u003Cspan class=\"n\"\u003EGenerator\u003C/span\u003E\u003Cspan class=\"w\"\u003E \u003C/span\u003E\u003Cspan class=\"n\"\u003EGPU\u003C/span\u003E\n\u003C/code\u003E\u003C/pre\u003E\u003C/div\u003E\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003ETrainer publishes weights to a circular buffer after each train step\u003C/li\u003E\n\u003Cli\u003EGenerators pull from the buffer via RDMA into a CPU staging buffer\u003C/li\u003E\n\u003Cli\u003EExplicit H2D copy scatters into GPU model parameters\u003C/li\u003E\n\u003Cli\u003EThe circular buffer has 3 slots, so training never blocks on reads\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EFuture improvement\u003C/strong\u003E: ideally we'd load from the trainer's CPU buffer\n  directly into the model's \u003Ccode\u003Estate_dict\u003C/code\u003E, skipping the staging copy.\n  We hit \u003Ccode\u003ERDMABuffer\u003C/code\u003E bugs doing that, so for now we use the extra buffer.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EAsync concurrency\u003C/strong\u003E (via threads):\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003E1 thread per generator, each using \u003Ccode\u003E.slice(procs=i)\u003C/code\u003E to address its generator\u003C/li\u003E\n\u003Cli\u003EEach generator pulls latest weights from the trainer before each trajectory\n  (\u003Ccode\u003Esync_weights_from_buffer\u003C/code\u003E short-circuits if version hasn't changed)\u003C/li\u003E\n\u003Cli\u003ETraining in the main thread\u003C/li\u003E\n\u003Cli\u003E\u003Ccode\u003Ethreading.Event\u003C/code\u003E coordinates shutdown when training completes\u003C/li\u003E\n\u003Cli\u003EGIL is released during I/O (actor calls) and CUDA (GPU compute), so threads\n  achieve real concurrency\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003ESync vs Async generation\u003C/strong\u003E:\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003ESync mode uses \u003Ccode\u003E.call()\u003C/code\u003E broadcast to trigger all generators at once, then waits\n  for all to finish before training\u003C/li\u003E\n\u003Cli\u003EAsync mode uses \u003Ccode\u003E.slice()\u003C/code\u003E per thread so each generator runs independently --\n  no generator waits for another\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EFault tolerance\u003C/strong\u003E (from \u003Ca href=\"./03_fault_tolerance.html\"\u003ENB03\u003C/a\u003E):\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003EGeneration loops wrap \u003Ccode\u003Egenerate_trajectory.call_one().get()\u003C/code\u003E in \u003Ccode\u003Etry/except\u003C/code\u003E\u003C/li\u003E\n\u003Cli\u003EOn failure, the generator logs and retries instead of crashing the loop\u003C/li\u003E\n\u003C/ul\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "14bf2752cf457144964fe4d3557c20eb", "console": [], "id": "wAgl", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"scaling-up\"\u003EScaling Up\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EWhat we built here scales naturally with Monarch:\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EScale\u003C/th\u003E\n\u003Cth\u003EWhat Changes\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EMore generators\u003C/td\u003E\n\u003Ctd\u003EIncrease \u003Ccode\u003Enum_generators\u003C/code\u003E slider -- spawns larger ActorMesh, \u003Ccode\u003E.call()\u003C/code\u003E broadcast scales automatically\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EMore zorplex workers\u003C/td\u003E\n\u003Ctd\u003EIncrease \u003Ccode\u003ENUM_ZORPLEX\u003C/code\u003E -- parallel task generation via Service\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EMulti-node\u003C/td\u003E\n\u003Ctd\u003EUse \u003Ccode\u003ESlurmJob\u003C/code\u003E instead of \u003Ccode\u003Ethis_host()\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EBetter algorithms\u003C/td\u003E\n\u003Ctd\u003ESwap REINFORCE for PPO/GRPO (add importance sampling)\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EProduction generators\u003C/td\u003E\n\u003Ctd\u003EWrap generators in a Service too (health tracking, auto-scaling)\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EMore services\u003C/td\u003E\n\u003Ctd\u003EAdd reward models, search APIs as actors\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EThe patterns stay the same:\u003C/strong\u003E\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003EActors for isolation and GPU assignment\u003C/li\u003E\n\u003Cli\u003EEndpoints for communication (\u003Ccode\u003E.call_one().get()\u003C/code\u003E)\u003C/li\u003E\n\u003Cli\u003ERDMA + circular buffer for efficient weight transfer\u003C/li\u003E\n\u003Cli\u003EVersion tracking for consistency across actors\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cspan class=\"paragraph\"\u003EThis is the foundation for which you could build production systems, using monarch at scale.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "50e495670ea119d1c4b5cd85c8922ff9", "console": [], "id": "rEll", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"recap-the-full-journey\"\u003ERecap: The Full Journey\u003C/h2\u003E\n\u003Cspan class=\"paragraph\"\u003EWe've come a long way in this notebook series:\u003C/span\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003ENotebook\u003C/th\u003E\n\u003Cth\u003EWhat We Learned\u003C/th\u003E\n\u003C/tr\u003E\n\u003C/thead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E01\u003C/td\u003E\n\u003Ctd\u003EMonarch's history and the single-controller paradigm\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E02\u003C/td\u003E\n\u003Ctd\u003EInteractive development with \u003Ccode\u003Ethis_host()\u003C/code\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E03\u003C/td\u003E\n\u003Ctd\u003EFault tolerance with \u003Ccode\u003Etry/except\u003C/code\u003E on actor calls\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E04\u003C/td\u003E\n\u003Ctd\u003EDistributed tensors -- Monarch's tensor engine\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E05\u003C/td\u003E\n\u003Ctd\u003EZorplex benchmark -- where Qwen 0.5B struggles\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E06\u003C/td\u003E\n\u003Ctd\u003EServices for managing worker pools with health tracking\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E07\u003C/td\u003E\n\u003Ctd\u003ERDMA weight sync, circular buffers, CPU staging\u003C/td\u003E\n\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Cstrong\u003E08\u003C/strong\u003E\u003C/td\u003E\n\u003Ctd\u003E\u003Cstrong\u003EClosing the loop: async RL training end to end\u003C/strong\u003E\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/tbody\u003E\n\u003C/table\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EKey takeaways from this notebook:\u003C/strong\u003E\u003C/span\u003E\n\u003Cul\u003E\n\u003Cli\u003EMonarch makes distributed RL feel like local Python -- actors, endpoints,\n  and slicing compose naturally into a full training system\u003C/li\u003E\n\u003Cli\u003EAsync RL collects more data per unit wall time by running generators\n  and trainer concurrently\u003C/li\u003E\n\u003Cli\u003EThe circular buffer + CPU staging pattern from \u003Ca href=\"./07_rdma_weight_sync.html\"\u003ENB07\u003C/a\u003E decouples training\n  from weight distribution\u003C/li\u003E\n\u003Cli\u003EBefore/after evaluation closes the loop: we can measure whether training\n  actually improves the model\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EWhere to go next:\u003C/strong\u003E Forge GRPO implements these same patterns at production\nscale -- multiple nodes, larger models, PPO/GRPO instead of REINFORCE, and\nproper reward modeling. The Monarch primitives you've learned here are the\nbuilding blocks for all of it.\u003C/span\u003E\n\u003Chr /\u003E\n\u003Cspan class=\"paragraph\"\u003E\u003Cstrong\u003EPrevious:\u003C/strong\u003E \u003Ca href=\"./07b_weight_sync_deep_dive.html\"\u003ENB07b \u2014 RDMA Deep Dive\u003C/a\u003E\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": null, "console": [], "id": "dGlV", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}], "metadata": {"marimo_version": "0.19.9"}, "version": "1"},
            "runtimeConfig": null,
        };
    </script>
  
<marimo-code hidden="">
    %23!%2Fusr%2Fbin%2Fenv%20python3%0A%22%22%22%0ANotebook%2008%3A%20Closing%20the%20Loop%20-%20Async%20RL%20Training%0A%0AIn%20Notebook%2005%2C%20we%20saw%20that%20Qwen%200.5B%20struggles%20with%20compositional%20Zorplex%20tasks.%0ANow%20we%20close%20the%20loop%3A%20train%20the%20model%20using%20async%20RL%20with%20Monarch.%0A%0AThis%20notebook%20demonstrates%3A%0A-%20Building%20RL%20actors%20(Trainer%2C%20Generator%2C%20ReplayBuffer)%0A-%20Running%20concurrent%20generation%20and%20training%0A-%20Weight%20synchronization%20between%20actors%20(circular%20buffer%20%2B%20CPU%20staging)%0A-%20Real%20training%20metrics%20and%20before%2Fafter%20evaluation%0A%0ARun%20with%3A%20uv%20run%20marimo%20edit%20notebooks%2F08_rl_e2e.py%0A%22%22%22%0A%0A%23%20CRITICAL%3A%20Set%20allocator%20config%20BEFORE%20any%20PyTorch%20imports%20(including%20in%20subprocesses)%0A%23%20Set%20both%20names%20for%20compatibility%20(old%20and%20new%20PyTorch%20versions)%0A%0Aimport%20marimo%0A%0A__generated_with%20%3D%20%220.19.9%22%0Aapp%20%3D%20marimo.App(width%3D%22medium%22)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20import%20marimo%20as%20mo%0A%0A%20%20%20%20return%20(mo%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20%23%20Environment%20setup%20for%20Monarch%20subprocesses%0A%20%20%20%20import%20os%0A%20%20%20%20os.environ%5B%22TOKENIZERS_PARALLELISM%22%5D%20%3D%20%22false%22%0A%20%20%20%20os.environ%5B%22HF_HUB_OFFLINE%22%5D%20%3D%20%221%22%0A%20%20%20%20os.environ%5B%22TRANSFORMERS_OFFLINE%22%5D%20%3D%20%221%22%0A%20%20%20%20%23%20Note%3A%20CUDA_VISIBLE_DEVICES%20is%20set%20per-actor%20in%20setup()%0A%20%20%20%20%23%20Note%3A%20PYTORCH_ALLOC_CONF%20is%20set%20at%20module%20level%20for%20RDMA%0A%0A%20%20%20%20import%20sys%0A%20%20%20%20_src_dir%20%3D%20os.path.abspath(os.path.join(os.path.dirname(__file__)%20if%20%22__file__%22%20in%20dir()%20else%20os.getcwd()%2C%20%22..%22%2C%20%22src%22))%0A%20%20%20%20if%20_src_dir%20not%20in%20sys.path%3A%0A%20%20%20%20%20%20%20%20sys.path.insert(0%2C%20_src_dir)%0A%0A%20%20%20%20%23%20Set%20PYTHONPATH%20for%20Monarch%20subprocesses%0A%20%20%20%20_existing%20%3D%20os.environ.get(%22PYTHONPATH%22%2C%20%22%22)%0A%20%20%20%20os.environ%5B%22PYTHONPATH%22%5D%20%3D%20f%22%7B_src_dir%7D%3A%7B_existing%7D%22%20if%20_existing%20else%20_src_dir%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%20Closing%20the%20Loop%3A%20Async%20RL%20Training%0A%0A%20%20%20%20In%20**Notebook%2005**%2C%20we%20introduced%20the%20Zorplex%20benchmark%20and%20identified%20three%0A%20%20%20%20failure%20modes%3A%20wrong%20format%20(no%20%60%5BANSWER%5D%60%20tag)%2C%20tool%20spam%2C%20and%20wrong%20answers.%0A%20%20%20%20The%20model%20often%20gets%20the%20right%20value%20but%20fails%20to%20emit%20it%20correctly.%0A%0A%20%20%20%20**Now%20we%20close%20the%20loop**%3A%20train%20the%20model%20to%20get%20better%20at%20these%20tasks%2C%20and%0A%20%20%20%20track%20which%20failure%20modes%20improve%20during%20training.%0A%0A%20%20%20%20We'll%20build%20on%20patterns%20from%20across%20the%20series.%20The%20architecture%20we're%20building%3A%20multiple%20generators%0A%20%20%20%20feed%20trajectories%20into%20a%20replay%20buffer%20while%20a%20trainer%20continuously%20samples%20and%20updates%20the%20policy.%0A%0A%20%20%20%20We'll%20measure%20*before*%20and%20*after*%20accuracy%20--%20and%20failure%20mode%20breakdown%20--%20to%0A%20%20%20%20see%20if%20training%20actually%20helps.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20from%20collections%20import%20deque%0A%20%20%20%20import%20random%0A%20%20%20%20import%20torch%0A%20%20%20%20import%20torch.nn.functional%20as%20F%0A%0A%20%20%20%20%23%20Model%20imports%0A%20%20%20%20from%20transformers%20import%20AutoModelForCausalLM%2C%20AutoTokenizer%0A%0A%20%20%20%20%23%20Zorplex%20imports%0A%20%20%20%20from%20zorplex_rl%20import%20get_spec%2C%20Task%0A%20%20%20%20from%20zorplex_rl.evaluate%20import%20generate_with_tools%0A%0A%20%20%20%20%23%20RL%20primitives%20(shared%20dataclasses)%0A%20%20%20%20from%20rl_primitives%20import%20Trajectory%2C%20TrainMetrics%0A%0A%20%20%20%20%23%20RDMA%20imports%20(with%20fallback)%0A%20%20%20%20try%3A%0A%20%20%20%20%20%20%20%20from%20monarch.rdma%20import%20RDMABuffer%2C%20is_rdma_available%0A%20%20%20%20%20%20%20%20_rdma_available%20%3D%20is_rdma_available()%0A%20%20%20%20except%20Exception%3A%0A%20%20%20%20%20%20%20%20RDMABuffer%20%3D%20None%0A%20%20%20%20%20%20%20%20_rdma_available%20%3D%20False%0A%0A%20%20%20%20def%20rdma_available()%3A%0A%20%20%20%20%20%20%20%20return%20_rdma_available%0A%0A%20%20%20%20return%20(%0A%20%20%20%20%20%20%20%20AutoModelForCausalLM%2C%0A%20%20%20%20%20%20%20%20AutoTokenizer%2C%0A%20%20%20%20%20%20%20%20F%2C%0A%20%20%20%20%20%20%20%20RDMABuffer%2C%0A%20%20%20%20%20%20%20%20Task%2C%0A%20%20%20%20%20%20%20%20TrainMetrics%2C%0A%20%20%20%20%20%20%20%20Trajectory%2C%0A%20%20%20%20%20%20%20%20deque%2C%0A%20%20%20%20%20%20%20%20generate_with_tools%2C%0A%20%20%20%20%20%20%20%20get_spec%2C%0A%20%20%20%20%20%20%20%20random%2C%0A%20%20%20%20%20%20%20%20rdma_available%2C%0A%20%20%20%20%20%20%20%20torch%2C%0A%20%20%20%20)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20from%20monarch.actor%20import%20Actor%2C%20endpoint%2C%20current_rank%0A%0A%20%20%20%20return%20Actor%2C%20current_rank%2C%20endpoint%0A%0A%0A%40app.cell%0Adef%20_(TrainMetrics%2C%20Trajectory%2C%20mo)%3A%0A%20%20%20%20import%20dataclasses%20as%20_dc%0A%20%20%20%20_traj_fields%20%3D%20%5B(f.name%2C%20f.type.__name__%20if%20hasattr(f.type%2C%20'__name__')%20else%20str(f.type))%20for%20f%20in%20_dc.fields(Trajectory)%5D%0A%20%20%20%20_metrics_fields%20%3D%20%5B(f.name%2C%20f.type.__name__%20if%20hasattr(f.type%2C%20'__name__')%20else%20str(f.type))%20for%20f%20in%20_dc.fields(TrainMetrics)%5D%0A%0A%20%20%20%20mo.md(f%22%22%22%0A%20%20%20%20%23%23%20Shared%20Data%20Structures%0A%0A%20%20%20%20For%20clarity%2C%20we%20are%20using%20the%20following%20data%20structures%20in%20this%20notebook%3A%0A%0A%20%20%20%20**Trajectory**%20--%20one%20rollout%20from%20a%20generator%3A%0A%0A%20%20%20%20%7C%20Field%20%7C%20Type%20%7C%0A%20%20%20%20%7C-------%7C------%7C%0A%20%20%20%20%7B%22%22.join(f%22%7C%20%60%7Bn%7D%60%20%7C%20%60%7Bt%7D%60%20%7C%7Bchr(10)%7D%22%20for%20n%2C%20t%20in%20_traj_fields)%7D%0A%0A%20%20%20%20**TrainMetrics**%20--%20returned%20after%20each%20training%20step%3A%0A%0A%20%20%20%20%7C%20Field%20%7C%20Type%20%7C%0A%20%20%20%20%7C-------%7C------%7C%0A%20%20%20%20%7B%22%22.join(f%22%7C%20%60%7Bn%7D%60%20%7C%20%60%7Bt%7D%60%20%7C%7Bchr(10)%7D%22%20for%20n%2C%20t%20in%20_metrics_fields)%7D%0A%0A%20%20%20%20Key%20fields%3A%0A%20%20%20%20-%20%60model_only_text%60%20stores%20the%20model's%20generated%20tokens%20without%20injected%20tool%0A%20%20%20%20%20%20results%2C%20so%20the%20trainer%20can%20compute%20log-probabilities%20on%20exactly%20what%20the%20model%20produced.%0A%20%20%20%20-%20%60has_answer_tag%60%20tracks%20whether%20the%20model%20emitted%20%60%5BANSWER%5D%60%20--%20this%20is%20the%0A%20%20%20%20%20%20format%20compliance%20signal%20from%20%5BNB05%5D(.%2F05_rl_intro.html)'s%20failure%20mode%20analysis.%0A%20%20%20%20-%20%60failure_mode%60%20classifies%20each%20trajectory%20as%20%60%22success%22%60%2C%20%60%22wrong_format%22%60%2C%0A%20%20%20%20%20%20%60%22tool_spam%22%60%2C%20or%20%60%22wrong_answer%22%60%20so%20we%20can%20track%20which%20failure%20modes%20improve%0A%20%20%20%20%20%20during%20training.%0A%20%20%20%20-%20%60correct_rate%60%20and%20%60format_rate%60%20on%20%60TrainMetrics%60%20let%20us%20track%20these%20signals%0A%20%20%20%20%20%20per%20training%20step.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Service%20Infrastructure%0A%0A%20%20%20%20We%20import%20a%20**custom%20Service%20abstraction**%20from%20%60monarch_utils%60%20that%20manages%20worker%0A%20%20%20%20replicas%20with%20health%20tracking%20and%20round-robin%20routing.%20This%20is%20a%20utility%20we%20built%0A%20%20%20%20for%20this%20notebook%20series%20--%20the%20canonical%20Monarch%20pattern%20uses%20direct%20actor%0A%20%20%20%20references%20and%20slicing%2C%20which%20is%20what%20the%20Service%20wraps%20internally.%0A%0A%20%20%20%20(See%20notebook%2005%20for%20the%20full%20implementation.)%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20from%20monarch_utils.services%20import%20Service%2C%20register_service%0A%0A%20%20%20%20return%20Service%2C%20register_service%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20The%20%60setup()%60%20Pattern%0A%0A%20%20%20%20Actors%20in%20this%20notebook%20use%20a%20two-phase%20initialization%3A%0A%0A%20%20%20%201.%20**%60__init__%60**%20runs%20during%20%60spawn()%60%20--%20keep%20it%20lightweight%20(store%20config%2C%20set%20rank)%0A%20%20%20%202.%20**%60setup()%60**%20is%20an%20endpoint%20called%20explicitly%20after%20spawn%20--%20do%20heavy%20work%20here%0A%20%20%20%20%20%20%20(load%20models%2C%20allocate%20GPU%20memory%2C%20register%20RDMA%20buffers)%0A%0A%20%20%20%20Why%20not%20do%20everything%20in%20%60__init__%60%3F%20Two%20reasons%3A%0A%0A%20%20%20%20-%20**%60spawn()%60%20is%20asynchronous**%20--%20it%20returns%20immediately%2C%20and%20%60__init__%60%20runs%20in%0A%20%20%20%20%20%20the%20remote%20process%20before%20the%20first%20endpoint%20call.%20But%20you%20don't%20control%20*when*%2C%0A%20%20%20%20%20%20and%20you%20can't%20confirm%20it%20completed.%20An%20explicit%20%60setup()%60%20call%20lets%20you%20sequence%0A%20%20%20%20%20%20initialization%20(e.g.%2C%20set%20%60CUDA_VISIBLE_DEVICES%60%20and%20confirm%20it%20took%20effect%20before%0A%20%20%20%20%20%20loading%20a%20model).%0A%20%20%20%20-%20**Coordination**%20--%20you%20often%20need%20to%20initialize%20actors%20in%20a%20specific%20order%20(set%20up%0A%20%20%20%20%20%20the%20trainer%20before%20generators%20try%20to%20sync%20weights).%20Endpoint%20calls%20give%20you%20that%0A%20%20%20%20%20%20sequencing%3B%20%60__init__%60%20doesn't.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Actor%201%3A%20ZorplexWorker%0A%0A%20%20%20%20Tool%20execution%20environments%20(docker%20containers%2C%20sandboxes%2C%20API%20endpoints)%20naturally%0A%20%20%20%20form%20a%20fleet%20--%20you%20want%20many%20instances%20running%20in%20parallel%20to%20keep%20up%20with%0A%20%20%20%20generation%20throughput.%20That%20makes%20them%20a%20good%20fit%20for%20a%20**Service**%20(from%20%5BNB06%5D(.%2F06_services.html))%0A%20%20%20%20with%20health%20tracking%20and%20round-robin%20routing.%0A%0A%20%20%20%20Our%20ZorplexWorker%20actors%20handle%20Zorplex%20tasks%3A%0A%20%20%20%20-%20%60generate_task()%60%20--%20creates%20a%20new%20problem%0A%20%20%20%20-%20%60execute_tool()%60%20--%20handles%20LOOKUP%20calls%0A%20%20%20%20-%20%60check_answer()%60%20--%20verifies%20correctness%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(Actor%2C%20current_rank%2C%20endpoint%2C%20get_spec)%3A%0A%20%20%20%20class%20ZorplexWorker(Actor)%3A%0A%20%20%20%20%20%20%20%20%22%22%22Worker%20actor%20that%20handles%20Zorplex%20tool%20execution.%0A%0A%20%20%20%20%20%20%20%20Managed%20by%20a%20Service%20for%20load%20balancing%20across%20replicas.%0A%20%20%20%20%20%20%20%20%22%22%22%0A%0A%20%20%20%20%20%20%20%20def%20__init__(self%2C%20difficulty%3A%20str%20%3D%20%22easy%22%2C%20seed%3A%20int%20%3D%2042)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.rank%20%3D%20current_rank().rank%0A%20%20%20%20%20%20%20%20%20%20%20%20self.spec%20%3D%20get_spec(%22compositional%22%2C%20difficulty%3Ddifficulty%2C%20seed%3Dseed%20%2B%20self.rank)%0A%20%20%20%20%20%20%20%20%20%20%20%20self.calls_served%20%3D%200%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BZorplexWorker%3A%7Bself.rank%7D%5D%20Initialized%20with%20difficulty%3D%7Bdifficulty%7D%22)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20ping(self)%20-%3E%20bool%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20True%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20generate_task(self)%20-%3E%20tuple%5Bstr%2C%20int%5D%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Generate%20a%20new%20task.%20Returns%20(question%2C%20correct_answer).%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20task%20%3D%20self.spec.generate_task()%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20task.question%2C%20task.correct_answer%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20execute_tool(self%2C%20tool_name%3A%20str%2C%20argument%3A%20str)%20-%3E%20str%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Execute%20a%20tool%20call.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20from%20zorplex_rl.task_specs%20import%20ToolCall%0A%20%20%20%20%20%20%20%20%20%20%20%20tc%20%3D%20ToolCall(tool_name%2C%20argument)%0A%20%20%20%20%20%20%20%20%20%20%20%20result%20%3D%20self.spec.execute_tool(tc)%0A%20%20%20%20%20%20%20%20%20%20%20%20self.calls_served%20%2B%3D%201%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20str(result)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20get_system_prompt(self)%20-%3E%20str%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Get%20the%20system%20prompt%20with%20tool%20hints.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20self.spec.get_system_prompt(with_hint%3DTrue)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20check_answer(self%2C%20model_output%3A%20str%2C%20correct_answer%3A%20int)%20-%3E%20tuple%5Bbool%2C%20int%20%7C%20None%5D%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Check%20if%20model%20output%20contains%20the%20correct%20answer.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20extracted%20%3D%20self.spec.extract_answer(model_output%2C%20%5B%5D)%0A%20%20%20%20%20%20%20%20%20%20%20%20is_correct%20%3D%20extracted%20%3D%3D%20correct_answer%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20is_correct%2C%20extracted%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20stats(self)%20-%3E%20dict%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20%7B%22rank%22%3A%20self.rank%2C%20%22calls_served%22%3A%20self.calls_served%7D%0A%0A%20%20%20%20return%20(ZorplexWorker%2C)%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Actor%202%3A%20ReplayBuffer%0A%0A%20%20%20%20A%20simple%20actor%20that%20stores%20trajectories.%20Generators%20push%20trajectories%20in%2C%0A%20%20%20%20the%20trainer%20samples%20batches%20out.%0A%0A%20%20%20%20Recall%20our%20intro%20to%20async%20RL%20in%20notebook%204%20--%20the%20replay%20buffer%20is%20the%20decoupling%0A%20%20%20%20point%20that%20enables%20asynchronous%20execution.%20Generators%20push%2C%20trainer%20pulls%2C%20neither%0A%20%20%20%20waits%20for%20the%20other.%20A%20secondary%20benefit%20is%20decorrelation%3A%20random%20sampling%20breaks%0A%20%20%20%20the%20correlation%20between%20consecutive%20trajectories%20from%20the%20same%20generator%2C%20giving%0A%20%20%20%20better%20gradient%20estimates%20(especially%20when%20mixing%20tasks%20of%20different%20difficulties).%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(Actor%2C%20Trajectory%2C%20deque%2C%20endpoint%2C%20random)%3A%0A%20%20%20%20class%20ReplayBuffer(Actor)%3A%0A%20%20%20%20%20%20%20%20%22%22%22Stores%20trajectories%20for%20async%20RL%20training.%22%22%22%0A%0A%20%20%20%20%20%20%20%20def%20__init__(self%2C%20max_size%3A%20int%20%3D%201000)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.buffer%3A%20deque%5BTrajectory%5D%20%3D%20deque(maxlen%3Dmax_size)%0A%20%20%20%20%20%20%20%20%20%20%20%20self.total_added%20%3D%200%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BReplayBuffer%5D%20Initialized%20with%20max_size%3D%7Bmax_size%7D%22)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20add(self%2C%20trajectory%3A%20Trajectory)%20-%3E%20None%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Add%20a%20trajectory%20to%20the%20buffer.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20self.buffer.append(trajectory)%0A%20%20%20%20%20%20%20%20%20%20%20%20self.total_added%20%2B%3D%201%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20sample(self%2C%20batch_size%3A%20int)%20-%3E%20list%5BTrajectory%5D%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Sample%20a%20batch%20of%20trajectories.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20len(self.buffer)%20%3D%3D%200%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20%5B%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20n%20%3D%20min(batch_size%2C%20len(self.buffer))%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20random.sample(list(self.buffer)%2C%20n)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20size(self)%20-%3E%20int%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20len(self.buffer)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20clear(self)%20-%3E%20int%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Clear%20the%20buffer.%20Returns%20number%20of%20items%20removed.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20count%20%3D%20len(self.buffer)%0A%20%20%20%20%20%20%20%20%20%20%20%20self.buffer.clear()%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20count%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20stats(self)%20-%3E%20dict%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20len(self.buffer)%20%3D%3D%200%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20%7B%22size%22%3A%200%2C%20%22total_added%22%3A%20self.total_added%2C%20%22avg_reward%22%3A%200.0%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20rewards%20%3D%20%5Bt.reward%20for%20t%20in%20self.buffer%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20failure_modes%20%3D%20%7B%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20t%20in%20self.buffer%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20fm%20%3D%20t.failure_mode%20or%20%22unknown%22%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20failure_modes%5Bfm%5D%20%3D%20failure_modes.get(fm%2C%200)%20%2B%201%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22size%22%3A%20len(self.buffer)%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22total_added%22%3A%20self.total_added%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22avg_reward%22%3A%20sum(rewards)%20%2F%20len(rewards)%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22correct_rate%22%3A%20sum(1%20for%20t%20in%20self.buffer%20if%20t.is_correct)%20%2F%20len(self.buffer)%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22format_rate%22%3A%20sum(1%20for%20t%20in%20self.buffer%20if%20t.has_answer_tag)%20%2F%20len(self.buffer)%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22failure_modes%22%3A%20failure_modes%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%7D%0A%0A%20%20%20%20return%20(ReplayBuffer%2C)%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Actor%203%3A%20TrainerActor%0A%0A%20%20%20%20The%20trainer%20loads%20the%20model%2C%20receives%20batches%20of%20trajectories%2C%20and%20computes%0A%20%20%20%20policy%20gradient%20updates.%20We%20use%20**REINFORCE**%20--%20the%20simplest%20policy%20gradient%0A%20%20%20%20method.%20Production%20systems%20typically%20use%20PPO%20or%20GRPO%2C%20which%20are%20variations%20that%20improve%0A%20%20%20%20stability%2C%20but%20the%20approach%20looks%20similar%20from%20a%20systems%20perspective.%20In%20other%20words%2C%0A%20%20%20%20REINFORCE%20lets%20us%20focus%20on%20the%20*system*%20(actors%2C%20weight%20sync%2C%20async%20coordination)%20rather%0A%20%20%20%20than%20the%20algorithm.%0A%0A%20%20%20%20The%20loss%20for%20each%20trajectory%20is%3A%0A%20%20%20%20%60%60%60%0A%20%20%20%20loss%20%3D%20-sum(log_prob(response_token_i))%20*%20(reward%20-%20baseline)%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20The%20trainer%20is%20the%20most%20complex%20actor%2C%20with%20several%20responsibilities%3A%0A%0A%20%20%20%20-%20**%60setup()%60**%20%E2%80%94%20loads%20the%20model%20onto%20GPU%200%2C%20creates%20the%20optimizer%2C%20and%0A%20%20%20%20%20%20registers%20RDMA%20circular%20buffer%20slots%0A%20%20%20%20-%20**%60train_step()%60**%20%E2%80%94%20REINFORCE%20policy%20gradient%20on%20a%20batch%20of%20trajectories%0A%20%20%20%20-%20**%60get_weight_handle()%60**%20%E2%80%94%20returns%20an%20RDMA%20handle%20to%20the%20current%20circular%0A%20%20%20%20%20%20buffer%20slot%20for%20generators%20to%20pull%20from%0A%20%20%20%20-%20**%60evaluate_zorplex()%60**%20%E2%80%94%20runs%20deterministic%20evaluation%20for%20before%2Fafter%20comparison%0A%0A%20%20%20%20**GPU%20assignment%20note%3A**%20Monarch%20doesn't%20assign%20GPUs%20automatically%20%E2%80%94%0A%20%20%20%20%60spawn_procs%60%20creates%20processes%2C%20but%20it's%20up%20to%20you%20to%20set%0A%20%20%20%20%60CUDA_VISIBLE_DEVICES%60%20in%20%60setup()%60.%20Here%2C%20the%20trainer%20hardcodes%20GPU%200%0A%20%20%20%20and%20generators%20use%20GPU%201%2B.%0A%0A%20%20%20%20**Circular%20buffer%20with%20CPU%20staging**%20(from%20%5BNB07%5D(.%2F07_rdma_weight_sync.html))%3A%20After%20each%20training%20step%2C%0A%20%20%20%20weights%20are%20copied%20GPU%20-%3E%20CPU%20into%20a%20circular%20buffer%20slot.%20Generators%20read%0A%20%20%20%20from%20CPU%20via%20RDMA%2C%20then%20copy%20to%20their%20own%20GPU.%20This%20decouples%20training%20from%0A%20%20%20%20weight%20distribution.%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20Trainer%20GPU%20--D2H--%3E%20CPU%20slot%5Bv%20%25%203%5D%20--RDMA--%3E%20Generator%20CPU%20staging%20--H2D--%3E%20Generator%20GPU%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20Each%20slot%20is%20a%20single%20**contiguous**%20CPU%20buffer%20%E2%80%94%20all%20parameters%20packed%0A%20%20%20%20end-to-end.%20This%20means%20one%20RDMA%20read%20transfers%20the%20entire%20model.%20An%0A%20%20%20%20alternative%20is%20keeping%20parameters%20scattered%20and%20batching%20reads%20with%0A%20%20%20%20%60RDMAAction%60.%20We%20go%20into%20the%20different%20patterns%20and%20trade-offs%20in%20%5BNB07b%5D(.%2F07b_weight_sync_deep_dive.html).%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(%0A%20%20%20%20Actor%2C%0A%20%20%20%20AutoModelForCausalLM%2C%0A%20%20%20%20AutoTokenizer%2C%0A%20%20%20%20F%2C%0A%20%20%20%20RDMABuffer%2C%0A%20%20%20%20TrainMetrics%2C%0A%20%20%20%20Trajectory%2C%0A%20%20%20%20current_rank%2C%0A%20%20%20%20endpoint%2C%0A%20%20%20%20generate_with_tools%2C%0A%20%20%20%20get_spec%2C%0A%20%20%20%20rdma_available%2C%0A%20%20%20%20torch%2C%0A)%3A%0A%20%20%20%20class%20TrainerActor(Actor)%3A%0A%20%20%20%20%20%20%20%20%22%22%22Trains%20the%20model%20on%20trajectories.%0A%0A%20%20%20%20%20%20%20%20Uses%20setup()%20for%20heavy%20initialization%20(model%20loading%2C%20RDMA%20registration).%0A%20%20%20%20%20%20%20%20Implements%20circular%20buffer%20with%20CPU%20staging%20for%20weight%20distribution.%0A%20%20%20%20%20%20%20%20%22%22%22%0A%0A%20%20%20%20%20%20%20%20def%20__init__(%0A%20%20%20%20%20%20%20%20%20%20%20%20self%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20model_name%3A%20str%20%3D%20%22Qwen%2FQwen2.5-0.5B-Instruct%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20lr%3A%20float%20%3D%201e-5%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20device%3A%20str%20%3D%20%22cuda%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20n_buffer_slots%3A%20int%20%3D%203%2C%0A%20%20%20%20%20%20%20%20)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Lightweight%20init%20-%20just%20store%20config%0A%20%20%20%20%20%20%20%20%20%20%20%20self.model_name%20%3D%20model_name%0A%20%20%20%20%20%20%20%20%20%20%20%20self.lr%20%3D%20lr%0A%20%20%20%20%20%20%20%20%20%20%20%20self.device_config%20%3D%20device%0A%20%20%20%20%20%20%20%20%20%20%20%20self.n_buffer_slots%20%3D%20n_buffer_slots%0A%20%20%20%20%20%20%20%20%20%20%20%20self.rank%20%3D%20current_rank().rank%0A%20%20%20%20%20%20%20%20%20%20%20%20self._ready%20%3D%20False%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BTrainer%3A%7Bself.rank%7D%5D%20Spawned%2C%20waiting%20for%20setup()...%22)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20setup(self)%20-%3E%20dict%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Heavy%20initialization%3A%20load%20model%2C%20create%20optimizer%2C%20set%20up%20circular%20buffer.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20import%20os%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20self._ready%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20%7B%22status%22%3A%20%22already_ready%22%7D%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Trainer%20always%20uses%20GPU%200%0A%20%20%20%20%20%20%20%20%20%20%20%20os.environ%5B%22CUDA_VISIBLE_DEVICES%22%5D%20%3D%20%220%22%0A%20%20%20%20%20%20%20%20%20%20%20%20self.device%20%3D%20%22cuda%22%20if%20torch.cuda.is_available()%20else%20%22cpu%22%0A%20%20%20%20%20%20%20%20%20%20%20%20self.policy_version%20%3D%200%0A%20%20%20%20%20%20%20%20%20%20%20%20self.train_steps%20%3D%200%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BTrainer%3A%7Bself.rank%7D%5D%20Loading%20model%20%7Bself.model_name%7D%20on%20GPU%200...%22)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.tokenizer%20%3D%20AutoTokenizer.from_pretrained(self.model_name)%0A%20%20%20%20%20%20%20%20%20%20%20%20self.model%20%3D%20AutoModelForCausalLM.from_pretrained(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.model_name%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20torch_dtype%3Dtorch.bfloat16%20if%20self.device%20%3D%3D%20%22cuda%22%20else%20torch.float32%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20).to(self.device)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20self.tokenizer.pad_token%20is%20None%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.tokenizer.pad_token%20%3D%20self.tokenizer.eos_token%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.optimizer%20%3D%20torch.optim.AdamW(self.model.parameters()%2C%20lr%3Dself.lr)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20---%20Circular%20buffer%20with%20CPU%20staging%20---%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Each%20slot%20is%20a%20single%20contiguous%20CPU%20buffer%20that%20holds%20ALL%20model%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20parameters%20packed%20end-to-end.%20We%20copy%20the%20full%20state_dict%20into%20one%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20flat%20region%2C%20which%20means%20one%20RDMA%20read%20per%20sync%20(fewest%20round%20trips).%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Alternative%3A%20keep%20parameters%20scattered%20(one%20buffer%20per%20param)%20and%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20use%20RDMAAction%20to%20batch%20multiple%20reads%20into%20one%20operation.%20See%20NB07b%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20for%20a%20comparison%20of%20the%20different%20patterns%20and%20trade-offs.%0A%20%20%20%20%20%20%20%20%20%20%20%20total_bytes%20%3D%20sum(p.numel()%20*%20p.element_size()%20for%20p%20in%20self.model.parameters())%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self._slots%20%3D%20%5B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20torch.empty(total_bytes%2C%20dtype%3Dtorch.uint8)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20for%20_%20in%20range(self.n_buffer_slots)%0A%20%20%20%20%20%20%20%20%20%20%20%20%5D%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self._slot_handles%20%3D%20%5B%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20rdma_available()%20and%20RDMABuffer%20is%20not%20None%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20try%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20for%20slot%20in%20self._slots%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self._slot_handles.append(RDMABuffer(slot))%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BTrainer%3A%7Bself.rank%7D%5D%20RDMA%20handles%20registered%20for%20%7Bself.n_buffer_slots%7D%20circular%20buffer%20slots%22)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20except%20Exception%20as%20e%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BTrainer%3A%7Bself.rank%7D%5D%20RDMA%20registration%20failed%3A%20%7Be%7D%22)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self._slot_handles%20%3D%20%5B%5D%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self._param_meta%20%3D%20%7B%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20offset%20%3D%200%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20name%2C%20p%20in%20self.model.named_parameters()%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self._param_meta%5Bname%5D%20%3D%20(offset%2C%20tuple(p.shape)%2C%20p.dtype)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20offset%20%2B%3D%20p.numel()%20*%20p.element_size()%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self._publish_weights()%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self._ready%20%3D%20True%0A%20%20%20%20%20%20%20%20%20%20%20%20param_count%20%3D%20sum(p.numel()%20for%20p%20in%20self.model.parameters())%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BTrainer%3A%7Bself.rank%7D%5D%20Ready!%20%7Bparam_count%3A%2C%7D%20params%2C%20%22%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20f%22RDMA%3D%7Blen(self._slot_handles)%20%3E%200%7D%2C%20%22%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20f%22buffer_slots%3D%7Bself.n_buffer_slots%7D%22)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22status%22%3A%20%22ready%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22params%22%3A%20param_count%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22rdma%22%3A%20len(self._slot_handles)%20%3E%200%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22buffer_slots%22%3A%20self.n_buffer_slots%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%7D%0A%0A%20%20%20%20%20%20%20%20def%20_publish_weights(self)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Copy%20GPU%20params%20to%20the%20current%20circular%20buffer%20slot%20(D2H).%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20slot_idx%20%3D%20self.policy_version%20%25%20self.n_buffer_slots%0A%20%20%20%20%20%20%20%20%20%20%20%20slot%20%3D%20self._slots%5Bslot_idx%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20name%2C%20p%20in%20self.model.named_parameters()%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20off%2C%20shape%2C%20dtype%20%3D%20self._param_meta%5Bname%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20nbytes%20%3D%20p.numel()%20*%20p.element_size()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20slot%5Boff%3Aoff%20%2B%20nbytes%5D.copy_(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20p.data.view(-1).view(torch.uint8).cpu()%2C%20non_blocking%3DTrue%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20torch.cuda.synchronize()%20%20%23%20Ensure%20D2H%20complete%20before%20RDMA%20reads%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20get_weight_handle(self)%20-%3E%20tuple%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Get%20RDMA%20handle%20for%20the%20latest%20weight%20slot.%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20Returns%20(handle_or_None%2C%20param_meta%2C%20version%2C%20total_bytes).%0A%20%20%20%20%20%20%20%20%20%20%20%20If%20RDMA%20unavailable%2C%20handle%20is%20None%20and%20caller%20should%20use%20get_state_dict().%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20total_bytes%20%3D%20sum(p.numel()%20*%20p.element_size()%20for%20p%20in%20self.model.parameters())%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20self._slot_handles%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20slot_idx%20%3D%20self.policy_version%20%25%20self.n_buffer_slots%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20self._slot_handles%5Bslot_idx%5D%2C%20self._param_meta%2C%20self.policy_version%2C%20total_bytes%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20None%2C%20self._param_meta%2C%20self.policy_version%2C%20total_bytes%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20get_state_dict(self)%20-%3E%20tuple%5Bdict%2C%20int%5D%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Fallback%3A%20get%20state%20dict%20directly%20(when%20RDMA%20not%20available).%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20self.model.state_dict()%2C%20self.policy_version%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20get_version(self)%20-%3E%20int%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20self.policy_version%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20train_step(self%2C%20trajectories%3A%20list%5BTrajectory%5D%2C%20baseline%3A%20float)%20-%3E%20TrainMetrics%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Train%20on%20a%20batch%20of%20trajectories%20using%20REINFORCE.%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20Each%20trajectory%20carries%20pre-tokenized%20input_ids%20and%20a%20prompt_length%0A%20%20%20%20%20%20%20%20%20%20%20%20boundary%20from%20the%20generator%2C%20so%20we%20just%20slice%20and%20compute%20log-probs.%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20len(trajectories)%20%3D%3D%200%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20TrainMetrics(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20step%3Dself.train_steps%2C%20loss%3D0.0%2C%20batch_size%3D0%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20avg_reward%3D0.0%2C%20policy_version%3Dself.policy_version%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.model.train()%0A%20%20%20%20%20%20%20%20%20%20%20%20self.optimizer.zero_grad()%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20losses%20%3D%20%5B%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20valid_count%20%3D%200%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20traj%20in%20trajectories%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20not%20traj.input_ids%20or%20traj.prompt_length%20%3D%3D%200%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20continue%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Step%201%3A%20Load%20pre-tokenized%20sequence%20from%20the%20generator%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20full_ids%20%3D%20torch.tensor(traj.input_ids%2C%20device%3Dself.device).unsqueeze(0)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20prompt_len%20%3D%20traj.prompt_length%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20full_ids.shape%5B1%5D%20%3C%3D%20prompt_len%20%2B%201%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20continue%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Step%202-3%3A%20Forward%20pass%2C%20then%20slice%20at%20prompt_length%20for%20response-only%20log-probs%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20with%20torch.amp.autocast('cuda'%2C%20enabled%3Dself.device%20%3D%3D%20%22cuda%22)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20logits%20%3D%20self.model(full_ids).logits%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20logits%5Bi%5D%20predicts%20token%5Bi%2B1%5D%2C%20so%20start%20at%20prompt_len%20-%201%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20shift_logits%20%3D%20logits%5B%3A%2C%20prompt_len%20-%201%3A-1%2C%20%3A%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20shift_labels%20%3D%20full_ids%5B%3A%2C%20prompt_len%3A%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20log_probs%20%3D%20F.log_softmax(shift_logits%2C%20dim%3D-1)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20token_log_probs%20%3D%20log_probs.gather(2%2C%20shift_labels.unsqueeze(-1)).squeeze(-1)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Step%204%3A%20REINFORCE%20loss%20%3D%20-log_prob%20*%20advantage%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20advantage%20%3D%20traj.reward%20-%20baseline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20losses.append(-token_log_probs.sum()%20*%20advantage)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20valid_count%20%2B%3D%201%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Step%205%3A%20Optimizer%20step%2C%20then%20publish%20weights%20to%20circular%20buffer%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20valid_count%20%3E%200%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20avg_loss%20%3D%20torch.stack(losses).sum()%20%2F%20valid_count%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20avg_loss.backward()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20torch.nn.utils.clip_grad_norm_(self.model.parameters()%2C%20max_norm%3D1.0)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.optimizer.step()%0A%20%20%20%20%20%20%20%20%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20avg_loss%20%3D%20torch.tensor(0.0)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Bump%20version%2C%20then%20publish%20weights%20to%20the%20new%20slot.%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Safe%20because%20Monarch%20actors%20process%20endpoints%20sequentially.%0A%20%20%20%20%20%20%20%20%20%20%20%20self.policy_version%20%2B%3D%201%0A%20%20%20%20%20%20%20%20%20%20%20%20self._publish_weights()%0A%20%20%20%20%20%20%20%20%20%20%20%20self.train_steps%20%2B%3D%201%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20avg_reward%20%3D%20sum(t.reward%20for%20t%20in%20trajectories)%20%2F%20len(trajectories)%0A%20%20%20%20%20%20%20%20%20%20%20%20correct_rate%20%3D%20sum(1%20for%20t%20in%20trajectories%20if%20t.is_correct)%20%2F%20len(trajectories)%0A%20%20%20%20%20%20%20%20%20%20%20%20format_rate%20%3D%20sum(1%20for%20t%20in%20trajectories%20if%20t.has_answer_tag)%20%2F%20len(trajectories)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20TrainMetrics(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20step%3Dself.train_steps%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20loss%3Davg_loss.item()%20if%20torch.is_tensor(avg_loss)%20else%20avg_loss%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20batch_size%3Dlen(trajectories)%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20avg_reward%3Davg_reward%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20policy_version%3Dself.policy_version%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20correct_rate%3Dcorrect_rate%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20format_rate%3Dformat_rate%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20evaluate_zorplex(self%2C%20num_samples%3A%20int%20%3D%2010%2C%20seed%3A%20int%20%3D%2042)%20-%3E%20dict%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Evaluate%20current%20model%20on%20compositional%20Zorplex%20tasks.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20import%20re%20as%20_re%0A%20%20%20%20%20%20%20%20%20%20%20%20self.model.eval()%0A%20%20%20%20%20%20%20%20%20%20%20%20torch.manual_seed(seed)%20%20%23%20Deterministic%20evaluation%0A%20%20%20%20%20%20%20%20%20%20%20%20spec%20%3D%20get_spec(%22compositional%22%2C%20seed%3Dseed)%0A%20%20%20%20%20%20%20%20%20%20%20%20correct%20%3D%200%0A%20%20%20%20%20%20%20%20%20%20%20%20total_turns%20%3D%200%0A%20%20%20%20%20%20%20%20%20%20%20%20total_tools%20%3D%200%0A%20%20%20%20%20%20%20%20%20%20%20%20format_ok%20%3D%200%0A%20%20%20%20%20%20%20%20%20%20%20%20failure_modes%20%3D%20%7B%22success%22%3A%200%2C%20%22wrong_format%22%3A%200%2C%20%22tool_spam%22%3A%200%2C%20%22wrong_answer%22%3A%200%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20_%20in%20range(num_samples)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20task%20%3D%20spec.generate_task()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20result%20%3D%20generate_with_tools(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.model%2C%20self.tokenizer%2C%20spec%2C%20task%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.device%2C%20max_turns%3D5%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20temperature%3D0.0%2C%20do_sample%3DFalse%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20correct%20%2B%3D%20int(result.is_correct)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20total_turns%20%2B%3D%20len(result.turns)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20total_tools%20%2B%3D%20result.total_tool_calls%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20has_tag%20%3D%20bool(_re.search(r'%5C%5BANSWER%5C%5D'%2C%20result.final_text))%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20format_ok%20%2B%3D%20int(has_tag)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20result.is_correct%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20failure_modes%5B%22success%22%5D%20%2B%3D%201%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20elif%20not%20has_tag%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20failure_modes%5B%22wrong_format%22%5D%20%2B%3D%201%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20elif%20result.total_tool_calls%20%3E%203%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20failure_modes%5B%22tool_spam%22%5D%20%2B%3D%201%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20failure_modes%5B%22wrong_answer%22%5D%20%2B%3D%201%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22accuracy%22%3A%20correct%20%2F%20num_samples%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22correct%22%3A%20correct%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22total%22%3A%20num_samples%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22avg_turns%22%3A%20total_turns%20%2F%20num_samples%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22avg_tools%22%3A%20total_tools%20%2F%20num_samples%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22format_rate%22%3A%20format_ok%20%2F%20num_samples%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22failure_modes%22%3A%20failure_modes%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%7D%0A%0A%20%20%20%20return%20(TrainerActor%2C)%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%20How%20%60train_step%60%20Works%0A%0A%20%20%20%20Each%20trajectory%20arrives%20with%20pre-tokenized%20%60input_ids%60%20and%20a%20%60prompt_length%60%0A%20%20%20%20boundary%20(computed%20by%20the%20generator%20at%20generation%20time).%20The%20trainer%3A%0A%0A%20%20%20%201.%20**Loads%20the%20token%20sequence**%20directly%20from%20%60traj.input_ids%60%20--%20no%20re-tokenization.%0A%20%20%20%202.%20**Slices%20at%20%60prompt_length%60**%20to%20separate%20prompt%20from%20response%20tokens.%0A%20%20%20%203.%20**Computes%20log-probs**%20on%20response%20tokens%20only%20(%60logits%5Bi%5D%60%20predicts%20%60token%5Bi%2B1%5D%60%2C%0A%20%20%20%20%20%20%20so%20we%20start%20at%20%60prompt_length%20-%201%60).%0A%20%20%20%204.%20**Computes%20loss**%3A%20%60loss%20%3D%20-sum(log_probs)%20*%20advantage%60%20where%0A%20%20%20%20%20%20%20%60advantage%20%3D%20reward%20-%20baseline%60.%20Positive%20advantage%20reinforces%20the%20response.%0A%20%20%20%205.%20**Steps%20the%20optimizer**%20once%20for%20the%20whole%20batch%2C%20then%20publishes%20new%20weights%0A%20%20%20%20%20%20%20to%20the%20circular%20buffer.%0A%0A%20%20%20%20Look%20for%20the%20%60%23%20Step%20N%3A%60%20comments%20in%20%60train_step%60%20above%20--%20they%20correspond%20to%0A%20%20%20%20these%20steps.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Actor%204%3A%20GeneratorWorker%0A%0A%20%20%20%20Each%20generator%20loads%20its%20own%20copy%20of%20the%20model%2C%20generates%20its%20own%20tasks%20from%0A%20%20%20%20its%20seeded%20spec%2C%20and%20runs%20inference%20independently.%20The%20key%20endpoint%20is%0A%20%20%20%20%60generate_trajectory()%60%20--%20it%20generates%20a%20task%2C%20runs%20multi-turn%20inference%0A%20%20%20%20with%20tool%20execution%2C%20and%20returns%20a%20complete%20%60Trajectory%60%20with%20pre-tokenized%0A%20%20%20%20%60input_ids%60%20and%20%60prompt_length%60%20for%20the%20trainer.%0A%0A%20%20%20%20**Reward%20shaping.**%20Instead%20of%20a%20binary%200%2F1%20reward%2C%20we%20decompose%20rewards%20from%0A%20%20%20%20the%20failure%20modes%20identified%20in%20%5BNB05%5D(.%2F05_rl_intro.html)%3A%0A%0A%20%20%20%20%7C%20Component%20%7C%20Value%20%7C%20Why%20%7C%0A%20%20%20%20%7C-----------%7C-------%7C-----%7C%0A%20%20%20%20%7C%20Correct%20answer%20%7C%20%2B1.0%20%7C%20The%20main%20signal%20%7C%0A%20%20%20%20%7C%20Format%20compliance%20(%60%5BANSWER%5D%60%20tag)%20%7C%20%2B0.2%20%7C%20Learnable%20even%20when%20wrong%20%7C%0A%20%20%20%20%7C%20Tool%20spam%20penalty%20%7C%20-0.1%20per%20call%20beyond%202%20%7C%20Discourages%20degenerate%20loops%20%7C%0A%0A%20%20%20%20This%20means%20a%20correct%2C%20well-formatted%20response%20earns%20up%20to%201.2%2C%20while%20a%0A%20%20%20%20format-only%20success%20(wrong%20answer%20but%20used%20%60%5BANSWER%5D%60)%20earns%200.2.%20The%0A%20%20%20%20gradient%20signal%20is%20richer%20than%20binary%3A%20the%20model%20gets%20*partial%20credit*%20for%0A%20%20%20%20good%20formatting%20even%20before%20it%20learns%20the%20right%20answers.%0A%0A%20%20%20%20Weight%20sync%20uses%20the%20pattern%20from%20%5BNB07%5D(.%2F07_rdma_weight_sync.html)%3A%20the%20trainer%20publishes%20weights%20to%20CPU%0A%20%20%20%20slots%20(circular%20buffer)%2C%20and%20generators%20pull%20via%20RDMA%20into%20a%20CPU%20staging%0A%20%20%20%20buffer%2C%20then%20scatter%20into%20GPU%20parameters%20(H2D%20copy).%20Ideally%20we'd%20load%0A%20%20%20%20directly%20from%20the%20trainer's%20CPU%20buffer%20into%20the%20model's%20%60state_dict%60%20to%0A%20%20%20%20avoid%20the%20extra%20copy%2C%20but%20we%20hit%20%60RDMABuffer%60%20bugs%20doing%20that%20%E2%80%94%20will%20fix.%0A%20%20%20%20Fallback%20path%20(%60sync_weights%60%20using%20%60state_dict%60)%20stays%20for%20when%20RDMA%20is%20unavailable.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(%0A%20%20%20%20Actor%2C%0A%20%20%20%20AutoModelForCausalLM%2C%0A%20%20%20%20AutoTokenizer%2C%0A%20%20%20%20Task%2C%0A%20%20%20%20Trajectory%2C%0A%20%20%20%20current_rank%2C%0A%20%20%20%20endpoint%2C%0A%20%20%20%20generate_with_tools%2C%0A%20%20%20%20get_spec%2C%0A%20%20%20%20torch%2C%0A)%3A%0A%20%20%20%20class%20GeneratorWorker(Actor)%3A%0A%20%20%20%20%20%20%20%20%22%22%22Individual%20generator%20worker.%0A%0A%20%20%20%20%20%20%20%20Uses%20setup()%20for%20heavy%20initialization%20(model%20loading).%0A%20%20%20%20%20%20%20%20Weight%20sync%20uses%20CPU%20staging%20buffer%20for%20explicit%20RDMA%20-%3E%20H2D%20flow.%0A%20%20%20%20%20%20%20%20%22%22%22%0A%0A%20%20%20%20%20%20%20%20def%20__init__(%0A%20%20%20%20%20%20%20%20%20%20%20%20self%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20model_name%3A%20str%20%3D%20%22Qwen%2FQwen2.5-0.5B-Instruct%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20difficulty%3A%20str%20%3D%20%22easy%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20device%3A%20str%20%3D%20%22cuda%22%2C%0A%20%20%20%20%20%20%20%20)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Lightweight%20init%20-%20just%20store%20config%0A%20%20%20%20%20%20%20%20%20%20%20%20self.model_name%20%3D%20model_name%0A%20%20%20%20%20%20%20%20%20%20%20%20self.difficulty%20%3D%20difficulty%0A%20%20%20%20%20%20%20%20%20%20%20%20self.device_config%20%3D%20device%0A%20%20%20%20%20%20%20%20%20%20%20%20self.rank%20%3D%20current_rank().rank%0A%20%20%20%20%20%20%20%20%20%20%20%20self._ready%20%3D%20False%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BGeneratorWorker%3A%7Bself.rank%7D%5D%20Spawned%2C%20waiting%20for%20setup()...%22)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20setup(self)%20-%3E%20dict%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Heavy%20initialization%3A%20load%20model%2C%20create%20weight%20buffer.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20import%20os%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20self._ready%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20%7B%22status%22%3A%20%22already_ready%22%7D%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Generators%20use%20GPU%201%20%2B%20rank%20(trainer%20uses%20GPU%200)%0A%20%20%20%20%20%20%20%20%20%20%20%20gpu_id%20%3D%201%20%2B%20self.rank%0A%20%20%20%20%20%20%20%20%20%20%20%20os.environ%5B%22CUDA_VISIBLE_DEVICES%22%5D%20%3D%20str(gpu_id)%0A%20%20%20%20%20%20%20%20%20%20%20%20self.device%20%3D%20%22cuda%22%20if%20torch.cuda.is_available()%20else%20%22cpu%22%0A%20%20%20%20%20%20%20%20%20%20%20%20self.policy_version%20%3D%200%0A%20%20%20%20%20%20%20%20%20%20%20%20self.generations%20%3D%200%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BGeneratorWorker%3A%7Bself.rank%7D%5D%20Loading%20model%20%7Bself.model_name%7D%20on%20GPU%20%7Bgpu_id%7D...%22)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.tokenizer%20%3D%20AutoTokenizer.from_pretrained(self.model_name)%0A%20%20%20%20%20%20%20%20%20%20%20%20self.model%20%3D%20AutoModelForCausalLM.from_pretrained(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.model_name%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20torch_dtype%3Dtorch.bfloat16%20if%20self.device%20%3D%3D%20%22cuda%22%20else%20torch.float32%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20).to(self.device)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20self.tokenizer.pad_token%20is%20None%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.tokenizer.pad_token%20%3D%20self.tokenizer.eos_token%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.spec%20%3D%20get_spec(%22compositional%22%2C%20difficulty%3Dself.difficulty%2C%20seed%3D42%20%2B%20self.rank)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self._sync_buf%20%3D%20None%20%20%23%20CPU%20staging%20buffer%20for%20RDMA%20weight%20sync%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self._ready%20%3D%20True%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BGeneratorWorker%3A%7Bself.rank%7D%5D%20Ready%20on%20GPU%20%7Bgpu_id%7D!%22)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20%7B%22status%22%3A%20%22ready%22%2C%20%22rank%22%3A%20self.rank%2C%20%22gpu%22%3A%20gpu_id%7D%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20get_version(self)%20-%3E%20int%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20self.policy_version%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20sync_weights_from_buffer(self%2C%20handle%2C%20param_meta%3A%20dict%2C%20version%3A%20int%2C%20total_bytes%3A%20int)%20-%3E%20bool%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Sync%20weights%20via%20RDMA%20from%20trainer's%20circular%20buffer.%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20Flow%3A%20Trainer%20CPU%20slot%20--RDMA--%3E%20Generator%20CPU%20staging%20--H2D--%3E%20Generator%20GPU%20params%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20NOTE%3A%20Ideally%20we'd%20load%20weights%20from%20the%20trainer's%20CPU%20buffer%20directly%0A%20%20%20%20%20%20%20%20%20%20%20%20into%20the%20model's%20state_dict%20parameters%2C%20avoiding%20the%20intermediate%20copy.%0A%20%20%20%20%20%20%20%20%20%20%20%20We%20hit%20bugs%20with%20RDMABuffer%20targeting%20model%20tensors%20directly%2C%20so%20for%0A%20%20%20%20%20%20%20%20%20%20%20%20now%20we%20read%20into%20a%20separate%20CPU%20buffer%20and%20then%20do%20a%20H2D%20copy.%20Will%20fix.%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20version%20%3C%3D%20self.policy_version%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20False%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Allocate%20CPU%20staging%20buffer%20on%20first%20sync%20(reuse%20thereafter).%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Ideally%20we'd%20load%20from%20the%20trainer's%20CPU%20buffer%20straight%20into%20the%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20model's%20state_dict%20to%20skip%20this%20copy%2C%20but%20we%20hit%20RDMABuffer%20bugs%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20doing%20that%20--%20so%20for%20now%2C%20separate%20CPU%20buffer%20%2B%20H2D%20scatter.%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20self._sync_buf%20is%20None%20or%20self._sync_buf.numel()%20%3C%20total_bytes%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self._sync_buf%20%3D%20torch.empty(total_bytes%2C%20dtype%3Dtorch.uint8)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20RDMA%20read%3A%20trainer%20CPU%20slot%20-%3E%20generator%20CPU%20staging%20buffer%0A%20%20%20%20%20%20%20%20%20%20%20%20byte_view%20%3D%20self._sync_buf%5B%3Atotal_bytes%5D.flatten()%0A%20%20%20%20%20%20%20%20%20%20%20%20handle.read_into(byte_view).get()%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Scatter%20from%20CPU%20staging%20into%20GPU%20model%20params%20(H2D%20copy%20per%20parameter)%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20name%2C%20p%20in%20self.model.named_parameters()%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20off%2C%20shape%2C%20dtype%20%3D%20param_meta%5Bname%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20nbytes%20%3D%20p.numel()%20*%20p.element_size()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20src%20%3D%20self._sync_buf%5Boff%3Aoff%20%2B%20nbytes%5D.view(dtype).view(shape)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20p.data.copy_(src)%0A%20%20%20%20%20%20%20%20%20%20%20%20self.policy_version%20%3D%20version%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20True%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20sync_weights(self%2C%20state_dict%3A%20dict%2C%20version%3A%20int)%20-%3E%20bool%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Sync%20weights%20directly%20(fallback%20when%20RDMA%20unavailable).%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20version%20%3C%3D%20self.policy_version%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return%20False%0A%20%20%20%20%20%20%20%20%20%20%20%20self.model.load_state_dict(state_dict)%0A%20%20%20%20%20%20%20%20%20%20%20%20self.policy_version%20%3D%20version%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20True%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20generate(self%2C%20question%3A%20str%2C%20answer%3A%20int%2C%20max_turns%3A%20int%20%3D%205)%20-%3E%20Trajectory%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Generate%20a%20trajectory%20for%20a%20given%20task%20(used%20for%20examples%2Fdebugging).%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20self.model.eval()%0A%20%20%20%20%20%20%20%20%20%20%20%20task%20%3D%20Task(question%3Dquestion%2C%20correct_answer%3Danswer%2C%20metadata%3D%7B%7D)%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20self._run_generation(task%2C%20max_turns)%0A%0A%20%20%20%20%20%20%20%20%40endpoint%0A%20%20%20%20%20%20%20%20def%20generate_trajectory(self%2C%20max_turns%3A%20int%20%3D%205)%20-%3E%20Trajectory%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Generate%20a%20trajectory%20using%20a%20self-generated%20task.%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20Each%20generator%20has%20its%20own%20seeded%20spec%2C%20so%20broadcasting%20this%20endpoint%0A%20%20%20%20%20%20%20%20%20%20%20%20to%20all%20generators%20produces%20diverse%20trajectories%20from%20different%20tasks.%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20self.model.eval()%0A%20%20%20%20%20%20%20%20%20%20%20%20task%20%3D%20self.spec.generate_task()%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20self._run_generation(task%2C%20max_turns)%0A%0A%20%20%20%20%20%20%20%20def%20_run_generation(self%2C%20task%3A%20Task%2C%20max_turns%3A%20int)%20-%3E%20Trajectory%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Shared%20generation%20logic%3A%20run%20inference%2C%20compute%20tokens%2C%20return%20Trajectory.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20import%20re%20as%20_re%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20result%20%3D%20generate_with_tools(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20self.model%2C%20self.tokenizer%2C%20self.spec%2C%20task%2C%20self.device%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20max_turns%3Dmax_turns%2C%20max_tokens_per_turn%3D150%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20self.generations%20%2B%3D%201%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Build%20model-only%20text%20(generated%20tokens%20without%20injected%20tool%20results)%0A%20%20%20%20%20%20%20%20%20%20%20%20model_only_text%20%3D%20%22%22.join(t.generated_text%20for%20t%20in%20result.turns)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Detect%20%5BANSWER%5D%20tag%20and%20classify%20failure%20mode%0A%20%20%20%20%20%20%20%20%20%20%20%20has_answer_tag%20%3D%20bool(_re.search(r'%5C%5BANSWER%5C%5D'%2C%20result.final_text))%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20result.is_correct%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20failure_mode%20%3D%20%22success%22%0A%20%20%20%20%20%20%20%20%20%20%20%20elif%20not%20has_answer_tag%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20failure_mode%20%3D%20%22wrong_format%22%0A%20%20%20%20%20%20%20%20%20%20%20%20elif%20result.total_tool_calls%20%3E%203%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20failure_mode%20%3D%20%22tool_spam%22%0A%20%20%20%20%20%20%20%20%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20failure_mode%20%3D%20%22wrong_answer%22%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Pre-tokenize%20for%20the%20trainer%3A%20prompt%20%2B%20model_only_text%0A%20%20%20%20%20%20%20%20%20%20%20%20messages%20%3D%20%5B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7B%22role%22%3A%20%22system%22%2C%20%22content%22%3A%20self.spec.get_system_prompt(with_hint%3DTrue)%7D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7B%22role%22%3A%20%22user%22%2C%20%22content%22%3A%20task.question%7D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20prompt_text%20%3D%20self.tokenizer.apply_chat_template(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20messages%2C%20tokenize%3DFalse%2C%20add_generation_prompt%3DTrue%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20prompt_ids%20%3D%20self.tokenizer(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20prompt_text%2C%20return_tensors%3D%22pt%22%2C%20add_special_tokens%3DFalse%0A%20%20%20%20%20%20%20%20%20%20%20%20)%5B%22input_ids%22%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20prompt_length%20%3D%20prompt_ids.shape%5B1%5D%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20full_ids%20%3D%20self.tokenizer(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20prompt_text%20%2B%20model_only_text%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20return_tensors%3D%22pt%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20add_special_tokens%3DFalse%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20truncation%3DTrue%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20max_length%3D1024%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20)%5B%22input_ids%22%5D%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Reward%20shaping%20(see%20NB05%20%22From%20Failure%20Modes%20to%20RL%20Rewards%22)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20%20%20%2B1.0%20for%20correct%20answer%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20%20%20%2B0.2%20for%20format%20compliance%20(%5BANSWER%5D%20tag)%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20%20%20-0.1%20per%20tool%20call%20beyond%202%20(discourages%20tool%20spam)%0A%20%20%20%20%20%20%20%20%20%20%20%20reward%20%3D%200.0%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20result.is_correct%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20reward%20%2B%3D%201.0%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20has_answer_tag%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20reward%20%2B%3D%200.2%0A%20%20%20%20%20%20%20%20%20%20%20%20excess_tools%20%3D%20max(0%2C%20result.total_tool_calls%20-%202)%0A%20%20%20%20%20%20%20%20%20%20%20%20reward%20-%3D%200.1%20*%20excess_tools%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20Trajectory(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20task_question%3Dtask.question%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20task_answer%3Dtask.correct_answer%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20response_text%3Dresult.final_text%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20reward%3Dreward%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20is_correct%3Dresult.is_correct%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20num_turns%3Dlen(result.turns)%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20num_tool_calls%3Dresult.total_tool_calls%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20generator_id%3Dself.rank%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20policy_version%3Dself.policy_version%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20model_only_text%3Dmodel_only_text%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20has_answer_tag%3Dhas_answer_tag%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20failure_mode%3Dfailure_mode%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20input_ids%3Dfull_ids%5B0%5D.tolist()%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20prompt_length%3Dprompt_length%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%0A%20%20%20%20return%20(GeneratorWorker%2C)%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Architecture%20Overview%0A%0A%20%20%20%20Now%20we%20have%20all%20our%20actors%20defined.%20Here's%20how%20they%20connect%20--%20this%20is%20the%0A%20%20%20%20**single-controller%20paradigm**%20from%20%5BNB01%5D(.%2F01_history_and_vision.html)%3A%20the%20notebook%20process%20orchestrates%0A%20%20%20%20everything%2C%20but%20actors%20do%20the%20heavy%20lifting%20on%20their%20own%20GPUs.%0A%0A%20%20%20%20%60%60%60%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%20%20%20%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20%20GeneratorMesh%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%E2%94%82%20%20ZorplexService%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20(ActorMesh)%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%E2%94%82%20%20(Service)%20%20%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%E2%94%82%20%20tool%20%20%20%E2%94%82%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%E2%94%82%20Generator%200%20%20%20%E2%94%82%E2%94%80%E2%94%80%E2%94%BC%E2%94%80calls%E2%94%80%E2%94%80%E2%96%BA%E2%94%82%20%20%E2%94%82%20ZorplexWorker%20%E2%94%82%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%E2%94%82%20Generator%201%20%20%20%E2%94%82%E2%94%80%E2%94%80%E2%94%BC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%96%BA%E2%94%82%20%20%E2%94%82%20ZorplexWorker%20%E2%94%82%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%E2%94%82%E2%97%84%E2%94%80results%E2%94%82%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20%20%20%20%20%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%BC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20trajectories%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20v%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20%20%20%20ReplayBuffer%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%AC%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%E2%94%82%20sample%20batch%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20v%0A%20%20%20%20%E2%94%8C%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%90%0A%20%20%20%20%E2%94%82%20%20%20%20%20%20Trainer%20%20%20%20%20%20%20%20%E2%94%82%0A%20%20%20%20%E2%94%82%20%20(circular%20buffer)%20%20%E2%94%82%E2%94%80%E2%94%80%3E%20RDMA%20weight%20sync%0A%20%20%20%20%E2%94%94%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%80%E2%94%98%20%20%20%20%20%20to%20GeneratorMesh%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20Each%20generator%20calls%20zorplex%20tool%20endpoints%20during%20multi-turn%20inference%0A%20%20%20%20(e.g.%2C%20%60lookup_value%60%2C%20%60compute%60).%20The%20Service%20routes%20these%20calls%20round-robin%0A%20%20%20%20across%20ZorplexWorkers.%0A%0A%20%20%20%20**ActorMesh%20vs%20Service.**%20Generators%20are%20a%20plain%20**ActorMesh**%20--%20we%20address%0A%20%20%20%20them%20directly%20via%20%60.call()%60%20(broadcast%20to%20all)%20or%20%60.slice()%60%20(individual%0A%20%20%20%20access).%20This%20is%20natural%20for%20sync%20RL%20(broadcast%20generate%2C%20then%20train)%20and%0A%20%20%20%20for%20async%20RL%20(each%20thread%20slices%20its%20own%20generator).%20ZorplexWorkers%20are%0A%20%20%20%20wrapped%20in%20a%20**Service**%20(%5BNB06%5D(.%2F06_services.html)%20pattern)%20because%20they're%20stateless%3A%20any%0A%20%20%20%20worker%20can%20handle%20any%20request%2C%20so%20round-robin%20routing%20and%20health%20tracking%0A%20%20%20%20are%20useful.%20In%20production%20async%20RL%2C%20you%20might%20wrap%20generators%20in%20a%20Service%0A%20%20%20%20too%20--%20that%20gives%20you%20auto-scaling%20and%20health%20tracking%20--%20but%20here%20the%0A%20%20%20%20ActorMesh%20is%20simpler%20and%20lets%20us%20demonstrate%20both%20addressing%20patterns.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Sync%20vs%20Async%20RL%0A%0A%20%20%20%20**Sync%20RL**%20(traditional)%3A%0A%20%20%20%20%60%60%60%0A%20%20%20%20%7C--generate--%7C--train--%7C--generate--%7C--train--%7C--generate--%7C--train--%7C%0A%20%20%20%20%60%60%60%0A%20%20%20%20Only%20ONE%20thing%20happens%20at%20a%20time.%20GPU%20sits%20idle%20during%20generation%2C%0A%20%20%20%20generator%20sits%20idle%20during%20training.%0A%0A%20%20%20%20**Async%20RL**%20(what%20we're%20building)%3A%0A%20%20%20%20%60%60%60%0A%20%20%20%20Gen0%3A%20%20%7C%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%7C%0A%20%20%20%20Gen1%3A%20%20%7C%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%E2%96%88%7C%0A%20%20%20%20Train%3A%20%20%20%20%20%20%7C%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%E2%96%93%7C%0A%20%20%20%20%60%60%60%0A%20%20%20%20Everything%20runs%20concurrently.%20More%20data%20collected%2C%20better%20GPU%20utilization.%0A%0A%20%20%20%20We'll%20run%20BOTH%20modes%20with%20the%20**same%20actors**%20and%20compare%20wall%20time%2C%20throughput%2C%0A%20%20%20%20and%20utilization.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20num_steps_slider%20%3D%20mo.ui.slider(10%2C%20100%2C%20value%3D20%2C%20label%3D%22Training%20steps%22)%0A%20%20%20%20num_generators_slider%20%3D%20mo.ui.slider(1%2C%204%2C%20value%3D2%2C%20label%3D%22Generators%22)%0A%0A%20%20%20%20mo.md(f%22%22%22%0A%20%20%20%20%23%23%20Configuration%0A%0A%20%20%20%20Adjust%20parameters%20for%20the%20training%20run.%20**Marimo%20is%20reactive**%3A%20changing%20a%20slider%0A%20%20%20%20re-runs%20all%20downstream%20cells%20that%20depend%20on%20it.%20This%20means%20actors%20will%20be%0A%20%20%20%20re-spawned%20and%20both%20training%20loops%20will%20re-execute%20with%20the%20new%20values.%0A%0A%20%20%20%20%7Bnum_steps_slider%7D%0A%0A%20%20%20%20%7Bnum_generators_slider%7D%0A%0A%20%20%20%20**Batch%20size**%20is%20set%20to%20match%20the%20number%20of%20generators%20--%20each%20training%20step%0A%20%20%20%20trains%20on%20exactly%20one%20round%20of%20generation.%20This%20keeps%20the%20comparison%20fair%3A%0A%20%20%20%20sync%20and%20async%20train%20on%20the%20same%20amount%20of%20data%20per%20step.%0A%0A%20%20%20%20**Suggestions%3A**%20Start%20with%20defaults%20(20%20steps%2C%202%20generators)%20to%20see%20the%0A%20%20%20%20full%20pipeline.%20Then%20try%20increasing%20generators%20to%203-4%20to%20see%20the%20async%20throughput%0A%20%20%20%20advantage%20grow.%20Increasing%20training%20steps%20gives%20the%20model%20more%20updates%20but%20adds%0A%20%20%20%20wall%20time.%20Note%20that%20re-spawning%20actors%20(loading%20models%20onto%20GPUs)%20is%20the%20most%0A%20%20%20%20expensive%20part%20of%20the%20setup%20--%20the%20training%20loops%20themselves%20are%20relatively%20fast.%0A%0A%20%20%20%20**Try%20this%3A**%20Set%20generators%20to%201%20and%20watch%20the%20async%20timeline%20--%20with%20only%20one%0A%20%20%20%20generator%2C%20async%20degrades%20to%20near-sync%20performance%20because%20there's%20no%20parallel%0A%20%20%20%20generation%20to%20overlap%20with%20training.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%20num_generators_slider%2C%20num_steps_slider%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20import%20threading%0A%20%20%20%20import%20time%0A%20%20%20%20from%20dataclasses%20import%20dataclass%2C%20field%0A%0A%20%20%20%20%40dataclass%0A%20%20%20%20class%20TimingEvent%3A%0A%20%20%20%20%20%20%20%20%22%22%22A%20single%20timed%20event%20for%20timeline%20visualization.%22%22%22%0A%20%20%20%20%20%20%20%20actor_id%3A%20str%0A%20%20%20%20%20%20%20%20event_type%3A%20str%20%20%23%20%22generate%22%2C%20%22train%22%2C%20%22sync%22%0A%20%20%20%20%20%20%20%20start_time%3A%20float%0A%20%20%20%20%20%20%20%20duration%3A%20float%0A%0A%20%20%20%20%40dataclass%0A%20%20%20%20class%20TimingStats%3A%0A%20%20%20%20%20%20%20%20%22%22%22Timing%20statistics%20for%20a%20training%20run.%22%22%22%0A%20%20%20%20%20%20%20%20mode%3A%20str%0A%20%20%20%20%20%20%20%20num_generators%3A%20int%0A%20%20%20%20%20%20%20%20num_steps%3A%20int%0A%20%20%20%20%20%20%20%20total_generations%3A%20int%0A%20%20%20%20%20%20%20%20wall_time%3A%20float%0A%20%20%20%20%20%20%20%20gen_times%3A%20list%20%3D%20field(default_factory%3Dlist)%0A%20%20%20%20%20%20%20%20train_times%3A%20list%20%3D%20field(default_factory%3Dlist)%0A%20%20%20%20%20%20%20%20events%3A%20list%20%3D%20field(default_factory%3Dlist)%20%20%23%20List%20of%20TimingEvent%0A%20%20%20%20%20%20%20%20rdma_syncs%3A%20int%20%3D%200%0A%20%20%20%20%20%20%20%20direct_syncs%3A%20int%20%3D%200%0A%20%20%20%20%20%20%20%20staleness%3A%20list%20%3D%20field(default_factory%3Dlist)%20%20%23%20policy_version%20gaps%20per%20train%20batch%0A%0A%20%20%20%20%20%20%20%20%40property%0A%20%20%20%20%20%20%20%20def%20gens_per_second(self)%20-%3E%20float%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20self.total_generations%20%2F%20self.wall_time%20if%20self.wall_time%20%3E%200%20else%200%0A%0A%20%20%20%20%20%20%20%20%40property%0A%20%20%20%20%20%20%20%20def%20steps_per_second(self)%20-%3E%20float%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20self.num_steps%20%2F%20self.wall_time%20if%20self.wall_time%20%3E%200%20else%200%0A%0A%20%20%20%20return%20TimingEvent%2C%20TimingStats%2C%20threading%2C%20time%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Spawning%20and%20Initializing%20Actors%0A%0A%20%20%20%20This%20is%20the%20**single-controller%20paradigm**%20in%20action.%20The%20notebook%20process%0A%20%20%20%20orchestrates%20a%20careful%20initialization%20sequence%3A%0A%0A%20%20%20%201.%20Spawn%20ZorplexWorkers%20via%20a%20**Service**%20(%5BNB06%5D(.%2F06_services.html)%20pattern%20--%20health%20tracking%2C%20round-robin)%0A%20%20%20%202.%20Spawn%20GeneratorWorkers%20as%20a%20plain%20**ActorMesh**%20and%20call%20%60setup()%60%20on%20all%20via%0A%20%20%20%20%20%20%20%60.call()%60%20broadcast%20(loads%20model%20onto%20each%20GPU)%0A%20%20%20%203.%20Spawn%20ReplayBuffer%20(CPU-only%2C%20ready%20immediately)%0A%20%20%20%204.%20Spawn%20Trainer%2C%20then%20call%20%60setup()%60%20(loads%20model%20onto%20GPU%200%2C%20registers%20RDMA%20buffers)%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(%0A%20%20%20%20GeneratorWorker%2C%0A%20%20%20%20ReplayBuffer%2C%0A%20%20%20%20Service%2C%0A%20%20%20%20TrainerActor%2C%0A%20%20%20%20ZorplexWorker%2C%0A%20%20%20%20num_generators_slider%2C%0A%20%20%20%20num_steps_slider%2C%0A%20%20%20%20register_service%2C%0A)%3A%0A%20%20%20%20from%20monarch.actor%20import%20this_host%0A%0A%20%20%20%20NUM_STEPS%20%3D%20num_steps_slider.value%0A%20%20%20%20NUM_GENERATORS%20%3D%20num_generators_slider.value%0A%20%20%20%20NUM_ZORPLEX%20%3D%202%0A%20%20%20%20BATCH_SIZE%20%3D%20NUM_GENERATORS%20%20%23%20Train%20on%20exactly%20one%20round%20of%20generation%20per%20step%0A%0A%20%20%20%20def%20setup_actors()%3A%0A%20%20%20%20%20%20%20%20%22%22%22Spawn%20and%20initialize%20all%20actors.%20Returns%20them%20for%20reuse.%22%22%22%0A%20%20%20%20%20%20%20%20host%20%3D%20this_host()%0A%0A%20%20%20%20%20%20%20%20%23%201.%20ZorplexWorkers%20--%20wrapped%20in%20a%20Service%20(NB06%20pattern)%20for%0A%20%20%20%20%20%20%20%20%23%20%20%20%20health%20tracking%20and%20round-robin%20routing%0A%20%20%20%20%20%20%20%20zorplex_worker_procs%20%3D%20host.spawn_procs(per_host%3D%7B%22procs%22%3A%20NUM_ZORPLEX%7D)%0A%20%20%20%20%20%20%20%20zorplex_svc_procs%20%3D%20host.spawn_procs(per_host%3D%7B%22procs%22%3A%201%7D)%0A%20%20%20%20%20%20%20%20zorplex_svc%20%3D%20zorplex_svc_procs.spawn(%22zorplex_svc%22%2C%20Service%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20service_name%3D%22zorplex%22%2C%20worker_class%3DZorplexWorker%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20procs%3Dzorplex_worker_procs%2C%20procs_per_replica%3D1%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20difficulty%3D%22easy%22)%0A%0A%20%20%20%20%20%20%20%20%23%202.%20Generators%20--%20plain%20ActorMesh%20(no%20Service%20wrapper).%0A%20%20%20%20%20%20%20%20%23%20%20%20%20Each%20generator%20has%20its%20own%20GPU%20and%20model%20copy%3B%20we%20address%20them%0A%20%20%20%20%20%20%20%20%23%20%20%20%20via%20.call()%20broadcast%20or%20.slice()%20for%20individual%20access.%0A%20%20%20%20%20%20%20%20gen_procs%20%3D%20host.spawn_procs(per_host%3D%7B%22procs%22%3A%20NUM_GENERATORS%7D)%0A%20%20%20%20%20%20%20%20generators%20%3D%20gen_procs.spawn(%22generators%22%2C%20GeneratorWorker)%0A%0A%20%20%20%20%20%20%20%20%23%203.%20ReplayBuffer%0A%20%20%20%20%20%20%20%20buffer_procs%20%3D%20host.spawn_procs(per_host%3D%7B%22procs%22%3A%201%7D)%0A%20%20%20%20%20%20%20%20buffer%20%3D%20buffer_procs.spawn(%22buffer%22%2C%20ReplayBuffer%2C%20max_size%3D500)%0A%0A%20%20%20%20%20%20%20%20%23%204.%20Trainer%0A%20%20%20%20%20%20%20%20trainer_procs%20%3D%20host.spawn_procs(per_host%3D%7B%22procs%22%3A%201%7D)%0A%20%20%20%20%20%20%20%20trainer%20%3D%20trainer_procs.spawn(%22trainer%22%2C%20TrainerActor)%0A%0A%20%20%20%20%20%20%20%20%23%20Initialize%20actors%20that%20need%20setup%0A%20%20%20%20%20%20%20%20zorplex_svc.ping.call_one().get()%0A%0A%20%20%20%20%20%20%20%20print(%22%5BSETUP%5D%20Setting%20up%20generator%20workers...%22)%0A%20%20%20%20%20%20%20%20generators.setup.call().get()%20%20%23%20broadcast%20setup%20to%20all%20generators%0A%0A%20%20%20%20%20%20%20%20buffer.stats.call_one().get()%0A%0A%20%20%20%20%20%20%20%20print(%22%5BSETUP%5D%20Setting%20up%20trainer...%22)%0A%20%20%20%20%20%20%20%20trainer.setup.call_one().get()%0A%0A%20%20%20%20%20%20%20%20register_service(%22zorplex%22%2C%20zorplex_svc)%0A%0A%20%20%20%20%20%20%20%20print(f%22%5BSETUP%5D%20All%20actors%20ready!%20%7BNUM_GENERATORS%7D%20generators%2C%20%7BNUM_ZORPLEX%7D%20zorplex%20workers%22)%0A%0A%20%20%20%20%20%20%20%20%23%20Track%20ProcMeshes%20for%20cleanup%0A%20%20%20%20%20%20%20%20proc_meshes%20%3D%20%5Bzorplex_worker_procs%2C%20zorplex_svc_procs%2C%20gen_procs%2C%20buffer_procs%2C%20trainer_procs%5D%0A%0A%20%20%20%20%20%20%20%20return%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%22trainer%22%3A%20trainer%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%22buffer%22%3A%20buffer%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%22generators%22%3A%20generators%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%22zorplex_svc%22%3A%20zorplex_svc%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%22_proc_meshes%22%3A%20proc_meshes%2C%0A%20%20%20%20%20%20%20%20%7D%0A%0A%20%20%20%20def%20teardown_actors(actors)%3A%0A%20%20%20%20%20%20%20%20%22%22%22Stop%20all%20ProcMeshes%2C%20releasing%20processes%20and%20GPU%20memory.%22%22%22%0A%20%20%20%20%20%20%20%20for%20pm%20in%20actors.get(%22_proc_meshes%22%2C%20%5B%5D)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20try%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20pm.stop(%22teardown%20for%20re-init%22).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20except%20Exception%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20pass%20%20%23%20Best-effort%20cleanup%0A%20%20%20%20%20%20%20%20print(%22%5BTEARDOWN%5D%20All%20actors%20stopped.%22)%0A%0A%20%20%20%20actors%20%3D%20setup_actors()%0A%20%20%20%20return%20(%0A%20%20%20%20%20%20%20%20BATCH_SIZE%2C%0A%20%20%20%20%20%20%20%20NUM_GENERATORS%2C%0A%20%20%20%20%20%20%20%20NUM_STEPS%2C%0A%20%20%20%20%20%20%20%20actors%2C%0A%20%20%20%20%20%20%20%20setup_actors%2C%0A%20%20%20%20%20%20%20%20teardown_actors%2C%0A%20%20%20%20)%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Before%20Training%3A%20Zorplex%20Baseline%0A%0A%20%20%20%20Let's%20evaluate%20the%20model%20*before*%20any%20training%20to%20establish%20a%20baseline.%0A%20%20%20%20We%20run%2010%20compositional%20Zorplex%20tasks%20and%20record%20accuracy%2C%20average%20turns%2C%0A%20%20%20%20and%20tool%20usage.%20This%20gives%20us%20a%20concrete%20%22before%22%20snapshot%20to%20compare%20against.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(actors%2C%20mo)%3A%0A%20%20%20%20print(%22Evaluating%20pre-training%20baseline...%22)%0A%20%20%20%20pre_eval%20%3D%20actors%5B%22trainer%22%5D.evaluate_zorplex.call_one(num_samples%3D10%2C%20seed%3D42).get()%0A%0A%20%20%20%20mo.md(f%22%22%22%0A%20%20%20%20%23%23%23%20Pre-Training%20Results%0A%0A%20%20%20%20**Metrics%20refresher**%20(from%20%5BNB05%5D(.%2F05_rl_intro.html))%3A%20*Accuracy*%20is%20how%20often%20the%20model%20gets%20the%0A%20%20%20%20correct%20answer.%20*Format%20compliance*%20tracks%20whether%20it%20emits%20the%20%60%5BANSWER%5D%60%20tag%0A%20%20%20%20we%20trained%20it%20to%20use.%20*Avg%20turns%2Ftool%20calls*%20measure%20how%20many%20interaction%0A%20%20%20%20steps%20the%20model%20takes%20%E2%80%94%20lower%20is%20more%20efficient.%0A%0A%20%20%20%20%7C%20Metric%20%7C%20Value%20%7C%0A%20%20%20%20%7C--------%7C-------%7C%0A%20%20%20%20%7C%20Accuracy%20%7C%20%7Bpre_eval%5B'accuracy'%5D%3A.0%25%7D%20(%7Bpre_eval%5B'correct'%5D%7D%2F%7Bpre_eval%5B'total'%5D%7D)%20%7C%0A%20%20%20%20%7C%20Format%20compliance%20%7C%20%7Bpre_eval%5B'format_rate'%5D%3A.0%25%7D%20%7C%0A%20%20%20%20%7C%20Avg%20turns%20%7C%20%7Bpre_eval%5B'avg_turns'%5D%3A.1f%7D%20%7C%0A%20%20%20%20%7C%20Avg%20tool%20calls%20%7C%20%7Bpre_eval%5B'avg_tools'%5D%3A.1f%7D%20%7C%0A%0A%20%20%20%20**Failure%20mode%20breakdown%3A**%0A%0A%20%20%20%20%7C%20Mode%20%7C%20Count%20%7C%0A%20%20%20%20%7C------%7C-------%7C%0A%20%20%20%20%7C%20Success%20%7C%20%7Bpre_eval%5B'failure_modes'%5D%5B'success'%5D%7D%20%7C%0A%20%20%20%20%7C%20Wrong%20format%20%7C%20%7Bpre_eval%5B'failure_modes'%5D%5B'wrong_format'%5D%7D%20%7C%0A%20%20%20%20%7C%20Tool%20spam%20%7C%20%7Bpre_eval%5B'failure_modes'%5D%5B'tool_spam'%5D%7D%20%7C%0A%20%20%20%20%7C%20Wrong%20answer%20%7C%20%7Bpre_eval%5B'failure_modes'%5D%5B'wrong_answer'%5D%7D%20%7C%0A%0A%20%20%20%20This%20is%20our%20starting%20point.%20Let's%20see%20if%20training%20improves%20it%20at%20all...%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%20(pre_eval%2C)%0A%0A%0A%40app.cell%0Adef%20_(actors%2C%20mo)%3A%0A%20%20%20%20%23%20Generate%20one%20example%20trajectory%20so%20the%20reader%20can%20see%20what%20the%20pipeline%20produces%0A%20%20%20%20_gen%20%3D%20actors%5B%22generators%22%5D.slice(procs%3D0)%0A%20%20%20%20_example_traj%20%3D%20_gen.generate_trajectory.call_one().get()%0A%0A%20%20%20%20_status%20%3D%20%22Correct%22%20if%20_example_traj.is_correct%20else%20%22Wrong%22%0A%20%20%20%20_reward%20%3D%20_example_traj.reward%0A%0A%20%20%20%20%23%20Truncate%20long%20responses%20for%20display%0A%20%20%20%20_resp_display%20%3D%20_example_traj.response_text%5B%3A500%5D%0A%20%20%20%20if%20len(_example_traj.response_text)%20%3E%20500%3A%0A%20%20%20%20%20%20%20%20_resp_display%20%2B%3D%20%22...%22%0A%0A%20%20%20%20mo.md(f%22%22%22%0A%20%20%20%20%23%23%23%20Example%20Trajectory%0A%0A%20%20%20%20Here's%20what%20a%20single%20generation%20looks%20like%20--%20this%20is%20the%20data%20unit%20flowing%0A%20%20%20%20through%20the%20pipeline%3A%0A%0A%20%20%20%20%7C%20Field%20%7C%20Value%20%7C%0A%20%20%20%20%7C-------%7C-------%7C%0A%20%20%20%20%7C%20Question%20%7C%20%7B_example_traj.task_question%5B%3A100%5D%7D...%20%7C%0A%20%20%20%20%7C%20Correct%20answer%20%7C%20%60%7B_example_traj.task_answer%7D%60%20%7C%0A%20%20%20%20%7C%20Result%20%7C%20**%7B_status%7D**%20(reward%3D%7B_reward%3A.2f%7D)%20%7C%0A%20%20%20%20%7C%20Failure%20mode%20%7C%20%60%7B_example_traj.failure_mode%7D%60%20%7C%0A%20%20%20%20%7C%20Format%20(%60%5BANSWER%5D%60%20tag)%20%7C%20%7B%22Yes%22%20if%20_example_traj.has_answer_tag%20else%20%22No%22%7D%20%7C%0A%20%20%20%20%7C%20Turns%20%7C%20%7B_example_traj.num_turns%7D%20%7C%0A%20%20%20%20%7C%20Tool%20calls%20%7C%20%7B_example_traj.num_tool_calls%7D%20%7C%0A%0A%20%20%20%20**Model%20response**%20(first%20500%20chars)%3A%0A%20%20%20%20%60%60%60%0A%20%20%20%20%7B_resp_display%7D%0A%20%20%20%20%60%60%60%0A%0A%20%20%20%20Each%20generator%20produces%20trajectories%20like%20this%2C%20which%20flow%20into%20the%20replay%20buffer%0A%20%20%20%20for%20the%20trainer%20to%20sample%20from.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(NUM_GENERATORS%2C%20NUM_STEPS%2C%20TimingEvent%2C%20TimingStats%2C%20time)%3A%0A%20%20%20%20def%20run_sync_loop(actors)%20-%3E%20TimingStats%3A%0A%20%20%20%20%20%20%20%20%22%22%22%0A%20%20%20%20%20%20%20%20SYNC%20MODE%3A%20Broadcast%20generate%20to%20all%20generators%2C%20then%20train.%0A%20%20%20%20%20%20%20%20Pattern%3A%20generate%20batch%20-%3E%20train%20-%3E%20generate%20batch%20-%3E%20train%20...%0A%0A%20%20%20%20%20%20%20%20Uses%20.call()%20to%20broadcast%20generate_trajectory%20to%20all%20generators%0A%20%20%20%20%20%20%20%20simultaneously.%20Each%20generator%20produces%20a%20different%20trajectory%0A%20%20%20%20%20%20%20%20(different%20seed%2C%20stochastic%20sampling)%2C%20but%20the%20call%20is%20synchronous%20--%0A%20%20%20%20%20%20%20%20we%20wait%20for%20ALL%20generators%20to%20finish%20before%20training.%0A%20%20%20%20%20%20%20%20%22%22%22%0A%20%20%20%20%20%20%20%20print(%22%5Cn%22%20%2B%20%22%3D%22%20*%2060)%0A%20%20%20%20%20%20%20%20print(%22SYNC%20MODE%3A%20Broadcast%20Generate%20-%3E%20Train%22)%0A%20%20%20%20%20%20%20%20print(%22%3D%22%20*%2060)%0A%0A%20%20%20%20%20%20%20%20trainer%20%3D%20actors%5B%22trainer%22%5D%0A%20%20%20%20%20%20%20%20generators%20%3D%20actors%5B%22generators%22%5D%0A%0A%20%20%20%20%20%20%20%20stats%20%3D%20TimingStats(%0A%20%20%20%20%20%20%20%20%20%20%20%20mode%3D%22SYNC%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20num_generators%3DNUM_GENERATORS%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20num_steps%3DNUM_STEPS%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20total_generations%3D0%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20wall_time%3D0%2C%0A%20%20%20%20%20%20%20%20)%0A%0A%20%20%20%20%20%20%20%20baseline%20%3D%200.5%0A%20%20%20%20%20%20%20%20t0%20%3D%20time.perf_counter()%0A%0A%20%20%20%20%20%20%20%20for%20step%20in%20range(NUM_STEPS)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Generate%20trajectories%20--%20broadcast%20to%20ALL%20generators%0A%20%20%20%20%20%20%20%20%20%20%20%20gen_start%20%3D%20time.perf_counter()%0A%20%20%20%20%20%20%20%20%20%20%20%20traj_mesh%20%3D%20generators.generate_trajectory.call().get()%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Collect%20into%20a%20plain%20list%20--%20no%20buffer%20in%20sync%20mode.%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20We%20train%20on%20exactly%20what%20we%20just%20generated%2C%20so%20staleness%20is%200.%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20(Async%20mode%20uses%20the%20buffer%20because%20generators%20and%20trainer%20are%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20decoupled%20in%20time%20--%20that's%20where%20the%20buffer%20matters.)%0A%20%20%20%20%20%20%20%20%20%20%20%20batch%20%3D%20list(traj_mesh.values())%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20gen_time%20%3D%20time.perf_counter()%20-%20gen_start%0A%20%20%20%20%20%20%20%20%20%20%20%20stats.gen_times.append(gen_time)%0A%20%20%20%20%20%20%20%20%20%20%20%20stats.total_generations%20%2B%3D%20NUM_GENERATORS%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Record%20one%20event%20per%20generator%20(they%20ran%20in%20parallel)%0A%20%20%20%20%20%20%20%20%20%20%20%20for%20gi%20in%20range(NUM_GENERATORS)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20stats.events.append(TimingEvent(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20actor_id%3Df%22Gen%7Bgi%7D%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20event_type%3D%22generate%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20start_time%3Dgen_start%20-%20t0%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20duration%3Dgen_time%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20))%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Train%20directly%20on%20the%20batch%20we%20just%20generated%20(no%20buffer)%0A%20%20%20%20%20%20%20%20%20%20%20%20train_start%20%3D%20time.perf_counter()%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20batch%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20metrics%20%3D%20trainer.train_step.call_one(batch%2C%20baseline).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20baseline%20%3D%200.9%20*%20baseline%20%2B%200.1%20*%20metrics.avg_reward%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Staleness%20should%20be%200%3A%20we%20generated%20with%20current%20policy%20and%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20train%20immediately.%20This%20contrasts%20with%20async%20mode.%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20batch_staleness%20%3D%20%5Bmetrics.policy_version%20-%20t.policy_version%20for%20t%20in%20batch%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20stats.staleness.extend(batch_staleness)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Sync%20weights%20to%20all%20generators%20(broadcast)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20try%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20handle%2C%20param_meta%2C%20version%2C%20total_bytes%20%3D%20trainer.get_weight_handle.call_one().get()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20handle%20is%20not%20None%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20generators.sync_weights_from_buffer.call(handle%2C%20param_meta%2C%20version%2C%20total_bytes).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20state_dict%2C%20ver%20%3D%20trainer.get_state_dict.call_one().get()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20generators.sync_weights.call(state_dict%2C%20ver).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20except%20Exception%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20pass%20%20%23%20Non-fatal%3A%20generators%20will%20use%20slightly%20stale%20weights%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20train_time%20%3D%20time.perf_counter()%20-%20train_start%0A%20%20%20%20%20%20%20%20%20%20%20%20stats.train_times.append(train_time)%0A%20%20%20%20%20%20%20%20%20%20%20%20stats.events.append(TimingEvent(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20actor_id%3D%22Train%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20event_type%3D%22train%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20start_time%3Dtrain_start%20-%20t0%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20duration%3Dtrain_time%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20))%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20correct_count%20%3D%20sum(1%20for%20t%20in%20traj_mesh.values()%20if%20t.is_correct)%0A%20%20%20%20%20%20%20%20%20%20%20%20format_count%20%3D%20sum(1%20for%20t%20in%20traj_mesh.values()%20if%20t.has_answer_tag)%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BSYNC%20%7Bstep%20%2B%201%3A2d%7D%5D%20%7Bcorrect_count%7D%2F%7BNUM_GENERATORS%7D%20correct%20%22%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20f%22%7Bformat_count%7D%2F%7BNUM_GENERATORS%7D%20formatted%20%22%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20f%22gen%3D%7Bgen_time%20*%201000%3A.0f%7Dms%20train%3D%7Btrain_time%20*%201000%3A.0f%7Dms%22)%0A%0A%20%20%20%20%20%20%20%20stats.wall_time%20%3D%20time.perf_counter()%20-%20t0%0A%20%20%20%20%20%20%20%20return%20stats%0A%0A%20%20%20%20return%20(run_sync_loop%2C)%0A%0A%0A%40app.cell%0Adef%20_(%0A%20%20%20%20BATCH_SIZE%2C%0A%20%20%20%20NUM_GENERATORS%2C%0A%20%20%20%20NUM_STEPS%2C%0A%20%20%20%20TimingEvent%2C%0A%20%20%20%20TimingStats%2C%0A%20%20%20%20threading%2C%0A%20%20%20%20time%2C%0A)%3A%0A%20%20%20%20def%20run_async_loop(actors)%20-%3E%20TimingStats%3A%0A%20%20%20%20%20%20%20%20%22%22%22%0A%20%20%20%20%20%20%20%20ASYNC%20MODE%3A%20All%20generators%20running%20concurrently%20with%20trainer.%0A%20%20%20%20%20%20%20%20-%201%20thread%20per%20generator%20(each%20uses%20.slice()%20to%20address%20its%20generator)%0A%20%20%20%20%20%20%20%20-%20Training%20in%20main%20thread%0A%20%20%20%20%20%20%20%20-%20Each%20generator%20pulls%20latest%20weights%20before%20each%20trajectory%0A%0A%20%20%20%20%20%20%20%20Uses%20try%2Fexcept%20pattern%20from%20NB03%20for%20fault%20tolerance%20in%20generation%20loops.%0A%20%20%20%20%20%20%20%20%22%22%22%0A%20%20%20%20%20%20%20%20print(%22%5Cn%22%20%2B%20%22%3D%22%20*%2060)%0A%20%20%20%20%20%20%20%20print(f%22ASYNC%20MODE%3A%20%7BNUM_GENERATORS%7D%20Generators%20%2B%201%20Trainer%20(Concurrent)%22)%0A%20%20%20%20%20%20%20%20print(%22%3D%22%20*%2060)%0A%0A%20%20%20%20%20%20%20%20trainer%20%3D%20actors%5B%22trainer%22%5D%0A%20%20%20%20%20%20%20%20buffer%20%3D%20actors%5B%22buffer%22%5D%0A%20%20%20%20%20%20%20%20generators%20%3D%20actors%5B%22generators%22%5D%0A%0A%20%20%20%20%20%20%20%20stats%20%3D%20TimingStats(%0A%20%20%20%20%20%20%20%20%20%20%20%20mode%3D%22ASYNC%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20num_generators%3DNUM_GENERATORS%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20num_steps%3DNUM_STEPS%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20total_generations%3D0%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20wall_time%3D0%2C%0A%20%20%20%20%20%20%20%20)%0A%0A%20%20%20%20%20%20%20%20lock%20%3D%20threading.Lock()%0A%20%20%20%20%20%20%20%20stop_flag%20%3D%20threading.Event()%0A%20%20%20%20%20%20%20%20t0%20%3D%20time.perf_counter()%0A%0A%20%20%20%20%20%20%20%20def%20generation_loop(gen_idx)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%22%22%22Each%20generator%20gets%20its%20own%20thread%2C%20using%20.slice()%20for%20individual%20access.%22%22%22%0A%20%20%20%20%20%20%20%20%20%20%20%20gen%20%3D%20generators.slice(procs%3Dgen_idx)%0A%20%20%20%20%20%20%20%20%20%20%20%20while%20not%20stop_flag.is_set()%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20gen_start%20%3D%20time.perf_counter()%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20try%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Pull%20latest%20weights%20before%20generating.%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20sync_weights_from_buffer%20short-circuits%20if%20version%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20hasn't%20changed%2C%20so%20this%20is%20cheap%20when%20there's%20nothing%20new.%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20handle%2C%20param_meta%2C%20version%2C%20total_bytes%20%3D%20trainer.get_weight_handle.call_one().get()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20handle%20is%20not%20None%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20synced%20%3D%20gen.sync_weights_from_buffer.call_one(handle%2C%20param_meta%2C%20version%2C%20total_bytes).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20synced%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20with%20lock%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20stats.rdma_syncs%20%2B%3D%201%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20state_dict%2C%20ver%20%3D%20trainer.get_state_dict.call_one().get()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20synced%20%3D%20gen.sync_weights.call_one(state_dict%2C%20ver).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20synced%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20with%20lock%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20stats.direct_syncs%20%2B%3D%201%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Generate%20trajectory%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20traj%20%3D%20gen.generate_trajectory.call_one().get()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20buffer.add.call_one(traj).get()%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20gen_time%20%3D%20time.perf_counter()%20-%20gen_start%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20with%20lock%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20stats.gen_times.append(gen_time)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20stats.total_generations%20%2B%3D%201%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20count%20%3D%20stats.total_generations%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20stats.events.append(TimingEvent(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20actor_id%3Df%22Gen%7Bgen_idx%7D%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20event_type%3D%22generate%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20start_time%3Dgen_start%20-%20t0%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20duration%3Dgen_time%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20))%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20status%20%3D%20%22correct%22%20if%20traj.is_correct%20else%20traj.failure_mode%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BGEN%7Bgen_idx%7D%20%23%7Bcount%3A2d%7D%5D%20%7Bstatus%7D%20gen%3D%7Bgen_time%20*%201000%3A.0f%7Dms%22)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20except%20Exception%20as%20e%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20try%2Fexcept%20pattern%20from%20NB03%20--%20log%20and%20continue%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BGEN%7Bgen_idx%7D%5D%20Error%3A%20%7Be%7D%2C%20retrying...%22)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20continue%0A%0A%20%20%20%20%20%20%20%20%23%20Start%201%20thread%20per%20generator%2C%20each%20using%20.slice()%20for%20its%20worker%0A%20%20%20%20%20%20%20%20gen_threads%20%3D%20%5B%5D%0A%20%20%20%20%20%20%20%20for%20idx%20in%20range(NUM_GENERATORS)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20t%20%3D%20threading.Thread(target%3Dgeneration_loop%2C%20args%3D(idx%2C)%2C%20daemon%3DTrue)%0A%20%20%20%20%20%20%20%20%20%20%20%20t.start()%0A%20%20%20%20%20%20%20%20%20%20%20%20gen_threads.append(t)%0A%0A%20%20%20%20%20%20%20%20%23%20Training%20in%20main%20thread%0A%20%20%20%20%20%20%20%20train_steps_done%20%3D%200%0A%20%20%20%20%20%20%20%20baseline%20%3D%200.5%0A%0A%20%20%20%20%20%20%20%20while%20train_steps_done%20%3C%20NUM_STEPS%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%23%20Wait%20for%20enough%20samples%0A%20%20%20%20%20%20%20%20%20%20%20%20while%20True%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20size%20%3D%20buffer.size.call_one().get()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20size%20%3E%3D%20BATCH_SIZE%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20break%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20time.sleep(0.02)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20train_start%20%3D%20time.perf_counter()%0A%20%20%20%20%20%20%20%20%20%20%20%20batch%20%3D%20buffer.sample.call_one(BATCH_SIZE).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20batch%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20metrics%20%3D%20trainer.train_step.call_one(batch%2C%20baseline).get()%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20baseline%20%3D%200.9%20*%20baseline%20%2B%200.1%20*%20metrics.avg_reward%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20Measure%20staleness%3A%20in%20async%20mode%2C%20trajectories%20may%20have%20been%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20generated%20with%20an%20older%20policy%20version.%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20batch_staleness%20%3D%20%5Bmetrics.policy_version%20-%20t.policy_version%20for%20t%20in%20batch%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20with%20lock%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20stats.staleness.extend(batch_staleness)%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20train_time%20%3D%20time.perf_counter()%20-%20train_start%0A%20%20%20%20%20%20%20%20%20%20%20%20with%20lock%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20stats.train_times.append(train_time)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20stats.events.append(TimingEvent(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20actor_id%3D%22Train%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20event_type%3D%22train%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20start_time%3Dtrain_start%20-%20t0%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20duration%3Dtrain_time%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20))%0A%20%20%20%20%20%20%20%20%20%20%20%20train_steps_done%20%2B%3D%201%0A%0A%20%20%20%20%20%20%20%20%20%20%20%20print(f%22%5BTRAIN%20%7Btrain_steps_done%3A2d%7D%5D%20time%3D%7Btrain_time%20*%201000%3A.0f%7Dms%20buffer%3D%7Bsize%7D%22)%0A%0A%20%20%20%20%20%20%20%20stop_flag.set()%0A%0A%20%20%20%20%20%20%20%20for%20t%20in%20gen_threads%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20t.join(timeout%3D2.0)%0A%0A%20%20%20%20%20%20%20%20stats.wall_time%20%3D%20time.perf_counter()%20-%20t0%0A%20%20%20%20%20%20%20%20return%20stats%0A%0A%20%20%20%20return%20(run_async_loop%2C)%0A%0A%0A%40app.cell%0Adef%20_(actors%2C%20run_sync_loop)%3A%0A%20%20%20%20sync_stats%20%3D%20run_sync_loop(actors)%0A%20%20%20%20print(f%22%5CnSync%20complete%3A%20%7Bsync_stats.wall_time%3A.2f%7Ds%2C%20%22%0A%20%20%20%20%20%20%20%20%20%20f%22%7Bsync_stats.total_generations%7D%20generations%2C%20%22%0A%20%20%20%20%20%20%20%20%20%20f%22%7Bsync_stats.gens_per_second%3A.2f%7D%20gens%2Fs%22)%0A%0A%20%20%20%20%23%20Evaluate%20immediately%20after%20sync%20training%0A%20%20%20%20print(%22Evaluating%20post-sync%20performance...%22)%0A%20%20%20%20sync_post_eval%20%3D%20actors%5B%22trainer%22%5D.evaluate_zorplex.call_one(num_samples%3D10%2C%20seed%3D42).get()%0A%20%20%20%20print(f%22Post-sync%20accuracy%3A%20%7Bsync_post_eval%5B'accuracy'%5D%3A.0%25%7D%22)%0A%20%20%20%20return%20sync_post_eval%2C%20sync_stats%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%20Re-initializing%20for%20Async%0A%0A%20%20%20%20To%20compare%20fairly%2C%20we%20tear%20down%20all%20actors%20and%20re-spawn%20from%20scratch%20so%20async%0A%20%20%20%20training%20starts%20from%20the%20same%20untrained%20baseline.%20%60ProcMesh.stop()%60%20releases%0A%20%20%20%20the%20processes%20and%20frees%20GPU%20memory%20before%20we%20spawn%20fresh%20ones.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(actors%2C%20run_async_loop%2C%20setup_actors%2C%20teardown_actors)%3A%0A%20%20%20%20%23%20Tear%20down%20sync%20actors%20to%20free%20GPU%20memory%0A%20%20%20%20teardown_actors(actors)%0A%0A%20%20%20%20%23%20Re-spawn%20everything%20so%20async%20starts%20from%20the%20same%20untrained%20baseline%0A%20%20%20%20print(%22Re-spawning%20actors%20for%20async%20run...%22)%0A%20%20%20%20async_actors%20%3D%20setup_actors()%0A%20%20%20%20print(%22Actors%20re-initialized.%20Starting%20async%20loop...%22)%0A%0A%20%20%20%20async_stats%20%3D%20run_async_loop(async_actors)%0A%20%20%20%20print(f%22%5CnAsync%20complete%3A%20%7Basync_stats.wall_time%3A.2f%7Ds%2C%20%22%0A%20%20%20%20%20%20%20%20%20%20f%22%7Basync_stats.total_generations%7D%20generations%2C%20%22%0A%20%20%20%20%20%20%20%20%20%20f%22%7Basync_stats.gens_per_second%3A.2f%7D%20gens%2Fs%22)%0A%0A%20%20%20%20%23%20Evaluate%20immediately%20after%20async%20training%0A%20%20%20%20print(%22Evaluating%20post-async%20performance...%22)%0A%20%20%20%20async_post_eval%20%3D%20async_actors%5B%22trainer%22%5D.evaluate_zorplex.call_one(num_samples%3D10%2C%20seed%3D42).get()%0A%20%20%20%20print(f%22Post-async%20accuracy%3A%20%7Basync_post_eval%5B'accuracy'%5D%3A.0%25%7D%22)%0A%20%20%20%20return%20async_post_eval%2C%20async_stats%0A%0A%0A%40app.cell%0Adef%20_(async_stats%2C%20mo%2C%20sync_stats)%3A%0A%20%20%20%20def%20_build_comparison(sync_s%2C%20async_s)%20-%3E%20str%3A%0A%20%20%20%20%20%20%20%20speedup%20%3D%20sync_s.wall_time%20%2F%20async_s.wall_time%20if%20async_s.wall_time%20%3E%200%20else%200%0A%20%20%20%20%20%20%20%20gen_ratio%20%3D%20async_s.gens_per_second%20%2F%20sync_s.gens_per_second%20if%20sync_s.gens_per_second%20%3E%200%20else%200%0A%0A%20%20%20%20%20%20%20%20avg_sync_gen%20%3D%20sum(sync_s.gen_times)%20%2F%20len(sync_s.gen_times)%20*%201000%20if%20sync_s.gen_times%20else%200%0A%20%20%20%20%20%20%20%20avg_async_gen%20%3D%20sum(async_s.gen_times)%20%2F%20len(async_s.gen_times)%20*%201000%20if%20async_s.gen_times%20else%200%0A%20%20%20%20%20%20%20%20avg_sync_train%20%3D%20sum(sync_s.train_times)%20%2F%20len(sync_s.train_times)%20*%201000%20if%20sync_s.train_times%20else%200%0A%20%20%20%20%20%20%20%20avg_async_train%20%3D%20sum(async_s.train_times)%20%2F%20len(async_s.train_times)%20*%201000%20if%20async_s.train_times%20else%200%0A%0A%20%20%20%20%20%20%20%20async_syncs%20%3D%20async_s.rdma_syncs%20%2B%20async_s.direct_syncs%0A%0A%20%20%20%20%20%20%20%20return%20f%22%22%22%0A%20%20%20%20%23%23%20Sync%20vs%20Async%20Comparison%0A%0A%20%20%20%20%7C%20Metric%20%7C%20SYNC%20%7C%20ASYNC%20%7C%20Ratio%20%7C%0A%20%20%20%20%7C--------%7C------%7C-------%7C-------%7C%0A%20%20%20%20%7C%20Wall%20time%20%7C%20%7Bsync_s.wall_time%3A.2f%7Ds%20%7C%20%7Basync_s.wall_time%3A.2f%7Ds%20%7C%20**%7Bspeedup%3A.2f%7Dx**%20speedup%20%7C%0A%20%20%20%20%7C%20Generations%20%7C%20%7Bsync_s.total_generations%7D%20%7C%20%7Basync_s.total_generations%7D%20%7C%20%7Basync_s.total_generations%20%2F%20max(sync_s.total_generations%2C%201)%3A.1f%7Dx%20%7C%0A%20%20%20%20%7C%20Gens%2Fsecond%20%7C%20%7Bsync_s.gens_per_second%3A.2f%7D%20%7C%20%7Basync_s.gens_per_second%3A.2f%7D%20%7C%20**%7Bgen_ratio%3A.1f%7Dx**%20throughput%20%7C%0A%20%20%20%20%7C%20Avg%20gen%20time%20%7C%20%7Bavg_sync_gen%3A.0f%7Dms%20%7C%20%7Bavg_async_gen%3A.0f%7Dms%20%7C%20%7C%0A%20%20%20%20%7C%20Avg%20train%20time%20%7C%20%7Bavg_sync_train%3A.0f%7Dms%20%7C%20%7Bavg_async_train%3A.0f%7Dms%20%7C%20%7C%0A%20%20%20%20%7C%20Weight%20syncs%20%7C%20%7Bsync_s.total_generations%7D%20(every%20step)%20%7C%20%7Basync_syncs%7D%20(per-generator)%20%7C%20%7C%0A%0A%20%20%20%20%23%23%23%20Key%20Observations%0A%0A%20%20%20%20-%20**Data%20throughput**%3A%20Async%20collected%20**%7Bgen_ratio%3A.1f%7Dx**%20more%20trajectories%20per%20second.%0A%20%20%20%20%20%20More%20data%20means%20better%20gradient%20estimates.%0A%20%20%20%20-%20**GPU%20utilization**%3A%20In%20sync%20mode%2C%20the%20trainer%20GPU%20sits%20idle%20during%20generation%20and%0A%20%20%20%20%20%20vice%20versa.%20Async%20keeps%20both%20busy.%0A%20%20%20%20-%20**Generators%20ran%20in%20parallel**%3A%20%7Basync_s.num_generators%7D%20generators%20each%20had%20their%20own%0A%20%20%20%20%20%20thread%2C%20producing%20data%20independently.%0A%20%20%20%20-%20The%20trainer%20consumed%20from%20the%20replay%20buffer%20continuously%2C%20never%20waiting%20for%20a%20specific%0A%20%20%20%20%20%20generator%20to%20finish.%0A%0A%20%20%20%20In%20production%20with%20more%20generators%2C%20the%20throughput%20advantage%20grows%20further.%0A%20%20%20%20%22%22%22%0A%0A%20%20%20%20comparison_md%20%3D%20_build_comparison(sync_stats%2C%20async_stats)%0A%20%20%20%20mo.md(comparison_md)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(async_stats%2C%20mo%2C%20sync_stats)%3A%0A%20%20%20%20import%20matplotlib.pyplot%20as%20plt%0A%20%20%20%20import%20matplotlib.patches%20as%20mpatches%0A%0A%20%20%20%20def%20_plot_timeline(stats%2C%20ax%2C%20title)%3A%0A%20%20%20%20%20%20%20%20%22%22%22Plot%20a%20Gantt%20chart%20of%20timing%20events.%22%22%22%0A%20%20%20%20%20%20%20%20color_map%20%3D%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%22generate%22%3A%20%22%234CAF50%22%2C%20%20%23%20green%0A%20%20%20%20%20%20%20%20%20%20%20%20%22train%22%3A%20%22%23E91E63%22%2C%20%20%20%20%20%23%20pink%0A%20%20%20%20%20%20%20%20%20%20%20%20%22sync%22%3A%20%22%239C27B0%22%2C%20%20%20%20%20%20%23%20purple%0A%20%20%20%20%20%20%20%20%7D%0A%0A%20%20%20%20%20%20%20%20%23%20Collect%20unique%20actor%20IDs%20and%20assign%20y%20positions%0A%20%20%20%20%20%20%20%20actor_ids%20%3D%20%5B%5D%0A%20%20%20%20%20%20%20%20for%20ev%20in%20stats.events%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20ev.actor_id%20not%20in%20actor_ids%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20actor_ids.append(ev.actor_id)%0A%0A%20%20%20%20%20%20%20%20%23%20Sort%3A%20Gen0%2C%20Gen1%2C%20...%2C%20Train%2C%20Sync%0A%20%20%20%20%20%20%20%20gen_ids%20%3D%20sorted(%5Ba%20for%20a%20in%20actor_ids%20if%20a.startswith(%22Gen%22)%5D)%0A%20%20%20%20%20%20%20%20other_ids%20%3D%20%5Ba%20for%20a%20in%20%5B%22Train%22%2C%20%22Sync%22%5D%20if%20a%20in%20actor_ids%5D%0A%20%20%20%20%20%20%20%20actor_ids%20%3D%20gen_ids%20%2B%20other_ids%0A%0A%20%20%20%20%20%20%20%20y_map%20%3D%20%7Baid%3A%20i%20for%20i%2C%20aid%20in%20enumerate(actor_ids)%7D%0A%0A%20%20%20%20%20%20%20%20for%20ev%20in%20stats.events%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20ev.actor_id%20in%20y_map%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20y%20%3D%20y_map%5Bev.actor_id%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20color%20%3D%20color_map.get(ev.event_type%2C%20%22%23999999%22)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20ax.barh(y%2C%20ev.duration%2C%20left%3Dev.start_time%2C%20height%3D0.6%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20color%3Dcolor%2C%20alpha%3D0.8%2C%20edgecolor%3D%22white%22%2C%20linewidth%3D0.5)%0A%0A%20%20%20%20%20%20%20%20ax.set_yticks(range(len(actor_ids)))%0A%20%20%20%20%20%20%20%20ax.set_yticklabels(actor_ids)%0A%20%20%20%20%20%20%20%20ax.set_xlabel(%22Wall%20time%20(seconds)%22)%0A%20%20%20%20%20%20%20%20ax.set_title(title)%0A%20%20%20%20%20%20%20%20ax.invert_yaxis()%0A%0A%20%20%20%20fig%2C%20(ax1%2C%20ax2)%20%3D%20plt.subplots(2%2C%201%2C%20figsize%3D(12%2C%206)%2C%20sharex%3DFalse)%0A%0A%20%20%20%20_plot_timeline(sync_stats%2C%20ax1%2C%20f%22SYNC%20(%7Bsync_stats.wall_time%3A.1f%7Ds)%22)%0A%20%20%20%20_plot_timeline(async_stats%2C%20ax2%2C%20f%22ASYNC%20(%7Basync_stats.wall_time%3A.1f%7Ds)%22)%0A%0A%20%20%20%20%23%20Legend%0A%20%20%20%20legend_patches%20%3D%20%5B%0A%20%20%20%20%20%20%20%20mpatches.Patch(color%3D%22%234CAF50%22%2C%20label%3D%22Generate%22)%2C%0A%20%20%20%20%20%20%20%20mpatches.Patch(color%3D%22%23E91E63%22%2C%20label%3D%22Train%22)%2C%0A%20%20%20%20%5D%0A%20%20%20%20fig.legend(handles%3Dlegend_patches%2C%20loc%3D%22upper%20right%22%2C%20framealpha%3D0.9)%0A%0A%20%20%20%20plt.tight_layout()%0A%20%20%20%20_timeline_desc%20%3D%20mo.md(%22%22%22%23%23%23%20Timeline%20Visualization%0A%0A%20%20%20%20The%20Gantt%20charts%20below%20show%20what%20each%20actor%20was%20doing%20over%20time.%20In%20sync%20mode%2C%0A%20%20%20%20bars%20are%20strictly%20sequential%20--%20notice%20the%20gaps%20between%20generation%20and%20training%20bars.%0A%20%20%20%20In%20async%20mode%2C%20generators%20and%20trainer%20overlap%20--%20that%20overlap%20is%20where%20the%20throughput%0A%20%20%20%20gain%20comes%20from.%0A%0A%20%20%20%20**Try%20this%3A**%20Look%20at%20the%20sync%20chart%20and%20count%20the%20idle%20gaps.%20Each%20gap%20is%20wasted%20GPU%0A%20%20%20%20time.%20Then%20look%20at%20the%20async%20chart%20--%20the%20trainer%20bar%20starts%20almost%20immediately%20because%0A%20%20%20%20generators%20are%20pre-filling%20the%20buffer%20concurrently.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20mo.vstack(%5B_timeline_desc%2C%20fig%5D)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(async_stats%2C%20mo%2C%20sync_stats)%3A%0A%20%20%20%20def%20_avg(lst)%3A%0A%20%20%20%20%20%20%20%20return%20sum(lst)%20%2F%20len(lst)%20if%20lst%20else%200.0%0A%0A%20%20%20%20_sync_avg%20%3D%20_avg(sync_stats.staleness)%0A%20%20%20%20_async_avg%20%3D%20_avg(async_stats.staleness)%0A%20%20%20%20_async_max%20%3D%20max(async_stats.staleness)%20if%20async_stats.staleness%20else%200%0A%0A%20%20%20%20mo.md(f%22%22%22%0A%20%20%20%20%23%23%23%20Policy%20Staleness%3A%20The%20Cost%20of%20Async%0A%0A%20%20%20%20Async%20mode%20gives%20us%20better%20hardware%20utilization%2C%20but%20there's%20a%20trade-off%3A%0A%20%20%20%20**policy%20staleness**.%20Generators%20produce%20trajectories%20using%20an%20older%20version%0A%20%20%20%20of%20the%20policy%20while%20the%20trainer%20has%20already%20moved%20on.%20This%20is%20*off-policy*%0A%20%20%20%20data%20--%20the%20log-probabilities%20computed%20during%20training%20don't%20match%20the%20policy%0A%20%20%20%20that%20generated%20the%20trajectory.%0A%0A%20%20%20%20We%20measure%20staleness%20as%20%60trainer_version%20-%20trajectory_version%60%20at%20each%0A%20%20%20%20training%20step%3A%0A%0A%20%20%20%20%7C%20Metric%20%7C%20SYNC%20%7C%20ASYNC%20%7C%0A%20%20%20%20%7C--------%7C------%7C-------%7C%0A%20%20%20%20%7C%20Avg%20staleness%20%7C%20%7B_sync_avg%3A.1f%7D%20%7C%20%7B_async_avg%3A.1f%7D%20%7C%0A%20%20%20%20%7C%20Max%20staleness%20%7C%20%7Bmax(sync_stats.staleness)%20if%20sync_stats.staleness%20else%200%7D%20%7C%20%7B_async_max%7D%20%7C%0A%0A%20%20%20%20Sync%20mode%20shows%20~0%20staleness%20because%20we%20sync%20weights%20to%20generators%20after%0A%20%20%20%20every%20training%20step.%20Async%20mode%20shows%20%3E0%20because%20generators%20keep%20producing%0A%20%20%20%20with%20older%20weights%20while%20the%20trainer%20advances.%0A%0A%20%20%20%20With%20REINFORCE%2C%20this%20introduces%20some%20bias.%20More%20sophisticated%20algorithms%0A%20%20%20%20(PPO%2C%20GRPO)%20address%20this%20with%20importance%20sampling%20ratios%20(%60pi_new%20%2F%20pi_old%60)%0A%20%20%20%20and%20clipping%2C%20but%20that's%20beyond%20our%20scope%20here.%20For%20a%20small%20model%20with%20few%0A%20%20%20%20steps%2C%20the%20staleness%20is%20mild%20--%20and%20the%20throughput%20gain%20from%20async%20more%20than%0A%20%20%20%20compensates.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20After%20Training%3A%20Did%20It%20Improve%3F%0A%0A%20%20%20%20We%20ran%20sync%20and%20async%20training%20**independently**%20--%20each%20started%20from%20the%20same%0A%20%20%20%20untrained%20model%20(we%20re-spawned%20actors%20between%20runs).%20This%20lets%20us%20compare%0A%20%20%20%20both%20the%20throughput%20characteristics%20(above)%20and%20the%20training%20outcomes.%0A%0A%20%20%20%20Note%3A%20We're%20using%20a%20small%20model%20(0.5B)%20with%20few%20training%20steps%2C%20so%20dramatic%0A%20%20%20%20improvement%20isn't%20guaranteed.%20The%20point%20is%20the%20*infrastructure*%20--%20showing%20that%0A%20%20%20%20the%20full%20loop%20works%20end%20to%20end.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(async_post_eval%2C%20mo%2C%20pre_eval%2C%20sync_post_eval)%3A%0A%20%20%20%20def%20_delta(post%2C%20pre%2C%20key)%3A%0A%20%20%20%20%20%20%20%20return%20post%5Bkey%5D%20-%20pre%5Bkey%5D%0A%0A%20%20%20%20def%20_dir(delta)%3A%0A%20%20%20%20%20%20%20%20if%20delta%20%3E%200%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20%22improved%22%0A%20%20%20%20%20%20%20%20elif%20delta%20%3D%3D%200%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20%22unchanged%22%0A%20%20%20%20%20%20%20%20return%20%22decreased%22%0A%0A%20%20%20%20_sync_acc_d%20%3D%20_delta(sync_post_eval%2C%20pre_eval%2C%20%22accuracy%22)%0A%20%20%20%20_async_acc_d%20%3D%20_delta(async_post_eval%2C%20pre_eval%2C%20%22accuracy%22)%0A%20%20%20%20_sync_fmt_d%20%3D%20_delta(sync_post_eval%2C%20pre_eval%2C%20%22format_rate%22)%0A%20%20%20%20_async_fmt_d%20%3D%20_delta(async_post_eval%2C%20pre_eval%2C%20%22format_rate%22)%0A%0A%20%20%20%20_pre_fm%20%3D%20pre_eval%5B%22failure_modes%22%5D%0A%20%20%20%20_sync_fm%20%3D%20sync_post_eval%5B%22failure_modes%22%5D%0A%20%20%20%20_async_fm%20%3D%20async_post_eval%5B%22failure_modes%22%5D%0A%0A%20%20%20%20mo.md(f%22%22%22%0A%20%20%20%20%23%23%23%20Training%20Results%3A%20Baseline%20vs%20Sync%20vs%20Async%0A%0A%20%20%20%20Both%20runs%20started%20from%20the%20same%20untrained%20model%20and%20ran%20for%20the%20same%20number%0A%20%20%20%20of%20training%20steps.%0A%0A%20%20%20%20%7C%20Metric%20%7C%20Baseline%20%7C%20After%20Sync%20%7C%20After%20Async%20%7C%0A%20%20%20%20%7C--------%7C----------%7C------------%7C-------------%7C%0A%20%20%20%20%7C%20Accuracy%20%7C%20%7Bpre_eval%5B'accuracy'%5D%3A.0%25%7D%20%7C%20%7Bsync_post_eval%5B'accuracy'%5D%3A.0%25%7D%20(%7B_sync_acc_d%3A%2B.0%25%7D)%20%7C%20%7Basync_post_eval%5B'accuracy'%5D%3A.0%25%7D%20(%7B_async_acc_d%3A%2B.0%25%7D)%20%7C%0A%20%20%20%20%7C%20Format%20compliance%20%7C%20%7Bpre_eval%5B'format_rate'%5D%3A.0%25%7D%20%7C%20%7Bsync_post_eval%5B'format_rate'%5D%3A.0%25%7D%20(%7B_sync_fmt_d%3A%2B.0%25%7D)%20%7C%20%7Basync_post_eval%5B'format_rate'%5D%3A.0%25%7D%20(%7B_async_fmt_d%3A%2B.0%25%7D)%20%7C%0A%20%20%20%20%7C%20Avg%20turns%20%7C%20%7Bpre_eval%5B'avg_turns'%5D%3A.1f%7D%20%7C%20%7Bsync_post_eval%5B'avg_turns'%5D%3A.1f%7D%20%7C%20%7Basync_post_eval%5B'avg_turns'%5D%3A.1f%7D%20%7C%0A%20%20%20%20%7C%20Avg%20tool%20calls%20%7C%20%7Bpre_eval%5B'avg_tools'%5D%3A.1f%7D%20%7C%20%7Bsync_post_eval%5B'avg_tools'%5D%3A.1f%7D%20%7C%20%7Basync_post_eval%5B'avg_tools'%5D%3A.1f%7D%20%7C%0A%0A%20%20%20%20**Failure%20mode%20breakdown%3A**%0A%0A%20%20%20%20%7C%20Mode%20%7C%20Baseline%20%7C%20After%20Sync%20%7C%20After%20Async%20%7C%0A%20%20%20%20%7C------%7C----------%7C------------%7C-------------%7C%0A%20%20%20%20%7C%20Success%20%7C%20%7B_pre_fm%5B'success'%5D%7D%20%7C%20%7B_sync_fm%5B'success'%5D%7D%20%7C%20%7B_async_fm%5B'success'%5D%7D%20%7C%0A%20%20%20%20%7C%20Wrong%20format%20%7C%20%7B_pre_fm%5B'wrong_format'%5D%7D%20%7C%20%7B_sync_fm%5B'wrong_format'%5D%7D%20%7C%20%7B_async_fm%5B'wrong_format'%5D%7D%20%7C%0A%20%20%20%20%7C%20Tool%20spam%20%7C%20%7B_pre_fm%5B'tool_spam'%5D%7D%20%7C%20%7B_sync_fm%5B'tool_spam'%5D%7D%20%7C%20%7B_async_fm%5B'tool_spam'%5D%7D%20%7C%0A%20%20%20%20%7C%20Wrong%20answer%20%7C%20%7B_pre_fm%5B'wrong_answer'%5D%7D%20%7C%20%7B_sync_fm%5B'wrong_answer'%5D%7D%20%7C%20%7B_async_fm%5B'wrong_answer'%5D%7D%20%7C%0A%0A%20%20%20%20Sync%20accuracy%20%7B_dir(_sync_acc_d)%7D%20by%20%7Babs(_sync_acc_d)%3A.0%25%7D.%0A%20%20%20%20Async%20accuracy%20%7B_dir(_async_acc_d)%7D%20by%20%7Babs(_async_acc_d)%3A.0%25%7D.%0A%0A%20%20%20%20With%20a%200.5B%20model%20and%20only%20a%20few%20training%20steps%2C%20large%20gains%20are%20unlikely.%0A%20%20%20%20The%20key%20result%20is%20that%20the%20full%20pipeline%20works%3A%20generation%2C%20training%2C%0A%20%20%20%20weight%20sync%2C%20and%20evaluation%20all%20compose%20correctly%20through%20Monarch%20actors.%20The%0A%20%20%20%20failure%20mode%20breakdown%20shows%20*where*%20the%20model%20is%20improving%20(or%20not)%20--%20watch%0A%20%20%20%20for%20format%20compliance%20changes%20in%20particular%2C%20since%20that's%20the%20easiest%20RL%20win.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20What's%20Happening%20Under%20the%20Hood%0A%0A%20%20%20%20When%20you%20run%20the%20training%20loop%2C%20here's%20what%20each%20layer%20does%3A%0A%0A%20%20%20%20**Actor%20isolation**%3A%20Each%20actor%20(trainer%2C%20generators%2C%20buffer%2C%20zorplex%20workers)%0A%20%20%20%20runs%20in%20its%20own%20process%20with%20its%20own%20GPU%20assignment.%20%60CUDA_VISIBLE_DEVICES%60%20is%0A%20%20%20%20set%20in%20%60setup()%60%2C%20not%20at%20spawn%20time%20--%20the%20%60procs%60%20dimension%20in%20%60spawn_procs%60%0A%20%20%20%20is%20just%20a%20dimension%20name%2C%20not%20a%20GPU%20assignment.%0A%0A%20%20%20%20**Weight%20sync%20data%20flow**%20(circular%20buffer%20%2B%20CPU%20staging%20from%20%5BNB07%5D(.%2F07_rdma_weight_sync.html))%3A%0A%20%20%20%20%60%60%60%0A%20%20%20%20Trainer%20GPU%20%20--D2H--%3E%20%20CPU%20slot%5Bv%20%25%203%5D%20%20--RDMA--%3E%20%20Generator%20CPU%20staging%20%20--H2D--%3E%20%20Generator%20GPU%0A%20%20%20%20%60%60%60%0A%20%20%20%20-%20Trainer%20publishes%20weights%20to%20a%20circular%20buffer%20after%20each%20train%20step%0A%20%20%20%20-%20Generators%20pull%20from%20the%20buffer%20via%20RDMA%20into%20a%20CPU%20staging%20buffer%0A%20%20%20%20-%20Explicit%20H2D%20copy%20scatters%20into%20GPU%20model%20parameters%0A%20%20%20%20-%20The%20circular%20buffer%20has%203%20slots%2C%20so%20training%20never%20blocks%20on%20reads%0A%20%20%20%20-%20**Future%20improvement**%3A%20ideally%20we'd%20load%20from%20the%20trainer's%20CPU%20buffer%0A%20%20%20%20%20%20directly%20into%20the%20model's%20%60state_dict%60%2C%20skipping%20the%20staging%20copy.%0A%20%20%20%20%20%20We%20hit%20%60RDMABuffer%60%20bugs%20doing%20that%2C%20so%20for%20now%20we%20use%20the%20extra%20buffer.%0A%0A%20%20%20%20**Async%20concurrency**%20(via%20threads)%3A%0A%20%20%20%20-%201%20thread%20per%20generator%2C%20each%20using%20%60.slice(procs%3Di)%60%20to%20address%20its%20generator%0A%20%20%20%20-%20Each%20generator%20pulls%20latest%20weights%20from%20the%20trainer%20before%20each%20trajectory%0A%20%20%20%20%20%20(%60sync_weights_from_buffer%60%20short-circuits%20if%20version%20hasn't%20changed)%0A%20%20%20%20-%20Training%20in%20the%20main%20thread%0A%20%20%20%20-%20%60threading.Event%60%20coordinates%20shutdown%20when%20training%20completes%0A%20%20%20%20-%20GIL%20is%20released%20during%20I%2FO%20(actor%20calls)%20and%20CUDA%20(GPU%20compute)%2C%20so%20threads%0A%20%20%20%20%20%20achieve%20real%20concurrency%0A%0A%20%20%20%20**Sync%20vs%20Async%20generation**%3A%0A%20%20%20%20-%20Sync%20mode%20uses%20%60.call()%60%20broadcast%20to%20trigger%20all%20generators%20at%20once%2C%20then%20waits%0A%20%20%20%20%20%20for%20all%20to%20finish%20before%20training%0A%20%20%20%20-%20Async%20mode%20uses%20%60.slice()%60%20per%20thread%20so%20each%20generator%20runs%20independently%20--%0A%20%20%20%20%20%20no%20generator%20waits%20for%20another%0A%0A%20%20%20%20**Fault%20tolerance**%20(from%20%5BNB03%5D(.%2F03_fault_tolerance.html))%3A%0A%20%20%20%20-%20Generation%20loops%20wrap%20%60generate_trajectory.call_one().get()%60%20in%20%60try%2Fexcept%60%0A%20%20%20%20-%20On%20failure%2C%20the%20generator%20logs%20and%20retries%20instead%20of%20crashing%20the%20loop%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Scaling%20Up%0A%0A%20%20%20%20What%20we%20built%20here%20scales%20naturally%20with%20Monarch%3A%0A%0A%20%20%20%20%7C%20Scale%20%7C%20What%20Changes%20%7C%0A%20%20%20%20%7C-------%7C--------------%7C%0A%20%20%20%20%7C%20More%20generators%20%7C%20Increase%20%60num_generators%60%20slider%20--%20spawns%20larger%20ActorMesh%2C%20%60.call()%60%20broadcast%20scales%20automatically%20%7C%0A%20%20%20%20%7C%20More%20zorplex%20workers%20%7C%20Increase%20%60NUM_ZORPLEX%60%20--%20parallel%20task%20generation%20via%20Service%20%7C%0A%20%20%20%20%7C%20Multi-node%20%7C%20Use%20%60SlurmJob%60%20instead%20of%20%60this_host()%60%20%7C%0A%20%20%20%20%7C%20Better%20algorithms%20%7C%20Swap%20REINFORCE%20for%20PPO%2FGRPO%20(add%20importance%20sampling)%20%7C%0A%20%20%20%20%7C%20Production%20generators%20%7C%20Wrap%20generators%20in%20a%20Service%20too%20(health%20tracking%2C%20auto-scaling)%20%7C%0A%20%20%20%20%7C%20More%20services%20%7C%20Add%20reward%20models%2C%20search%20APIs%20as%20actors%20%7C%0A%0A%20%20%20%20**The%20patterns%20stay%20the%20same%3A**%0A%20%20%20%20-%20Actors%20for%20isolation%20and%20GPU%20assignment%0A%20%20%20%20-%20Endpoints%20for%20communication%20(%60.call_one().get()%60)%0A%20%20%20%20-%20RDMA%20%2B%20circular%20buffer%20for%20efficient%20weight%20transfer%0A%20%20%20%20-%20Version%20tracking%20for%20consistency%20across%20actors%0A%0A%20%20%20%20This%20is%20the%20foundation%20for%20which%20you%20could%20build%20production%20systems%2C%20using%20monarch%20at%20scale.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%20Recap%3A%20The%20Full%20Journey%0A%0A%20%20%20%20We've%20come%20a%20long%20way%20in%20this%20notebook%20series%3A%0A%0A%20%20%20%20%7C%20Notebook%20%7C%20What%20We%20Learned%20%7C%0A%20%20%20%20%7C----------%7C-----------------%7C%0A%20%20%20%20%7C%2001%20%7C%20Monarch's%20history%20and%20the%20single-controller%20paradigm%20%7C%0A%20%20%20%20%7C%2002%20%7C%20Interactive%20development%20with%20%60this_host()%60%20%7C%0A%20%20%20%20%7C%2003%20%7C%20Fault%20tolerance%20with%20%60try%2Fexcept%60%20on%20actor%20calls%20%7C%0A%20%20%20%20%7C%2004%20%7C%20Distributed%20tensors%20--%20Monarch's%20tensor%20engine%20%7C%0A%20%20%20%20%7C%2005%20%7C%20Zorplex%20benchmark%20--%20where%20Qwen%200.5B%20struggles%20%7C%0A%20%20%20%20%7C%2006%20%7C%20Services%20for%20managing%20worker%20pools%20with%20health%20tracking%20%7C%0A%20%20%20%20%7C%2007%20%7C%20RDMA%20weight%20sync%2C%20circular%20buffers%2C%20CPU%20staging%20%7C%0A%20%20%20%20%7C%20**08**%20%7C%20**Closing%20the%20loop%3A%20async%20RL%20training%20end%20to%20end**%20%7C%0A%0A%20%20%20%20**Key%20takeaways%20from%20this%20notebook%3A**%0A%0A%20%20%20%20-%20Monarch%20makes%20distributed%20RL%20feel%20like%20local%20Python%20--%20actors%2C%20endpoints%2C%0A%20%20%20%20%20%20and%20slicing%20compose%20naturally%20into%20a%20full%20training%20system%0A%20%20%20%20-%20Async%20RL%20collects%20more%20data%20per%20unit%20wall%20time%20by%20running%20generators%0A%20%20%20%20%20%20and%20trainer%20concurrently%0A%20%20%20%20-%20The%20circular%20buffer%20%2B%20CPU%20staging%20pattern%20from%20%5BNB07%5D(.%2F07_rdma_weight_sync.html)%20decouples%20training%0A%20%20%20%20%20%20from%20weight%20distribution%0A%20%20%20%20-%20Before%2Fafter%20evaluation%20closes%20the%20loop%3A%20we%20can%20measure%20whether%20training%0A%20%20%20%20%20%20actually%20improves%20the%20model%0A%0A%20%20%20%20**Where%20to%20go%20next%3A**%20Forge%20GRPO%20implements%20these%20same%20patterns%20at%20production%0A%20%20%20%20scale%20--%20multiple%20nodes%2C%20larger%20models%2C%20PPO%2FGRPO%20instead%20of%20REINFORCE%2C%20and%0A%20%20%20%20proper%20reward%20modeling.%20The%20Monarch%20primitives%20you've%20learned%20here%20are%20the%0A%20%20%20%20building%20blocks%20for%20all%20of%20it.%0A%0A%20%20%20%20---%0A%0A%20%20%20%20**Previous%3A**%20%5BNB07b%20%E2%80%94%20RDMA%20Deep%20Dive%5D(.%2F07b_weight_sync_deep_dive.html)%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20return%0A%0A%0Aif%20__name__%20%3D%3D%20%22__main__%22%3A%0A%20%20%20%20app.run()%0A
</marimo-code>

<marimo-code-hash hidden="">aa949da010076d0a08a87fc91e9a14b9</marimo-code-hash>
</body>
</html>
