import marimo

__generated_with = "0.19.7"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell
def _():
    # Shared imports for the notebook
    import time
    import torch
    from monarch.actor import Actor, endpoint, this_host, current_rank
    from monarch.rdma import RDMABuffer, is_rdma_available
    return Actor, RDMABuffer, endpoint, is_rdma_available, this_host, torch


@app.cell
def _(mo):
    mo.md(r"""
    # RDMA & Weight Synchronization

    This notebook explores efficient weight synchronization for async RL systems.

    **Outline:**

    1. **Why Weight Sync Matters** - On-policy vs off-policy, model scale
    2. **The Bandwidth Hierarchy** - NVLink, InfiniBand, PCIe
    3. **The Problem: Collectives Are Blocking** - Why RL needs something different
    4. **The Magic Pointer Pattern** - Control plane vs data plane separation
    5. **CPU Staging** - Decoupling trainer and generator timing
    6. **Circular Weight Buffers** - Versioning without memory churn
    7. **Weight Re-sharding** - Handling different tensor layouts
    8. **Putting It All Together** - Live demo with concurrent loops

    **Want to go deeper?** Check out **06b_weight_sync_deep_dive.py** for ibverbs internals,
    memory registration benchmarks, and full implementations. This notebook focuses on
    the concepts and patterns you need to know for async RL.
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 1. Why Weight Sync Matters

    ### The On-Policy Problem

    Traditional RL algorithms want to be **on-policy**: generate experience using the current
    policy, then immediately use that experience to update the policy. This creates a tight loop:

    ```
    On-Policy RL:
    ┌──────────────────────────────────────────────────────────────────┐
    │  generate(policy_v1) → train(samples) → policy_v2 → repeat       │
    │                                                                  │
    │  Experience from v1 is only valid for updating v1                │
    └──────────────────────────────────────────────────────────────────┘
    ```

    **Async RL breaks this rule.** Generators run continuously while the trainer updates weights.
    By the time a sample reaches the trainer, it was generated by an old policy version:

    ```
    Async RL (off-policy):
    ┌──────────────────────────────────────────────────────────────────┐
    │  Generator: policy_v1 → sample₁                                  │
    │  Trainer:   train(sample₁) → policy_v2                           │
    │  Generator: policy_v1 → sample₂  ← still using v1!               │
    │  Trainer:   train(sample₂) → policy_v3                           │
    │                                                                  │
    │  Samples are "stale" - generated by older policy versions        │
    └──────────────────────────────────────────────────────────────────┘
    ```

    This **off-policy-ness** can work up to a degree, but must be limited. The generators
    need fresh weights regularly to stay "close enough" to on-policy. Weight sync frequency
    becomes a key hyperparameter trading off:

    - **Too slow**: Samples become too stale, training diverges
    - **Too fast**: Weight sync overhead dominates, negating async benefits

    ### The Scale Problem

    For LLM-based RL, the weights are **massive**. Back-of-envelope math
    (1 parameter ≈ 2 bytes in bf16):

    | Model | Weight Size |
    |-------|-------------|
    | Llama 3.1 70B | ~140 GB |
    | Llama 3.1 405B | ~810 GB |
    | DeepSeek V3 671B | ~1.3 TB |

    These weights need to move from trainer → generators regularly. If we're
    not careful, our "async RL training workload" just becomes a weight syncing
    workload. Let's look at the bandwidth hierarchy to understand why this is
    tricky and what we can do about it.
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 2. The Bandwidth Hierarchy

    Modern HPC clusters have multiple interconnects with vastly different bandwidths:

    ```
    ┌──────────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │                                              NODE A                                                      │
    │                                                                                                          │
    │    ┌───────────────────────────────────────────────────────────────────────────────────────────────┐     │
    │    │                              NVSwitch / NVLink Fabric                                         │     │
    │    │  ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐                      │     │
    │    │  │GPU 0 │ │GPU 1 │ │GPU 2 │ │GPU 3 │ │GPU 4 │ │GPU 5 │ │GPU 6 │ │GPU 7 │                      │     │
    │    │  └──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘                      │     │
    │    │     ########################################################################  900 GB/s NVLink │     │
    │    └─────────────────────────────────────────┬─────────────────────────────────────────────────────┘     │
    │                                              │                                                           │
    │                                         ======  64 GB/s PCIe                                             │
    │                                              │                                                           │
    │    ┌─────────┐  ------ 48 GB/s ------ ┌──────┴──┐                  ┌───────┐          ┌───────┐          │
    │    │  CPU 0  │     CPU interconnect   │  CPU 1  │ ====== 64 GB/s ══│ NIC 0 │          │ NIC 1 │          │
    │    └────┬────┘                        └────┬────┘      PCIe        └───┬───┘          └───┬───┘          │
    │         │                                  │                           │                  │              │
    │         ══════════════════════ 64 GB/s PCIe ═══════════════════════════╪══════════════════╪              │
    │                                                                        │                  │              │
    └────────────────────────────────────────────────────────────────────────┼──────────────────┼──────────────┘
                                                                             │                  │
                                                                           ======  50 GB/s   ======
                                                                        IB NDR400         IB NDR400
                                                                             │                  │
                                                            ┌────────────────┴──────────────────┴────────────────┐
                                                            │                                                    │
                                                            │              InfiniBand Switch                     │
                                                            │                                                    │
                                                            └────────────────┬──────────────────┬────────────────┘
                                                                             │                  │
                                                                           ======  50 GB/s   ======
                                                                        IB NDR400         IB NDR400
                                                                             │                  │
    ┌────────────────────────────────────────────────────────────────────────┼──────────────────┼──────────────┐
    │                                                                        │                  │              │
    │         ══════════════════════ 64 GB/s PCIe ═══════════════════════════╪══════════════════╪              │
    │         │                                  │                           │                  │              │
    │    ┌────┴────┐                        ┌────┴────┐      PCIe        ┌───┴───┐          ┌───┴───┐          │
    │    │  CPU 0  │     CPU interconnect   │  CPU 1  │ ====== 64 GB/s ══│ NIC 0 │          │ NIC 1 │          │
    │    └─────────┘ ------ 48 GB/s ------  └─────────┘                  └───────┘          └───────┘          │
    │                                              │                                                           │
    │                                           ======  64 GB/s PCIe                                           │
    │                                              │                                                           │
    │    ┌─────────────────────────────────────────┴─────────────────────────────────────────────────────┐     │
    │    │     ########################################################################  900 GB/s NVLink │     │
    │    │  ┌──┴───┐ ┌──┴───┐ ┌──┴───┐ ┌──┴───┐ ┌──┴───┐ ┌──┴───┐ ┌──┴───┐ ┌──┴───┐                      │     │
    │    │  │GPU 0 │ │GPU 1 │ │GPU 2 │ │GPU 3 │ │GPU 4 │ │GPU 5 │ │GPU 6 │ │GPU 7 │                      │     │
    │    │  └──────┘ └──────┘ └──────┘ └──────┘ └──────┘ └──────┘ └──────┘ └──────┘                      │     │
    │    │                              NVSwitch / NVLink Fabric                                         │     │
    │    └───────────────────────────────────────────────────────────────────────────────────────────────┘     │
    │                                              NODE B                                                      │
    └──────────────────────────────────────────────────────────────────────────────────────────────────────────┘

    Bandwidth encoding (line intensity):
      ########  NVLink/NVSwitch   900 GB/s bidirectional (GPU ↔ GPU, same node)
      ========  PCIe Gen5 / RDMA  50-64 GB/s unidirectional (CPU↔GPU, CPU↔NIC, cross-node)
      --------  CPU interconnect  48 GB/s (CPU ↔ CPU, same node)
    ```

    ### A Note on Bandwidth Numbers

    Bandwidth specs vary by hardware generation, cluster configuration, and vendor.
    We'll use numbers from Meta's published Llama 3 training infrastructure
    ([Building Meta's GenAI Infrastructure](https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/)):

    > "Both of these solutions interconnect **400 Gbps endpoints**... we have successfully
    > used both RoCE and InfiniBand clusters for large, GenAI workloads (including our
    > ongoing training of Llama 3 on our RoCE cluster) without any network bottlenecks."

    **Important**: "400 Gbps" in networking is **full-duplex** - meaning 400 Gbps transmit
    AND 400 Gbps receive simultaneously. For weight sync (unidirectional: trainer → generator),
    we get the full 400 Gbps = **50 GB/s per NIC**.

    Meta's Grand Teton nodes have **8 RDMA NICs** (one per GPU, 1:1 mapping), giving
    400 GB/s aggregate unidirectional bandwidth per node. For more details on Grand Teton
    and Monarch's RDMA architecture, see the SIGCOMM 2024 paper:
    [RDMA over Ethernet for Distributed AI Training at Meta Scale](https://cs.stanford.edu/~keithw/sigcomm2024/sigcomm24-final246-acmpaginated.pdf).

    | Interconnect | Bandwidth | Notes |
    |--------------|-----------|-------|
    | **NVLink 4.0** | 900 GB/s bidirectional | ~450 GB/s per direction |
    | **RDMA (IB/RoCE)** | 400 Gbps = 50 GB/s | Per NIC, full-duplex |
    | **PCIe Gen5 x16** | 64 GB/s | Per direction |

    **Key observations:**

    1. **NVLink is fast but same-node only** - 450 GB/s, but can't cross the network
    2. **RDMA >> TCP** - 50 GB/s with zero-copy beats TCP significantly for cross-node
    3. **Multi-NIC scales** - 8 NICs × 50 GB/s = 400 GB/s, approaching NVLink speeds

    **Rule of thumb**: NVLink for same-node ops (gradients, activations).
    RDMA for cross-node communication (weight sync) - it's the only practical option.

    ### Back-of-Envelope: Syncing Large Models

    Let's do some quick math. DeepSeek V3 has 671B parameters (~1.34 TB in bf16).

    The key insight: **you're not shoving 1.3 TB through a single NIC**. The weights
    are distributed across many GPUs (via some combination of PP, EP, TP, FSDP),
    and each GPU has its own NIC. You get **aggregate bandwidth** across all NICs.

    The actual sync time depends on **both sides**:
    - **Trainer's aggregate upload bandwidth** (sending weights out)
    - **Generator's aggregate download bandwidth** (receiving weights)

    The bottleneck is whichever is smaller. And if multiple generators pull from
    the same trainer simultaneously, the trainer's bandwidth is divided among them.

    With Grand Teton's 8 NICs per node at 50 GB/s each (400 GB/s per node),
    the math is simple: **Time = Shard Size / Bandwidth**.

    The per-node shard size depends on how many nodes the model is spread across:
    - DeepSeek V3 (1.3 TB) across 8 nodes → ~160 GB per node
    - DeepSeek V3 (1.3 TB) across 16 nodes → ~80 GB per node

    | Per-node shard | Time to sync |
    |----------------|--------------|
    | ~160 GB (8 nodes) | 160 / 400 = **0.4 seconds** |
    | ~80 GB (16 nodes) | 80 / 400 = **0.2 seconds** |

    The exact per-node shard size depends on your parallelism strategy (PP, EP, TP, etc.),
    but the math works out: with modern RDMA hardware, you can sync even the largest
    models in **sub-second time**.

    Compare this to naive TCP: kernel copies, socket overhead, no zero-copy...
    easily 10x slower. **RDMA is the only way to make async RL practical at scale.**
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 3. The Problem: Collectives Are Blocking

    Most people use RDMA via **collectives** through PyTorch distributed:

    ```python
    import torch.distributed as dist

    dist.init_process_group(backend="nccl")
    dist.all_reduce(gradients, op=dist.ReduceOp.SUM)
    dist.broadcast(weights, src=0)
    ```

    This works great for training. But async RL has a different access pattern.

    ### High Variance in Generation Times

    Generators have wildly different completion times:
    - Some prompts → 10 tokens (fast)
    - Other prompts → 1000 tokens (slow)

    With collectives, fast generators wait for slow ones:

    ```
    Generator 0: ├── gen (fast) ──┤  ⚠️ WAITING...
    Generator 1: ├────── gen (slow) ──────┤
    Generator 2: ├── gen (fast) ──┤  ⚠️ WAITING...
                                          ↓
                              all_gather(weights)  # Everyone waits!
    ```

    ### The One-Sided Solution: RDMA

    What if the sender could write directly to the receiver's memory without coordination?

    ```
    Two-sided (send/recv):
      Sender: "I have data"  ──────────►  Receiver: "I'm ready"
      Sender: sends data     ──────────►  Receiver: receives data
                             2 messages required

    One-sided (RDMA):
      Sender: writes directly to receiver's memory
                             No coordination needed!
    ```

    This is what RDMA enables: **one-sided memory operations**.
    The trainer doesn't even know when generators pull weights - this is truly async!
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 4. The Magic Pointer Pattern

    One natural question that arises is along the lines of, "How do we actually represent one-sided puts/gets if not with NCCL collectives?"

    Here's a key insight: to represent remote data, we only need a **tiny handle** -
    an `(addr, rkey, size)` tuple that says "here's where my data lives."

    Monarch wraps this in `RDMABuffer`. Let's see how small it actually is:
    """)
    return


@app.cell
def _(Actor, RDMABuffer, endpoint, is_rdma_available, this_host, torch):
    # Measure actual size of RDMABuffer handles
    import pickle

    def show_fallback_sizes():
        """Fallback: show expected sizes based on RDMABuffer structure."""
        print("(RDMA not available - showing expected handle sizes)\n")
        print("RDMABuffer contains: addr (8B) + rkey (4B) + size (8B) + owner (~100B)")
        print("Total serialized size: ~150-200 bytes regardless of tensor size\n")

        sizes = [("1 KB", 1024), ("1 MB", 1024**2), ("1 GB", 1024**3)]
        handle_bytes = 150  # approximate

        for name, tensor_bytes in sizes:
            ratio = tensor_bytes / handle_bytes
            print(f"{name:<8} tensor → ~150 byte handle → {ratio:,.0f}x compression")

        print("\n→ Handle size is O(1) regardless of tensor size!")

    try:
        if not is_rdma_available():
            show_fallback_sizes()
        else:
            class BufferSizeDemo(Actor):
                """Actor that creates RDMABuffers and measures their size."""

                @endpoint
                def measure_buffer_sizes(self) -> list:
                    import pickle as _pickle
                    results = []
                    sizes = [
                        ("1 KB", 256),
                        ("1 MB", 256 * 1024),
                        ("10 MB", 256 * 1024 * 10),
                    ]

                    for name, numel in sizes:
                        tensor = torch.randn(numel)
                        tensor_bytes = tensor.numel() * tensor.element_size()

                        byte_tensor = tensor.view(torch.uint8).flatten()
                        buffer = RDMABuffer(byte_tensor)
                        handle_bytes = len(_pickle.dumps(buffer))

                        results.append((name, tensor_bytes, handle_bytes))

                    return results

            proc = this_host().spawn_procs(per_host={"procs": 1})
            demo = proc.spawn("buffer_demo", BufferSizeDemo)

            results = demo.measure_buffer_sizes.call_one().get()

            print("RDMABuffer handle size vs actual tensor size:\n")
            print(f"{'Tensor Size':<12} {'Actual Bytes':<15} {'Handle Size':<15} {'Ratio':<10}")
            print("-" * 55)

            for name, tensor_bytes, handle_bytes in results:
                ratio = tensor_bytes / handle_bytes
                print(f"{name:<12} {tensor_bytes:>12,} B   {handle_bytes:>6} B        {ratio:>8,.0f}x")

            print("\n→ Handle size is O(1) regardless of tensor size!")

    except Exception as e:
        print(f"(RDMA setup failed: {e})\n")
        show_fallback_sizes()
    return


@app.cell
def _(mo):
    mo.md(r"""
    ### The Magic Pointer

    This is the core pattern: **separate control plane from data plane**.

    - **Control plane** (actor messages): Send tiny handle (~100 bytes)
    - **Data plane** (RDMA): Bulk transfer of actual data (~10 GB)

    Think of `RDMABuffer` as a **magic pointer** - it's a pointer that works across machines:

    ```
    Trainer                              Generator
    ┌─────────────┐                     ┌─────────────┐
    │ weights     │                     │ local copy  │
    │ (10 GB)     │                     │ (empty)     │
    └──────┬──────┘                     └──────┬──────┘
           │                                   │
           │  1. Create RDMABuffer             │
           │     (register memory, get handle) │
           │                                   │
           ├─────── 2. Send handle ───────────►│  (~100 bytes via actor)
           │                                   │
           │◄────── 3. RDMA read ──────────────┤  (~10 GB via hardware)
           │        (no trainer involvement!)  │
    ```

    The trainer doesn't even know when generators pull weights. True one-sided.

    ### RDMABuffer in Action

    ```python
    from monarch.rdma import RDMABuffer

    # Trainer side: register weights
    weights = torch.randn(1024, 1024, device="cuda")
    buffer = RDMABuffer(weights.view(torch.uint8).flatten())

    # Return buffer as part of an endpoint response
    @endpoint
    def get_weight_handle(self) -> RDMABuffer:
        return self.buffer

    # Generator side: receive handle, pull directly into GPU
    handle = trainer.get_weight_handle.call_one().get()  # Tiny message
    gpu_weights = model.weights.view(torch.uint8).flatten()
    handle.read_into(gpu_weights).get()                   # Bulk RDMA → GPU
    ```

    **Want to understand how RDMA works under the hood?** Check out **06b_weight_sync_deep_dive.py**
    for ibverbs internals, queue pair setup, and why Monarch's actor model is such a natural fit
    for managing RDMA connections. It's actors all the way down!
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ### Live Demo: Trainer → Generator Weight Sync

    Let's see this in action with a simple example. A trainer holds weights,
    a generator pulls them via RDMA.
    """)
    return


@app.cell
def _(Actor, RDMABuffer, endpoint, is_rdma_available, this_host, torch):
    # Simple trainer → generator weight sync demo

    def show_fallback_demo():
        """Show what would happen with RDMA."""
        print("(RDMA not available - showing conceptual flow)\n")
        print("1. Trainer creates weights (e.g., 4 MB tensor)")
        print("2. Trainer wraps weights in RDMABuffer → tiny handle (~150 bytes)")
        print("3. Trainer sends handle to Generator via actor message")
        print("4. Generator calls handle.read_into(local_buffer)")
        print("5. RDMA hardware transfers 4 MB directly, trainer not involved!")
        print("\n→ Zero-copy, one-sided, no serialization overhead")

    try:
        if not is_rdma_available():
            show_fallback_demo()
        else:
            class Sender(Actor):
                """Sender that holds data and exposes an RDMA handle."""

                def __init__(self, size: int):
                    # Create some data to send
                    self.data = torch.randn(size, dtype=torch.float32)
                    # Register with RDMA
                    self.data_bytes = self.data.view(torch.uint8).flatten()
                    self.buffer = RDMABuffer(self.data_bytes)
                    print(f"[Sender] Created data: {self.data.numel() * 4 / 1e6:.1f} MB")

                @endpoint
                def get_handle(self) -> RDMABuffer:
                    """Return tiny handle (not the data itself!)"""
                    return self.buffer

                @endpoint
                def get_checksum(self) -> float:
                    """For verification: sum of data"""
                    return float(self.data.sum())

            class Receiver(Actor):
                """Receiver that pulls data from sender via RDMA."""

                def __init__(self, size: int):
                    # Pre-allocate space for data
                    self.data = torch.zeros(size, dtype=torch.float32)
                    self.data_bytes = self.data.view(torch.uint8).flatten()
                    print(f"[Receiver] Allocated buffer: {self.data.numel() * 4 / 1e6:.1f} MB")

                @endpoint
                def pull_data(self, handle: RDMABuffer) -> float:
                    """Pull data via RDMA read, return checksum for verification."""
                    # This is the magic: RDMA read directly into our buffer
                    handle.read_into(self.data_bytes).get()
                    return float(self.data.sum())

            # Spawn sender and receiver
            procs = this_host().spawn_procs(per_host={"procs": 2})

            sender = procs.slice(procs=0).spawn("sender", Sender, 1024 * 1024)  # 4 MB
            receiver = procs.slice(procs=1).spawn("receiver", Receiver, 1024 * 1024)

            # Step 1: Get handle from sender (tiny message!)
            handle = sender.get_handle.call_one().get()
            print(f"\n[Orchestrator] Got handle from sender")

            # Step 2: Send handle to receiver, have it pull data
            receiver_checksum = receiver.pull_data.call_one(handle).get()
            sender_checksum = sender.get_checksum.call_one().get()

            print(f"[Orchestrator] Sender checksum: {sender_checksum:.2f}")
            print(f"[Orchestrator] Receiver checksum: {receiver_checksum:.2f}")
            print(f"[Orchestrator] Match: {abs(sender_checksum - receiver_checksum) < 0.01}")

    except Exception as e:
        print(f"(Demo failed: {e})\n")
        show_fallback_demo()
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 5. CPU Staging Pattern

    ### GPU-Native RDMA Works!

    First, let's be clear: **GPU-native RDMA works** and is fast:
    - GPUDirect RDMA: NIC reads directly from GPU memory
    - No CPU copy needed (when hardware supports it)
    - Great for synchronous transfers

    ### Why CPU Staging for Async RL?

    The issue isn't bandwidth - it's **timing**:

    ```
    Direct GPU→GPU RDMA:
    ┌─────────────────────────────────────────────────────┐
    │ Generator GPU is mid-inference                       │
    │ ├── layer 1 ──┤ [RDMA arrives, needs sync!]         │
    │               ↓                                      │
    │         cudaDeviceSynchronize()  ← Blocks inference! │
    └─────────────────────────────────────────────────────┘
    ```

    With CPU staging, nothing on the critical path blocks:

    ```
    Trainer GPU ──► CPU staging buffer (RDMA registered)
                          │
                          │ [Sits here, ready anytime]
                          │
                          ▼
    Generator grabs when ready ──► Generator GPU
    ```

    The CPU buffer is a **temporal decoupling point**.
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 6. Circular Weight Buffers

    ### The Versioning Problem

    In async RL, trainer updates weights continuously. Generators need to:
    1. **Grab the latest** weights (not stale ones)
    2. **Not block** waiting for updates
    3. **Avoid memory churn** (re-registering RDMA buffers is expensive)

    ### Solution: Circular Buffer

    ```
    Trainer writes:     v0    →  v1  →  v2  →  v3  →  v4  →  v5  → ...
                         ↓        ↓      ↓
    Buffer slots:      [slot0] [slot1] [slot2]  (circular, reused)
                         v3      v4      v5

    Generator reads: "Give me latest" → v5
    ```

    Benefits:
    - **Pre-registered RDMA buffers** - no memory registration on hot path
    - **Lock-free reads** - generators always get a consistent snapshot
    - **Bounded memory** - only N versions in flight

    The key insight: register all slots at init time, then just write to them.
    No allocation, no registration on the critical path.

    **Want to see a full implementation?** Check out **06b_weight_sync_deep_dive.py** for a
    complete `CircularWeightBuffer` class with versioning and RDMA integration.
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 7. Weight Re-sharding

    ### The Sharding Mismatch Problem

    Trainer and Generator often have **different tensor layouts**. Consider an example:

    | Role | Parallelism | Sharding |
    |------|-------------|----------|
    | Trainer | FSDP (8 GPUs) | `Shard(0)` - rows split across 8 GPUs |
    | Generator | TP (2 GPUs) | `Shard(1)` - columns split across 2 GPUs |

    Therefore we cannot always directly transfer weights - we need **re-sharding**.

    Consider a simple example where the trainer may be row-sharded and the generator may be column-sharded:

    ```
    Trainer (row-sharded):          Generator (column-sharded):
    ┌──────────────────┐            ┌─────────┬─────────┐
    │ GPU 0: rows 0-127│            │ GPU 0   │ GPU 1   │
    ├──────────────────┤     →      │ cols    │ cols    │
    │ GPU 1: rows 128+ │            │ 0-511   │ 512+    │
    └──────────────────┘            └─────────┴─────────┘
    ```

    ### Two Approaches

    **Gather Then Slice** (simple but wasteful):
    One approach is to materialize the entire tensor, i.e. `gather`, transfer the full tensor, and then slice on the receiver side:
    1. Each receiver gathers ALL sender shards → full tensor
    2. Each receiver slices out its portion
    3. **Problem**: 2x redundant data transfer

    **Routed Transfer** (optimal):
    A more efficient approach is to only transfer the data that needs to be transferred:
    1. Pre-compute which sender chunks overlap with which receiver regions
    2. Send only the exact chunks needed
    3. **Benefit**: Minimal bandwidth, no redundancy

    ```
    GATHER: G0 receives T0,T1,T2,T3 → discards T2,T3 (50% waste!)
    ROUTED: G0 receives T0,T1 only → exactly what it needs
    ```

    The routed approach batches all needed transfers into one plan.
    Pre-compute the plan once at handshake, execute it on each sync.

    **Want to see the overlap computation and benchmarks?** Check out **06b_weight_sync_deep_dive.py**
    for the full DTensor re-sharding implementation with placement-aware routing.
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 8. Putting It All Together

    The full async RL weight sync pattern:

    ```
    ┌─────────────────────────────────────────────────────────────────┐
    │                         TRAINER                                  │
    │  1. Train step completes                                        │
    │  2. Copy weights to CPU staging buffer (non-blocking D2H)       │
    │  3. Publish to circular buffer with version tag                 │
    │  4. Continue training (no blocking!)                            │
    └─────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
    ┌─────────────────────────────────────────────────────────────────┐
    │              CIRCULAR BUFFER (CPU, RDMA-registered)             │
    │  [slot 0: v3] [slot 1: v4] [slot 2: v5]                        │
    │                                 ↑ latest                        │
    └─────────────────────────────────────────────────────────────────┘
                                    │
              ┌─────────────────────┼─────────────────────┐
              ▼                     ▼                     ▼
    ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐
    │   GENERATOR 0   │   │   GENERATOR 1   │   │   GENERATOR 2   │
    │                 │   │                 │   │                 │
    │ After gen done: │   │ After gen done: │   │ After gen done: │
    │ 1. Get latest   │   │ 1. Get latest   │   │ 1. Get latest   │
    │    version      │   │    version      │   │    version      │
    │ 2. RDMA read    │   │ 2. RDMA read    │   │ 2. RDMA read    │
    │    → GPU        │   │    → GPU        │   │    → GPU        │
    │ 3. Re-shard if  │   │ 3. Re-shard if  │   │ 3. Re-shard if  │
    │    needed       │   │    needed       │   │    needed       │
    └─────────────────┘   └─────────────────┘   └─────────────────┘
    ```

    **Key properties:**
    - Trainer never blocks waiting for generators
    - Generators pull directly to GPU when *they're* ready
    - Re-sharding happens locally on each generator
    - Circular buffer bounds memory, reuses RDMA registrations
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ### Live Demo: Async Weight Sync

    Let's see this in action! We'll simulate the core async RL pattern:

    - **1 Trainer**: Runs training steps, publishes new weights to a 3-slot circular buffer
    - **4 Generators**: Each independently syncs to latest weights, then generates

    All 5 actors run **concurrently and independently**. The trainer never waits for generators,
    and each generator grabs weights whenever it's ready (at slightly different rates to show
    the async behavior). To verify correctness, we set weights to `version` and check on read.
    """)
    return


@app.cell
def _(
    Actor,
    RDMABuffer,
    current_rank,
    endpoint,
    is_rdma_available,
    this_host,
    torch,
):
    import threading

    def show_fallback():
        print("(RDMA not available - showing conceptual flow)\n")
        print("What would happen with RDMA:")
        print("  [Trainer] Publishes v0, v1, v2... to circular buffer")
        print("  [Generator] Syncs when ready, verifies weights match version")
        print("  Both run independently, no blocking!")

    try:
        if not is_rdma_available():
            show_fallback()
        else:
            class Trainer(Actor):
                """Trainer with circular buffer for weight versioning."""

                def __init__(self, weight_size: int):
                    self.n_slots = 5
                    self.version = 0
                    self.slots = []
                    self.handles = []
                    for _ in range(self.n_slots):
                        slot = torch.zeros(weight_size, dtype=torch.float32)
                        self.slots.append(slot)
                        self.handles.append(RDMABuffer(slot.view(torch.uint8).flatten()))
                    print(f"[Trainer] Initialized {self.n_slots}-slot circular buffer")

                @endpoint
                def get_latest(self) -> tuple:
                    if self.version == 0:
                        return None, -1
                    v = self.version - 1
                    return self.handles[v % self.n_slots], v

                @endpoint
                def train_step(self):
                    """Single training step: publish new weights."""
                    import sys
                    slot_idx = self.version % self.n_slots
                    self.slots[slot_idx].fill_(float(self.version))
                    self.version += 1
                    print(f"[Trainer] Published v{self.version - 1}")
                    sys.stdout.flush()
                    return self.version

            class Generator(Actor):
                """Generator that syncs weights and generates."""

                def __init__(self, weight_size: int, trainer):
                    self.gen_id = current_rank().rank
                    self.trainer = trainer
                    self.weights = torch.zeros(weight_size, dtype=torch.float32)
                    self.weight_bytes = self.weights.view(torch.uint8).flatten()
                    self.current_version = -1
                    print(f"[Gen {self.gen_id}] Initialized")

                @endpoint
                def generate_step(self) -> int:
                    """Single generate step: sync if needed, then generate."""
                    import sys
                    # Try to sync weights
                    handle, version = self.trainer.get_latest.call_one().get()
                    if handle is not None and version > self.current_version:
                        handle.read_into(self.weight_bytes).get()
                        actual = int(self.weights[0].item())
                        if actual >= version:
                            self.current_version = actual
                            print(f"[Gen {self.gen_id}] Synced to v{actual}")
                            sys.stdout.flush()
                    return self.current_version

            # Spawn trainer and generators
            n_generators = 4
            trainer_proc = this_host().spawn_procs(per_host={"procs": 1})
            generator_procs = this_host().spawn_procs(per_host={"procs": n_generators})

            trainer = trainer_proc.spawn("trainer", Trainer, weight_size=1024)
            generators = generator_procs.spawn("generators", Generator, weight_size=1024, trainer=trainer)

            print("\n--- Running async RL simulation ---\n")

            # Results storage
            _results = {"trainer": None, "generators": [None] * n_generators}

            def run_trainer(n_steps, step_time):
                import time
                for _ in range(n_steps):
                    time.sleep(step_time)
                    _results["trainer"] = trainer.train_step.call_one().get()

            def run_generator(gen_actor, gen_idx, n_iters, gen_time):
                import time
                for _ in range(n_iters):
                    version = gen_actor.generate_step.call_one().get()
                    if version >= 0:
                        time.sleep(gen_time)
                        print(f"[Gen {gen_idx}] Generated (v{version})")
                    else:
                        time.sleep(0.05)
                _results["generators"][gen_idx] = version

            # Create threads for trainer and each generator
            threads = []

            # Trainer thread
            t = threading.Thread(target=run_trainer, args=(6, 1.0))  # 1 second per step
            threads.append(t)

            # Generator threads
            for i in range(n_generators):
                gen_actor = generators.slice(procs=i)
                t = threading.Thread(target=run_generator, args=(gen_actor, i, 5, 0.25))
                threads.append(t)

            # Start all threads
            for t in threads:
                t.start()

            # Wait for all to complete
            for t in threads:
                t.join()

            print(f"\n--- Done! Trainer published {_results['trainer']} versions ---")
            print(f"Generators ended on versions: {_results['generators']}")
            print("All pulled independently via RDMA, weights verified!")

    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"(Demo failed: {e})")
        show_fallback()
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 9. Going Further: TorchStore

    All the patterns we've covered - RDMA memory registration, magic pointers, circular buffers,
    pre-computed transfer plans - are building blocks. If you need a **production-ready solution**,
    check out [TorchStore](https://github.com/meta-pytorch/torchstore).

    ### What is TorchStore?

    TorchStore is a **distributed, asynchronous key-value store for PyTorch tensors** built on
    Monarch's actor framework. It abstracts away the RDMA complexity while giving you:

    ```python
    from torchstore import TorchStore

    # Store tensors with async API
    await ts.put("model/layer1/weights", tensor)

    # Retrieve with optional in-place and slice semantics
    await ts.get("model/layer1/weights", inplace_tensor=buffer)

    # Native PyTorch checkpoint support
    await ts.put_state_dict(model.state_dict())
    loaded = await ts.get_state_dict()
    ```

    ### When to Use What

    | Scenario | Solution |
    |----------|----------|
    | Learning RDMA patterns | This notebook + 06b |
    | Custom RL weight sync | See 06b for `RDMABuffer` + `RDMAAction` patterns |
    | General tensor storage | Use TorchStore |
    | Checkpointing | Use TorchStore's `put_state_dict` |
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## Summary

    ### Key Takeaways

    1. **Bandwidth hierarchy matters**: NVLink (900 GB/s) >> InfiniBand (50 GB/s)
       - Keep frequent operations on NVLink, use RDMA for cross-node

    2. **Collectives block, RL needs async**: High variance in generation times makes
       synchronous operations expensive

    3. **Magic pointer pattern**: Tiny handle over control plane, bulk data over data plane
       - ~100 bytes to describe 10 GB transfer

    4. **CPU staging**: Temporal decoupling for async RL
       - Nothing blocks on the critical path

    5. **Circular buffers**: Version weights without memory churn
       - Pre-register RDMA buffers, reuse slots

    6. **Weight re-sharding**: Different layouts need overlap computation
       - Routed approach avoids redundant transfers

    ### Want More?

    - **06b_weight_sync_deep_dive.py** - ibverbs internals, benchmarks, full implementations
    - **07_async_rl_e2e.py** - Complete async RL system using these patterns
    """)
    return


if __name__ == "__main__":
    app.run()
