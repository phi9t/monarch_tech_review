import marimo

__generated_with = "0.19.7"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell
def _():
    # Shared imports for the notebook
    import time
    import torch
    from monarch.actor import Actor, endpoint, this_host, current_rank
    from monarch.rdma import RDMABuffer, is_rdma_available
    return (
        Actor,
        RDMABuffer,
        current_rank,
        endpoint,
        is_rdma_available,
        this_host,
        torch,
    )


@app.cell
def _(mo):
    mo.md(r"""
    # RDMA & Weight Synchronization

    You've built your async RL system. Generators are humming, the trainer is
    learning. Then you check GPU utilization and discover that most of your
    "training" time is spent copying weights. Your async system has become a
    weight-syncing system that occasionally does RL.

    This notebook is about making that problem disappear — using RDMA to move
    weights from trainer to generators so fast it becomes invisible.

    **Want to go deeper?** Check out **06b_weight_sync_deep_dive.py** for ibverbs internals,
    memory registration benchmarks, and full implementations. This notebook focuses on
    the concepts and patterns you need to know for async RL.
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ### Prerequisites

    This notebook assumes familiarity with:

    - **PyTorch basics** - tensors, dtypes, device placement
    - **On-policy vs off-policy RL** - covered in [NB04: Async RL](./04_async_rl.py)
    - **Monarch Actor model** - spawning actors, endpoints, `call_one`/`call` (from [NB01](./01_history_and_vision.py) and [NB02](./02_interactive_devx.py))
    - **Basic networking concepts** - what bandwidth and latency mean, client-server vs peer-to-peer
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 1. Why Weight Sync Matters

    ### The On-Policy Problem

    Traditional RL algorithms want to be **on-policy**: generate experience using the current
    policy, then immediately use that experience to update the policy. This creates a tight loop:

    ```
    On-Policy RL:
    ┌──────────────────────────────────────────────────────────────────┐
    │  generate(policy_v1) → train(samples) → policy_v2 → repeat       │
    │                                                                  │
    │  Experience from v1 is only valid for updating v1                │
    └──────────────────────────────────────────────────────────────────┘
    ```

    **Async RL breaks this rule.** Generators run continuously while the trainer updates weights.
    By the time a sample reaches the trainer, it was generated by an old policy version:

    ```
    Async RL (off-policy):
    ┌──────────────────────────────────────────────────────────────────┐
    │  Generator: policy_v1 → sample₁                                  │
    │  Trainer:   train(sample₁) → policy_v2                           │
    │  Generator: policy_v1 → sample₂  ← still using v1!               │
    │  Trainer:   train(sample₂) → policy_v3                           │
    │                                                                  │
    │  Samples are "stale" - generated by older policy versions        │
    └──────────────────────────────────────────────────────────────────┘
    ```

    This **off-policy-ness** can work up to a degree, but must be limited. The generators
    need fresh weights regularly to stay "close enough" to on-policy. Weight sync frequency
    becomes a key hyperparameter trading off:

    - **Too slow**: Samples become too stale, training diverges
    - **Too fast**: Weight sync overhead dominates, negating async benefits

    ### The Scale Problem

    For LLM-based RL, the weights are **massive**. Back-of-envelope math
    (1 parameter ≈ 2 bytes in bf16):

    | Model | Weight Size |
    |-------|-------------|
    | Llama 3.1 70B | ~140 GB |
    | Llama 3.1 405B | ~810 GB |
    | DeepSeek V3 671B | ~1.3 TB |

    These weights need to move from trainer → generators regularly. If we're
    not careful, our "async RL training workload" just becomes a weight syncing
    workload. Let's look at the bandwidth hierarchy to understand why this is
    tricky and what we can do about it.
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 2. The Bandwidth Hierarchy

    For weight sync, we care about one specific data path — trainer GPU to generator
    GPU, across nodes. Here's the chain of interconnects a weight tensor traverses:

    ```
    Trainer GPU ──PCIe──► CPU ──PCIe──► NIC ══RDMA══► NIC ──PCIe──► CPU ──PCIe──► Generator GPU
      same node     (64 GB/s)    (50 GB/s)     (64 GB/s)      same node
    ```

    The bottleneck is the cross-node RDMA link at 50 GB/s per NIC. But modern nodes
    have **8 NICs** (one per GPU), so aggregate cross-node bandwidth is 400 GB/s.

    Here's how all these interconnects fit together in a full node:

    ```
    ┌──────────────────────────────────────────────────────────────────────────────────────────────────────────┐
    │                                              NODE A                                                      │
    │                                                                                                          │
    │    ┌───────────────────────────────────────────────────────────────────────────────────────────────┐     │
    │    │                              NVSwitch / NVLink Fabric                                         │     │
    │    │  ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐                      │     │
    │    │  │GPU 0 │ │GPU 1 │ │GPU 2 │ │GPU 3 │ │GPU 4 │ │GPU 5 │ │GPU 6 │ │GPU 7 │                      │     │
    │    │  └──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘                      │     │
    │    │     ########################################################################  900 GB/s NVLink │     │
    │    └─────────────────────────────────────────┬─────────────────────────────────────────────────────┘     │
    │                                              │                                                           │
    │                                         ======  64 GB/s PCIe                                             │
    │                                              │                                                           │
    │    ┌─────────┐                        ┌──────┴──┐                  ┌───────┐          ┌───────┐          │
    │    │  CPU 0  │                        │  CPU 1  │ ====== 64 GB/s ══│ NIC 0 │          │ NIC 1 │          │
    │    └────┬────┘                        └────┬────┘      PCIe        └───┬───┘          └───┬───┘          │
    │         │                                  │                           │                  │              │
    │         ══════════════════════ 64 GB/s PCIe ═══════════════════════════╪══════════════════╪              │
    │                                                                        │                  │              │
    └────────────────────────────────────────────────────────────────────────┼──────────────────┼──────────────┘
                                                                             │                  │
                                                                           ======  50 GB/s   ======
                                                                      RDMA (IB/RoCE)   RDMA (IB/RoCE)
                                                                             │                  │
                                                            ┌────────────────┴──────────────────┴────────────────┐
                                                            │                                                    │
                                                            │              InfiniBand Switch                     │
                                                            │                                                    │
                                                            └────────────────┬──────────────────┬────────────────┘
                                                                             │                  │
                                                                           ======  50 GB/s   ======
                                                                      RDMA (IB/RoCE)   RDMA (IB/RoCE)
                                                                             │                  │
    ┌────────────────────────────────────────────────────────────────────────┼──────────────────┼──────────────┐
    │                                                                        │                  │              │
    │         ══════════════════════ 64 GB/s PCIe ═══════════════════════════╪══════════════════╪              │
    │         │                                  │                           │                  │              │
    │    ┌────┴────┐                        ┌────┴────┐      PCIe        ┌───┴───┐          ┌───┴───┐          │
    │    │  CPU 0  │                        │  CPU 1  │ ====== 64 GB/s ══│ NIC 0 │          │ NIC 1 │          │
    │    └─────────┘                        └─────────┘                  └───────┘          └───────┘          │
    │                                              │                                                           │
    │                                           ======  64 GB/s PCIe                                           │
    │                                              │                                                           │
    │    ┌─────────────────────────────────────────┴─────────────────────────────────────────────────────┐     │
    │    │     ########################################################################  900 GB/s NVLink │     │
    │    │  ┌──┴───┐ ┌──┴───┐ ┌──┴───┐ ┌──┴───┐ ┌──┴───┐ ┌──┴───┐ ┌──┴───┐ ┌──┴───┐                      │     │
    │    │  │GPU 0 │ │GPU 1 │ │GPU 2 │ │GPU 3 │ │GPU 4 │ │GPU 5 │ │GPU 6 │ │GPU 7 │                      │     │
    │    │  └──────┘ └──────┘ └──────┘ └──────┘ └──────┘ └──────┘ └──────┘ └──────┘                      │     │
    │    │                              NVSwitch / NVLink Fabric                                         │     │
    │    └───────────────────────────────────────────────────────────────────────────────────────────────┘     │
    │                                              NODE B                                                      │
    └──────────────────────────────────────────────────────────────────────────────────────────────────────────┘

    Bandwidth encoding (line intensity):
      ########  NVLink/NVSwitch   900 GB/s bidirectional (GPU ↔ GPU, same node)
      ========  PCIe Gen5 / RDMA  50-64 GB/s unidirectional (CPU↔GPU, CPU↔NIC, cross-node)
      (Showing 2 of 8 NICs for clarity — each GPU has a dedicated NIC)
    ```

    ### A Note on Bandwidth Numbers

    Bandwidth specs vary by hardware generation, cluster configuration, and vendor.
    We'll use numbers from Meta's published Llama 3 training infrastructure
    ([Building Meta's GenAI Infrastructure](https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/)):

    > "Both of these solutions interconnect **400 Gbps endpoints**... we have successfully
    > used both RoCE and InfiniBand clusters for large, GenAI workloads (including our
    > ongoing training of Llama 3 on our RoCE cluster) without any network bottlenecks."

    **Important**: "400 Gbps" in networking is **full-duplex** - meaning 400 Gbps transmit
    AND 400 Gbps receive simultaneously. For weight sync (unidirectional: trainer → generator),
    we get the full 400 Gbps = **50 GB/s per NIC**.

    Meta's Grand Teton nodes have **8 RDMA NICs** (one per GPU, 1:1 mapping), giving
    400 GB/s aggregate unidirectional bandwidth per node. For more details on Grand Teton
    and Monarch's RDMA architecture, see the SIGCOMM 2024 paper:
    [RDMA over Ethernet for Distributed AI Training at Meta Scale](https://cs.stanford.edu/~keithw/sigcomm2024/sigcomm24-final246-acmpaginated.pdf).

    | Interconnect | Bandwidth | Notes |
    |--------------|-----------|-------|
    | **NVLink 4.0** | 900 GB/s bidirectional | ~450 GB/s per direction |
    | **RDMA (IB/RoCE)** | 400 Gbps = 50 GB/s | Per NIC, full-duplex |
    | **PCIe Gen5 x16** | 64 GB/s | Per direction |

    **Key observations:**

    1. **NVLink is fast but same-node only** - 450 GB/s, but can't cross the network
    2. **RDMA >> TCP** - 50 GB/s with zero-copy beats TCP significantly for cross-node
    3. **Multi-NIC scales** - 8 NICs × 50 GB/s = 400 GB/s, approaching NVLink speeds

    **Rule of thumb**: NVLink for same-node ops (gradients, activations).
    RDMA for cross-node communication (weight sync) - it's the only practical option.
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ### 2b. Back-of-Envelope: Syncing Large Models

    Let's do some quick math. DeepSeek V3 has 671B parameters (~1.34 TB in bf16).

    The key insight: **you're not shoving 1.3 TB through a single NIC**. The weights
    are distributed across many GPUs (via some combination of PP, EP, TP, FSDP),
    and each GPU has its own NIC. You get **aggregate bandwidth** across all NICs.

    The actual sync time depends on **both sides**:
    - **Trainer's aggregate upload bandwidth** (sending weights out)
    - **Generator's aggregate download bandwidth** (receiving weights)

    The bottleneck is whichever is smaller. And if multiple generators pull from
    the same trainer simultaneously, the trainer's bandwidth is divided among them.

    With Grand Teton's 8 NICs per node at 50 GB/s each (400 GB/s per node),
    the math is simple: **Time = Shard Size / Bandwidth**.

    The per-node shard size depends on how many nodes the model is spread across:
    - DeepSeek V3 (1.3 TB) across 8 nodes → ~160 GB per node
    - DeepSeek V3 (1.3 TB) across 16 nodes → ~80 GB per node

    | Per-node shard | Time to sync |
    |----------------|--------------|
    | ~160 GB (8 nodes) | 160 / 400 = **0.4 seconds** |
    | ~80 GB (16 nodes) | 80 / 400 = **0.2 seconds** |

    The exact per-node shard size depends on your parallelism strategy (PP, EP, TP, etc.),
    but the math works out: with modern RDMA hardware, you can sync even the largest
    models in **sub-second time**.

    Compare this to naive TCP: kernel copies, socket overhead, no zero-copy...
    easily 10x slower. **RDMA is the only way to make async RL practical at scale.**
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 3. The Problem: Collectives Are Blocking

    Most people use RDMA via **collectives** through PyTorch distributed:

    ```python
    import torch.distributed as dist

    dist.init_process_group(backend="nccl")
    dist.all_reduce(gradients, op=dist.ReduceOp.SUM)
    dist.broadcast(weights, src=0)
    ```

    This works great for training. But async RL has a different access pattern.

    ### High Variance in Generation Times

    Generators have wildly different completion times:
    - Some prompts → 10 tokens (fast)
    - Other prompts → 1000 tokens (slow)

    With collectives, fast generators wait for slow ones:

    ```
    Generator 0: ├── gen (fast) ──┤  ⚠️ WAITING...
    Generator 1: ├────── gen (slow) ──────┤
    Generator 2: ├── gen (fast) ──┤  ⚠️ WAITING...
                                          ↓
                              all_gather(weights)  # Everyone waits!
    ```

    ### The One-Sided Solution: RDMA

    What if the sender could write directly to the receiver's memory without coordination?

    ```
    Two-sided (send/recv):
      Sender: "I have data"  ──────────►  Receiver: "I'm ready"
      Sender: sends data     ──────────►  Receiver: receives data
                             2 messages required

    One-sided (RDMA):
      Sender: writes directly to receiver's memory
                             No coordination needed!
    ```

    This is what RDMA enables: **one-sided memory operations**.
    The trainer doesn't even know when generators pull weights - this is truly async!

    RDMA isn't just "faster TCP" — it bypasses the kernel entirely. No socket buffers,
    no context switches, no serialization. The NIC reads directly from registered memory.
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 4. The Magic Pointer Pattern

    One natural question that arises is along the lines of, "How do we actually represent one-sided puts/gets if not with NCCL collectives?"

    Here's a key insight: to represent remote data, we only need a **tiny handle** -
    an `(addr, rkey, size)` tuple that says "here's where my data lives."

    Monarch wraps this in `RDMABuffer`. Let's see how small it actually is:
    """)
    return


@app.cell
def _(Actor, RDMABuffer, endpoint, is_rdma_available, this_host, torch):
    # Measure actual size of RDMABuffer handles
    import pickle

    def show_fallback_sizes():
        """Fallback: show expected sizes based on RDMABuffer structure."""
        print("(RDMA not available - showing expected handle sizes)\n")
        print("RDMABuffer contains: addr (8B) + rkey (4B) + size (8B) + owner (~100B)")
        print("Total serialized size: ~150-200 bytes regardless of tensor size\n")

        sizes = [("1 KB", 1024), ("1 MB", 1024**2), ("1 GB", 1024**3)]
        handle_bytes = 150  # approximate

        for name, tensor_bytes in sizes:
            ratio = tensor_bytes / handle_bytes
            print(f"{name:<8} tensor → ~150 byte handle → {ratio:,.0f}x compression")

        print("\n→ Handle size is O(1) regardless of tensor size!")

    try:
        if not is_rdma_available():
            show_fallback_sizes()
        else:
            class BufferSizeDemo(Actor):
                """Actor that creates RDMABuffers and measures their size."""

                @endpoint
                def measure_buffer_sizes(self) -> list:
                    import pickle as _pickle
                    results = []
                    sizes = [
                        ("1 KB", 256),
                        ("1 MB", 256 * 1024),
                        ("10 MB", 256 * 1024 * 10),
                    ]

                    for name, numel in sizes:
                        tensor = torch.randn(numel)
                        tensor_bytes = tensor.numel() * tensor.element_size()

                        byte_tensor = tensor.view(torch.uint8).flatten()
                        buffer = RDMABuffer(byte_tensor)
                        handle_bytes = len(_pickle.dumps(buffer))

                        results.append((name, tensor_bytes, handle_bytes))

                    return results

            proc = this_host().spawn_procs(per_host={"procs": 1})
            demo = proc.spawn("buffer_demo", BufferSizeDemo)

            results = demo.measure_buffer_sizes.call_one().get()

            print("RDMABuffer handle size vs actual tensor size:\n")
            print(f"{'Tensor Size':<12} {'Actual Bytes':<15} {'Handle Size':<15} {'Ratio':<10}")
            print("-" * 55)

            for name, tensor_bytes, handle_bytes in results:
                ratio = tensor_bytes / handle_bytes
                print(f"{name:<12} {tensor_bytes:>12,} B   {handle_bytes:>6} B        {ratio:>8,.0f}x")

            print("\n→ Handle size is O(1) regardless of tensor size!")

    except Exception as e:
        print(f"(RDMA setup failed: {e})\n")
        show_fallback_sizes()
    return


@app.cell
def _(mo):
    mo.md(r"""
    ### The Magic Pointer

    This is the core pattern: **separate control plane from data plane**.

    - **Control plane** (actor messages): Send tiny handle (~100 bytes)
    - **Data plane** (RDMA): Bulk transfer of actual data (~10 GB)

    Think of `RDMABuffer` as a **magic pointer** - it's a pointer that works across machines:

    ```
    Trainer                              Generator
    ┌─────────────┐                     ┌─────────────┐
    │ weights     │                     │ local copy  │
    │ (10 GB)     │                     │ (empty)     │
    └──────┬──────┘                     └──────┬──────┘
           │                                   │
           │  1. Create RDMABuffer             │
           │     (register memory, get handle) │
           │                                   │
           ├─────── 2. Send handle ───────────►│  (~100 bytes via actor)
           │                                   │
           │◄────── 3. RDMA read ──────────────┤  (~10 GB via hardware)
           │        (no trainer involvement!)  │
    ```

    The trainer doesn't even know when generators pull weights. True one-sided.

    ### RDMABuffer in Action

    ```python
    from monarch.rdma import RDMABuffer

    # Trainer side: register weights
    weights = torch.randn(1024, 1024, device="cuda")
    buffer = RDMABuffer(weights.view(torch.uint8).flatten())

    # Return buffer as part of an endpoint response
    @endpoint
    def get_weight_handle(self) -> RDMABuffer:
        return self.buffer

    # Generator side: receive handle, pull directly into GPU
    handle = trainer.get_weight_handle.call_one().get()  # Tiny message
    gpu_weights = model.weights.view(torch.uint8).flatten()
    handle.read_into(gpu_weights).get()                   # Bulk RDMA transfer

    # Push model: caller writes local src_tensor into the remote RDMABuffer
    buffer.write_from(src_tensor).get()
    ```

    **Pull vs Push**: `read_into` is the **pull** model (generator reads remote data into
    its local buffer), while `write_from` is the **push** model (caller writes local data
    into the remote buffer). Both are one-sided RDMA operations — the remote side is not
    involved. In async RL, pull is more natural because each generator decides *when* it
    needs fresh weights.

    **Want to understand how RDMA works under the hood?** Check out **06b_weight_sync_deep_dive.py**
    for ibverbs internals, queue pair setup, and why Monarch's actor model is such a natural fit
    for managing RDMA connections. It's actors all the way down!
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ### Live Demo: Trainer → Generator Weight Sync

    Let's see this in action with a simple example. A trainer holds weights,
    a generator pulls them via RDMA.
    """)
    return


@app.cell
def _(Actor, RDMABuffer, endpoint, is_rdma_available, this_host, torch):
    # Simple trainer → generator weight sync demo

    def show_fallback_demo():
        """Show what would happen with RDMA."""
        print("(RDMA not available - showing conceptual flow)\n")
        print("1. Trainer creates weights (e.g., 4 MB tensor)")
        print("2. Trainer wraps weights in RDMABuffer → tiny handle (~150 bytes)")
        print("3. Trainer sends handle to Generator via actor message")
        print("4. Generator calls handle.read_into(local_buffer)")
        print("5. RDMA hardware transfers 4 MB directly, trainer not involved!")
        print("\n→ Zero-copy, one-sided, no serialization overhead")

    try:
        if not is_rdma_available():
            show_fallback_demo()
        else:
            class Sender(Actor):
                """Sender that holds data and exposes an RDMA handle."""

                def __init__(self, size: int):
                    # Create some data to send
                    self.data = torch.randn(size, dtype=torch.float32)
                    # Register with RDMA
                    self.data_bytes = self.data.view(torch.uint8).flatten()
                    self.buffer = RDMABuffer(self.data_bytes)
                    print(f"[Sender] Created data: {self.data.numel() * 4 / 1e6:.1f} MB")

                @endpoint
                def get_handle(self) -> RDMABuffer:
                    """Return tiny handle (not the data itself!)"""
                    return self.buffer

                @endpoint
                def get_checksum(self) -> float:
                    """For verification: sum of data"""
                    return float(self.data.sum())

            class Receiver(Actor):
                """Receiver that pulls data from sender via RDMA."""

                def __init__(self, size: int):
                    # Pre-allocate space for data
                    self.data = torch.zeros(size, dtype=torch.float32)
                    self.data_bytes = self.data.view(torch.uint8).flatten()
                    print(f"[Receiver] Allocated buffer: {self.data.numel() * 4 / 1e6:.1f} MB")

                @endpoint
                def pull_data(self, handle: RDMABuffer) -> float:
                    """Pull data via RDMA read, return checksum for verification."""
                    # This is the magic: RDMA read directly into our buffer
                    handle.read_into(self.data_bytes).get()
                    return float(self.data.sum())

            # Spawn sender and receiver
            procs = this_host().spawn_procs(per_host={"procs": 2})

            sender = procs.slice(procs=0).spawn("sender", Sender, 1024 * 1024)  # 4 MB
            receiver = procs.slice(procs=1).spawn("receiver", Receiver, 1024 * 1024)

            # Step 1: Get handle from sender (tiny message!)
            handle = sender.get_handle.call_one().get()
            print(f"\n[Orchestrator] Got handle from sender")

            # Step 2: Send handle to receiver, have it pull data
            receiver_checksum = receiver.pull_data.call_one(handle).get()
            sender_checksum = sender.get_checksum.call_one().get()

            print(f"[Orchestrator] Sender checksum: {sender_checksum:.2f}")
            print(f"[Orchestrator] Receiver checksum: {receiver_checksum:.2f}")
            print(f"[Orchestrator] Match: {abs(sender_checksum - receiver_checksum) < 0.01}")

    except Exception as e:
        print(f"(Demo failed: {e})\n")
        show_fallback_demo()
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 5. CPU Staging Pattern

    ### GPU-Native RDMA Exists (and Monarch Supports It)

    GPU-native RDMA (GPUDirect) is real and works well - the NIC reads directly from GPU
    memory with no CPU copy. Monarch supports this at the Rust level. For synchronous
    bulk transfers, it's excellent.

    ### CPU Staging: A Deliberate Architectural Choice

    For async RL, CPU staging isn't a workaround for missing GPUDirect - it's the
    **preferred production pattern**. The reason is **temporal decoupling**: trainers
    and generators operate on completely independent timelines, and we need a buffer
    between them.

    The issue isn't bandwidth - it's **timing**:

    ```
    Direct GPU→GPU RDMA:
    ┌──────────────────────────────────────────────────────┐
    │ Generator GPU is mid-inference                       │
    │ ├── layer 1 ──┤ [RDMA arrives, needs sync!]          │
    │               ↓                                      │
    │         cudaDeviceSynchronize()  ← Blocks inference! │
    └──────────────────────────────────────────────────────┘
    ```

    With CPU staging, nothing on the critical path blocks:

    ```
    Trainer GPU ──► CPU staging buffer (RDMA registered)
                          │
                          │ [Sits here, ready anytime]
                          │
                          ▼
    Generator grabs when ready ──► Generator GPU
    ```

    The CPU buffer is a **temporal decoupling point**.

    Note: the CPU staging path does involve GPU↔CPU copies on each end. When we say
    RDMA is "zero-copy," we mean across the network — the NIC reads/writes directly
    from/to registered CPU memory with no kernel involvement.
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 6. Circular Weight Buffers

    ### One-Sided Isn't Free

    One-sided RDMA is powerful - trainers and generators can operate independently without
    explicit send/recv coordination. But "no coordination" isn't quite right. There's still
    a fundamental race condition lurking: **what if the trainer overwrites a buffer while a
    generator is reading from it?**

    With a single buffer, this is a real problem:

    ```
    Trainer: write v3 to buffer ──────────►  [buffer: v3...v2...v3]  ← corrupted!
    Generator: reading v2 from buffer ────►  read gets mix of v2 and v3
    ```

    We need *some* form of coordination. The question is: do we go back to message-passing
    (explicit locks, barriers, acknowledgments) and lose the async benefits? Or can we get
    coordination from the **structure itself**?

    ### Solution: Circular Buffer as Structural Coordination

    The key insight: GPU memory is scarce, but **CPU memory is abundant**. We can afford to
    keep multiple versions of the weights in CPU staging buffers. By writing to slots in a
    circular pattern, the trainer never overwrites a slot that a generator might still be
    reading - as long as we have enough slots to cover the timing gap.

    This is **structural coordination**: the circular buffer's design eliminates the race
    condition without any explicit synchronization messages between trainer and generator.

    ```
    Trainer writes:     v0    →  v1  →  v2  →  v3  →  v4  →  v5  → ...
                         ↓        ↓      ↓
    Buffer slots:      [slot0] [slot1] [slot2]  (circular, reused)
                         v3      v4      v5

    Generator reads: "Give me latest" → v5
    ```

    Benefits:
    - **No message-based coordination** - structure prevents races, not locks
    - **Pre-registered RDMA buffers** - no memory registration on hot path
    - **Lock-free reads** - generators always get a consistent snapshot
    - **Bounded memory** - only N versions in flight

    **Memory cost is real**: for a 70B model (140 GB in bf16), 5 slots = 700 GB of CPU RAM.
    This is feasible on HPC nodes (Grand Teton has ~1.5 TB RAM per node), but `n_slots` is
    bounded by available CPU memory, not just timing.

    The key design constraint: register all slots at init time, then just write to them.
    No allocation, no registration on the critical path. Tune `n_slots` so that the trainer
    can't lap the slowest generator — if it does, the generator reads a corrupted mix of
    two versions (not a graceful error, just silent data corruption).

    **Want to see a full implementation?** Check out **06b_weight_sync_deep_dive.py** for a
    complete `CircularWeightBuffer` class with versioning and RDMA integration.
    """)
    return


@app.cell
def _(mo):
    circ_slots_slider = mo.ui.slider(1, 8, value=3, label="Buffer slots (n_slots)")
    circ_writes_slider = mo.ui.slider(1, 10, value=2, label="Trainer writes per generator sync")
    mo.vstack([
        mo.md("**Try it**: adjust slots and write speed to see when lapping causes a race condition."),
        mo.hstack([circ_slots_slider, circ_writes_slider], justify="center", gap=1),
    ])
    return circ_slots_slider, circ_writes_slider


@app.cell
def _(circ_slots_slider, circ_writes_slider, mo):
    _n = circ_slots_slider.value
    _w = circ_writes_slider.value
    _safe = _w < _n

    # Build SVG showing circular buffer slots with write pointer and read pointer
    _slot_w, _slot_h = 64, 48
    _gap = 6
    _pad = 30
    _total_w = max(_n * (_slot_w + _gap) - _gap + 2 * _pad, 300)
    _total_h = 130

    _parts = [
        f'<svg xmlns="http://www.w3.org/2000/svg" width="{_total_w}" height="{_total_h}" '
        f'style="font-family: -apple-system, sans-serif;">'
    ]

    # Title
    _parts.append(
        f'<text x="{_total_w / 2}" y="18" text-anchor="middle" '
        f'font-size="12" fill="#666">Trainer writes {_w} version{"s" if _w != 1 else ""} '
        f'while generator reads from slot 0</text>'
    )

    _slot_y = 50
    for _i in range(_n):
        _x = _pad + _i * (_slot_w + _gap)

        # Which trainer write steps land on this slot?
        # Trainer writes to slots 1, 2, ..., wrapping around
        _write_steps = [j + 1 for j in range(_w) if (1 + j) % _n == _i]
        _is_gen_read = (_i == 0)
        _is_written = len(_write_steps) > 0
        _collision = _is_gen_read and _is_written

        # Colors
        if _collision:
            _fill, _stroke = "#fecaca", "#dc2626"
        elif _is_gen_read:
            _fill, _stroke = "#dbeafe", "#2563eb"
        elif _is_written:
            _fill, _stroke = "#dcfce7", "#16a34a"
        else:
            _fill, _stroke = "#f3f4f6", "#9ca3af"

        # Slot rectangle
        _parts.append(
            f'<rect x="{_x}" y="{_slot_y}" width="{_slot_w}" height="{_slot_h}" rx="5" '
            f'fill="{_fill}" stroke="{_stroke}" stroke-width="2"/>'
        )
        # Slot label
        _parts.append(
            f'<text x="{_x + _slot_w / 2}" y="{_slot_y + _slot_h / 2}" text-anchor="middle" '
            f'dominant-baseline="central" font-size="13" font-weight="bold" '
            f'fill="{_stroke}">slot {_i}</text>'
        )
        # Write step markers above slot
        if _write_steps:
            _step_label = ",".join(str(s) for s in _write_steps)
            _marker_color = "#dc2626" if _collision else "#16a34a"
            _parts.append(
                f'<text x="{_x + _slot_w / 2}" y="{_slot_y - 8}" text-anchor="middle" '
                f'font-size="10" fill="{_marker_color}">write #{_step_label}</text>'
            )
        # Generator reading label below slot 0
        if _is_gen_read:
            _gen_label = (
                "▲ gen reading — OVERWRITTEN!" if _collision else "▲ gen reading"
            )
            _gen_color = "#dc2626" if _collision else "#2563eb"
            _gen_weight = "bold" if _collision else "normal"
            _parts.append(
                f'<text x="{_x + _slot_w / 2}" y="{_slot_y + _slot_h + 16}" '
                f'text-anchor="middle" font-size="10" font-weight="{_gen_weight}" '
                f'fill="{_gen_color}">{_gen_label}</text>'
            )

    _parts.append("</svg>")
    _svg = "\n".join(_parts)

    if _safe:
        _callout = mo.callout(mo.md(
            f"**Safe** — {_n} slots, {_w} trainer writes between generator reads. "
            f"Generator finishes reading slot 0 before trainer wraps around to overwrite it."
        ), kind="success")
    else:
        _callout = mo.callout(mo.md(
            f"**RACE CONDITION** — {_n} slots, {_w} trainer writes per generator read. "
            f"Trainer wraps around and overwrites slot 0 on write #{_n} while generator is "
            f"still reading it! Result: corrupted weights (silent data corruption, not an error).\n\n"
            f"**Fix**: increase to at least **{_w + 1} slots**."
        ), kind="danger")

    mo.vstack([mo.Html(_svg), _callout])
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 7. Weight Re-sharding

    ### The Sharding Mismatch Problem

    Trainer and Generator often have **different tensor layouts**. Consider an example:

    | Role | Parallelism | Sharding |
    |------|-------------|----------|
    | Trainer | FSDP (8 GPUs) | `Shard(0)` - rows split across 8 GPUs |
    | Generator | TP (2 GPUs) | `Shard(1)` - columns split across 2 GPUs |

    Therefore we cannot always directly transfer weights - we need **re-sharding**.

    Consider a simple example where the trainer may be row-sharded and the generator may be column-sharded:

    ```
    Trainer (row-sharded):          Generator (column-sharded):
    ┌──────────────────┐            ┌─────────┬─────────┐
    │ GPU 0: rows 0-127│            │ GPU 0   │ GPU 1   │
    ├──────────────────┤     →      │ cols    │ cols    │
    │ GPU 1: rows 128+ │            │ 0-511   │ 512+    │
    └──────────────────┘            └─────────┴─────────┘
    ```

    ### Two Approaches

    **Gather Then Slice** (simple but wasteful):
    One approach is to materialize the entire tensor, i.e. `gather`, transfer the full tensor, and then slice on the receiver side:
    1. Each receiver gathers ALL sender shards → full tensor
    2. Each receiver slices out its portion
    3. **Problem**: 2x redundant data transfer

    **Routed Transfer** (optimal):
    A more efficient approach is to only transfer the data that needs to be transferred:
    1. Pre-compute which sender chunks overlap with which receiver regions
    2. Send only the exact chunks needed
    3. **Benefit**: Minimal bandwidth, no redundancy

    ```
    GATHER: G0 receives T0,T1,T2,T3 → discards T2,T3 (50% waste!)
    ROUTED: G0 receives T0,T1 only → exactly what it needs
    ```

    The routed approach batches all needed transfers into one plan.
    Pre-compute the plan once at handshake, execute it on each sync.

    **Want to see the overlap computation and benchmarks?** Check out **06b_weight_sync_deep_dive.py**
    for the full DTensor re-sharding implementation with placement-aware routing.
    """)
    return


@app.cell
def _():
    from dataclasses import dataclass
    from typing import List, Tuple

    @dataclass
    class ShardMetadata:
        """Metadata describing a tensor shard."""
        rank: int
        global_shape: Tuple[int, ...]
        offset: Tuple[int, ...]  # Start position in global tensor
        local_shape: Tuple[int, ...]  # Shape of this shard

    @dataclass
    class TransferChunk:
        """A chunk to transfer from sender to receiver."""
        sender_rank: int
        receiver_rank: int
        sender_offset: Tuple[int, int]  # Where to read from sender
        receiver_offset: Tuple[int, int]  # Where to write in receiver
        shape: Tuple[int, int]  # Shape of the chunk

    def compute_shard_metadata(
        global_shape: Tuple[int, int],
        num_ranks: int,
        shard_dim: int,
    ) -> List[ShardMetadata]:
        """Compute shard metadata for a given sharding."""
        shards = []
        dim_size = global_shape[shard_dim]
        shard_size = dim_size // num_ranks

        for rank in range(num_ranks):
            offset = [0, 0]
            local_shape = list(global_shape)

            offset[shard_dim] = rank * shard_size
            local_shape[shard_dim] = shard_size

            shards.append(ShardMetadata(
                rank=rank,
                global_shape=global_shape,
                offset=tuple(offset),
                local_shape=tuple(local_shape),
            ))

        return shards

    def compute_overlap(
        sender: ShardMetadata,
        receiver: ShardMetadata,
    ) -> "TransferChunk | None":
        """Compute overlap between sender and receiver shards."""
        s_start = sender.offset
        s_end = (s_start[0] + sender.local_shape[0], s_start[1] + sender.local_shape[1])

        r_start = receiver.offset
        r_end = (r_start[0] + receiver.local_shape[0], r_start[1] + receiver.local_shape[1])

        inter_start = (max(s_start[0], r_start[0]), max(s_start[1], r_start[1]))
        inter_end = (min(s_end[0], r_end[0]), min(s_end[1], r_end[1]))

        if inter_start[0] >= inter_end[0] or inter_start[1] >= inter_end[1]:
            return None

        shape = (inter_end[0] - inter_start[0], inter_end[1] - inter_start[1])

        sender_local = (inter_start[0] - s_start[0], inter_start[1] - s_start[1])
        receiver_local = (inter_start[0] - r_start[0], inter_start[1] - r_start[1])

        return TransferChunk(
            sender_rank=sender.rank,
            receiver_rank=receiver.rank,
            sender_offset=sender_local,
            receiver_offset=receiver_local,
            shape=shape,
        )

    def compute_transfer_plan(
        sender_shards: List[ShardMetadata],
        receiver_shards: List[ShardMetadata],
    ) -> List[TransferChunk]:
        """Compute all transfers needed for re-sharding."""
        transfers = []
        for sender in sender_shards:
            for receiver in receiver_shards:
                chunk = compute_overlap(sender, receiver)
                if chunk is not None:
                    transfers.append(chunk)
        return transfers

    def render_resharding_svg(trainer_shards, gen_shards, transfer_plan, global_shape):
        """Render a color-matched visualization of the re-sharding transfer plan.

        Returns HTML string with:
        - Two grids (trainer left, generator right) where each transfer chunk
          is colored the same on both sides so you can visually match them.
        - A transfer matrix table below showing which chunks move where.
        """
        # Layout constants
        grid_w, grid_h = 280, 240
        left_x, right_x = 80, 520
        top_y = 50
        svg_w = 880
        rows, cols = global_shape

        # Generate distinct colors for each transfer
        # Use HSL with evenly spaced hues for maximum distinction
        n_transfers = len(transfer_plan)
        transfer_colors = []
        for i in range(max(n_transfers, 1)):
            hue = (i * 360 // max(n_transfers, 1) + 10) % 360
            transfer_colors.append(f"hsl({hue}, 70%, 55%)")

        # Also need light shard background colors (just for shard boundaries)
        shard_border_color = "#444"

        parts = []
        svg_h = top_y + grid_h + 50
        parts.append(
            f'<svg xmlns="http://www.w3.org/2000/svg" width="{svg_w}" height="{svg_h}" '
            f'viewBox="0 0 {svg_w} {svg_h}" '
            f'style="font-family: -apple-system, sans-serif; font-size: 12px;">'
        )

        # Title labels
        parts.append(f'<text x="{left_x + grid_w // 2}" y="22" text-anchor="middle" '
                      f'font-weight="bold" font-size="15">Trainer shards</text>')
        parts.append(f'<text x="{right_x + grid_w // 2}" y="22" text-anchor="middle" '
                      f'font-weight="bold" font-size="15">Generator shards</text>')

        # Subtitle showing shard dim
        t_dim = "row" if (trainer_shards and trainer_shards[0].offset == (0, 0)
                          and len(trainer_shards) > 1
                          and trainer_shards[1].offset[0] > 0) else "col"
        g_dim = "row" if (gen_shards and gen_shards[0].offset == (0, 0)
                          and len(gen_shards) > 1
                          and gen_shards[1].offset[0] > 0) else "col"
        parts.append(f'<text x="{left_x + grid_w // 2}" y="38" text-anchor="middle" '
                      f'font-size="11" fill="#666">{len(trainer_shards)} GPUs, {t_dim}-sharded</text>')
        parts.append(f'<text x="{right_x + grid_w // 2}" y="38" text-anchor="middle" '
                      f'font-size="11" fill="#666">{len(gen_shards)} GPUs, {g_dim}-sharded</text>')

        # Helper: map global (row, col, h, w) to pixel rect
        def to_px(base_x, r, c, h, w):
            px = base_x + (c / cols) * grid_w
            py = top_y + (r / rows) * grid_h
            pw = (w / cols) * grid_w
            ph = (h / rows) * grid_h
            return px, py, pw, ph

        # Draw transfer chunks as colored rectangles on BOTH grids
        for i, t in enumerate(transfer_plan):
            color = transfer_colors[i]

            # Find sender/receiver shard global offsets
            s_shard = next(s for s in trainer_shards if s.rank == t.sender_rank)
            r_shard = next(s for s in gen_shards if s.rank == t.receiver_rank)

            # Trainer side: chunk in global coords
            s_row = s_shard.offset[0] + t.sender_offset[0]
            s_col = s_shard.offset[1] + t.sender_offset[1]
            px, py, pw, ph = to_px(left_x, s_row, s_col, t.shape[0], t.shape[1])
            parts.append(
                f'<rect x="{px:.1f}" y="{py:.1f}" width="{pw:.1f}" height="{ph:.1f}" '
                f'fill="{color}" fill-opacity="0.55" stroke="{color}" stroke-width="0.5"/>'
            )

            # Generator side: chunk in global coords
            r_row = r_shard.offset[0] + t.receiver_offset[0]
            r_col = r_shard.offset[1] + t.receiver_offset[1]
            px, py, pw, ph = to_px(right_x, r_row, r_col, t.shape[0], t.shape[1])
            parts.append(
                f'<rect x="{px:.1f}" y="{py:.1f}" width="{pw:.1f}" height="{ph:.1f}" '
                f'fill="{color}" fill-opacity="0.55" stroke="{color}" stroke-width="0.5"/>'
            )

        # Draw shard boundary outlines and rank labels on top
        def draw_shard_outlines(shards, base_x, prefix):
            for s in shards:
                x = base_x + (s.offset[1] / cols) * grid_w
                y = top_y + (s.offset[0] / rows) * grid_h
                w = (s.local_shape[1] / cols) * grid_w
                h = (s.local_shape[0] / rows) * grid_h
                parts.append(
                    f'<rect x="{x:.1f}" y="{y:.1f}" width="{w:.1f}" height="{h:.1f}" '
                    f'fill="none" stroke="{shard_border_color}" stroke-width="2"/>'
                )
                # Rank label
                cx, cy = x + w / 2, y + h / 2
                parts.append(
                    f'<text x="{cx:.1f}" y="{cy:.1f}" text-anchor="middle" '
                    f'dominant-baseline="central" font-weight="bold" font-size="13" '
                    f'fill="#222">{prefix}{s.rank}</text>'
                )

        draw_shard_outlines(trainer_shards, left_x, "T")
        draw_shard_outlines(gen_shards, right_x, "G")

        # Outer boxes
        parts.append(f'<rect x="{left_x}" y="{top_y}" width="{grid_w}" height="{grid_h}" '
                      f'fill="none" stroke="#222" stroke-width="2.5"/>')
        parts.append(f'<rect x="{right_x}" y="{top_y}" width="{grid_w}" height="{grid_h}" '
                      f'fill="none" stroke="#222" stroke-width="2.5"/>')

        # "=" sign between grids to show equivalence
        mid_x = (left_x + grid_w + right_x) // 2
        mid_y = top_y + grid_h // 2
        parts.append(f'<text x="{mid_x}" y="{mid_y}" text-anchor="middle" '
                      f'dominant-baseline="central" font-size="28" fill="#888">=</text>')
        parts.append(f'<text x="{mid_x}" y="{mid_y + 22}" text-anchor="middle" '
                      f'font-size="10" fill="#888">same data</text>')

        # Global shape label
        parts.append(
            f'<text x="{svg_w // 2}" y="{svg_h - 8}" text-anchor="middle" '
            f'font-size="11" fill="#888">Global tensor: {rows} x {cols}</text>'
        )

        parts.append("</svg>")
        svg_str = "\n".join(parts)

        # Build transfer matrix HTML table
        n_trainers = len(trainer_shards)
        n_gens = len(gen_shards)

        # Build lookup: (sender_rank, receiver_rank) -> (transfer_index, chunk)
        transfer_lookup = {}
        for i, t in enumerate(transfer_plan):
            transfer_lookup[(t.sender_rank, t.receiver_rank)] = (i, t)

        table_parts = [
            '<table style="border-collapse: collapse; margin-top: 12px; font-family: monospace; font-size: 12px;">',
            '<tr><th style="border: 1px solid #ccc; padding: 6px 10px; background: #f5f5f5;"></th>',
        ]
        for g in range(n_gens):
            table_parts.append(
                f'<th style="border: 1px solid #ccc; padding: 6px 10px; background: #f5f5f5;">G{g}</th>'
            )
        table_parts.append('</tr>')

        for t_rank in range(n_trainers):
            table_parts.append(
                f'<tr><td style="border: 1px solid #ccc; padding: 6px 10px; '
                f'background: #f5f5f5; font-weight: bold;">T{t_rank}</td>'
            )
            for g_rank in range(n_gens):
                key = (t_rank, g_rank)
                if key in transfer_lookup:
                    idx, chunk = transfer_lookup[key]
                    color = transfer_colors[idx]
                    table_parts.append(
                        f'<td style="border: 1px solid #ccc; padding: 6px 10px; '
                        f'background: {color}; color: white; text-align: center; '
                        f'font-weight: bold; text-shadow: 0 1px 2px rgba(0,0,0,0.4);">'
                        f'{chunk.shape[0]}x{chunk.shape[1]}</td>'
                    )
                else:
                    table_parts.append(
                        '<td style="border: 1px solid #ccc; padding: 6px 10px; '
                        'text-align: center; color: #ccc;">&mdash;</td>'
                    )
            table_parts.append('</tr>')

        table_parts.append('</table>')
        table_str = "\n".join(table_parts)

        # Combine SVG + table
        return (
            f'<div style="display: flex; flex-direction: column; align-items: center;">'
            f'{svg_str}'
            f'<div style="margin-top: 4px; font-size: 12px; color: #666;">'
            f'Matching colors = same data chunk. Matrix shows chunk shapes transferred.</div>'
            f'{table_str}'
            f'</div>'
        )
    return compute_shard_metadata, compute_transfer_plan, render_resharding_svg


@app.cell
def _(mo):
    # Interactive controls for re-sharding visualization
    trainer_gpu_slider = mo.ui.slider(1, 8, value=4, label="Trainer GPUs")
    gen_gpu_slider = mo.ui.slider(1, 8, value=2, label="Generator GPUs")
    trainer_dim_dropdown = mo.ui.dropdown(
        options={"Row (dim 0)": 0, "Col (dim 1)": 1},
        value="Row (dim 0)",
        label="Trainer shard dim",
    )
    gen_dim_dropdown = mo.ui.dropdown(
        options={"Row (dim 0)": 0, "Col (dim 1)": 1},
        value="Col (dim 1)",
        label="Generator shard dim",
    )

    mo.hstack(
        [trainer_gpu_slider, trainer_dim_dropdown, gen_gpu_slider, gen_dim_dropdown],
        justify="center",
        gap=1,
    )
    return (
        gen_dim_dropdown,
        gen_gpu_slider,
        trainer_dim_dropdown,
        trainer_gpu_slider,
    )


@app.cell
def _(
    compute_shard_metadata,
    compute_transfer_plan,
    gen_dim_dropdown,
    gen_gpu_slider,
    mo,
    render_resharding_svg,
    trainer_dim_dropdown,
    trainer_gpu_slider,
):
    # Compute shards and transfer plan based on widget values
    _global_shape = (512, 512)
    _t_shards = compute_shard_metadata(_global_shape, trainer_gpu_slider.value, trainer_dim_dropdown.value)
    _g_shards = compute_shard_metadata(_global_shape, gen_gpu_slider.value, gen_dim_dropdown.value)
    _plan = compute_transfer_plan(_t_shards, _g_shards)

    _svg = render_resharding_svg(_t_shards, _g_shards, _plan, _global_shape)

    # Compute stats for the callout
    _routed_bytes = sum(t.shape[0] * t.shape[1] * 2 for t in _plan)  # bf16 = 2 bytes
    _gather_bytes = _global_shape[0] * _global_shape[1] * 2 * gen_gpu_slider.value
    _waste_pct = (1 - _routed_bytes / _gather_bytes) * 100 if _gather_bytes > 0 else 0

    resharding_stats = {
        "transfers": len(_plan),
        "routed_bytes": _routed_bytes,
        "gather_bytes": _gather_bytes,
        "waste_pct": _waste_pct,
    }

    _stats_md = mo.md(f"""
    **Transfer Plan Stats** | **Transfers**: {len(_plan)} chunks | **Routed**: {_routed_bytes / 1024:.1f} KB | **Gather**: {_gather_bytes / 1024:.1f} KB | **Saved**: {_waste_pct:.0f}%
    """)

    mo.vstack([mo.Html(_svg), mo.callout(_stats_md, kind="info")])
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 8. Putting It All Together

    The full async RL weight sync pattern:

    ```
    ┌─────────────────────────────────────────────────────────────────┐
    │                         TRAINER                                 │
    │  1. Train step completes                                        │
    │  2. Copy weights to CPU staging buffer (non-blocking D2H)       │
    │  3. Publish to circular buffer with version tag                 │
    │  4. Continue training (no blocking!)                            │
    └─────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
    ┌─────────────────────────────────────────────────────────────────┐
    │              CIRCULAR BUFFER (CPU, RDMA-registered)             │
    │  [slot 0: v3] [slot 1: v4] [slot 2: v5]                         │
    │                                 ↑ latest                        │
    └─────────────────────────────────────────────────────────────────┘
                                    │
              ┌─────────────────────┼─────────────────────┐
              ▼                     ▼                     ▼
    ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐
    │   GENERATOR 0   │   │   GENERATOR 1   │   │   GENERATOR 2   │
    │                 │   │                 │   │                 │
    │ After gen done: │   │ After gen done: │   │ After gen done: │
    │ 1. Get latest   │   │ 1. Get latest   │   │ 1. Get latest   │
    │    version      │   │    version      │   │    version      │
    │ 2. RDMA read    │   │ 2. RDMA read    │   │ 2. RDMA read    │
    │    → GPU        │   │    → GPU        │   │    → GPU        │
    │ 3. Re-shard if  │   │ 3. Re-shard if  │   │ 3. Re-shard if  │
    │    needed       │   │    needed       │   │    needed       │
    └─────────────────┘   └─────────────────┘   └─────────────────┘
    ```

    **Key properties:**
    - Trainer never blocks waiting for generators
    - Generators pull directly to GPU when *they're* ready
    - Re-sharding happens locally on each generator
    - Circular buffer bounds memory, reuses RDMA registrations
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ### Live Demo: Async Weight Sync

    Let's see this in action! We'll simulate the core async RL pattern:

    - **1 Trainer**: Runs training steps, publishes new weights to a 3-slot circular buffer
    - **4 Generators**: Each independently syncs to latest weights, then generates

    All 5 actors run **concurrently and independently**. The trainer never waits for generators,
    and each generator grabs weights whenever it's ready (at slightly different rates to show
    the async behavior). To verify correctness, we set weights to `version` and check on read.

    **Race condition safety**: The circular buffer's slot count is tuned so that
    `buffer_size × update_interval >> sync_time`. This ensures a slot isn't overwritten
    while a generator is reading it. In this demo: 5 slots, ~1s between trainer updates,
    RDMA sync takes ~ms, so a race is effectively impossible. In production, tune
    `buffer_size` based on actual sync time and update frequency.
    """)
    return


@app.cell
def _(
    Actor,
    RDMABuffer,
    current_rank,
    endpoint,
    is_rdma_available,
    this_host,
    torch,
):
    import threading

    def show_fallback():
        print("(RDMA not available - showing conceptual flow)\n")
        print("What would happen with RDMA:")
        print("  [Trainer] Publishes v0, v1, v2... to circular buffer")
        print("  [Generator] Syncs when ready, verifies weights match version")
        print("  Both run independently, no blocking!")

    try:
        if not is_rdma_available():
            show_fallback()
        else:
            class Trainer(Actor):
                """Trainer with circular buffer for weight versioning."""

                def __init__(self, weight_size: int):
                    self.n_slots = 5
                    self.version = 0
                    self.slots = []
                    self.handles = []
                    for _ in range(self.n_slots):
                        slot = torch.zeros(weight_size, dtype=torch.float32)
                        self.slots.append(slot)
                        self.handles.append(RDMABuffer(slot.view(torch.uint8).flatten()))
                    print(f"[Trainer] Initialized {self.n_slots}-slot circular buffer")

                @endpoint
                def get_latest(self) -> tuple:
                    if self.version == 0:
                        return None, -1
                    v = self.version - 1
                    return self.handles[v % self.n_slots], v

                @endpoint
                def train_step(self):
                    """Single training step: publish new weights."""
                    import sys
                    slot_idx = self.version % self.n_slots
                    self.slots[slot_idx].fill_(float(self.version))
                    self.version += 1
                    print(f"[Trainer] Published v{self.version - 1}")
                    sys.stdout.flush()
                    return self.version

            class Generator(Actor):
                """Generator that syncs weights and generates."""

                def __init__(self, weight_size: int, trainer):
                    self.gen_id = current_rank().rank
                    self.trainer = trainer
                    self.weights = torch.zeros(weight_size, dtype=torch.float32)
                    self.weight_bytes = self.weights.view(torch.uint8).flatten()
                    self.current_version = -1
                    print(f"[Gen {self.gen_id}] Initialized")

                @endpoint
                def generate_step(self) -> int:
                    """Single generate step: sync if needed, then generate."""
                    import sys
                    # Try to sync weights
                    handle, version = self.trainer.get_latest.call_one().get()
                    if handle is not None and version > self.current_version:
                        handle.read_into(self.weight_bytes).get()
                        actual = int(self.weights[0].item())
                        if actual >= version:  # >= not == : a later version may have written to same slot
                            self.current_version = actual
                            print(f"[Gen {self.gen_id}] Synced to v{actual}")
                            sys.stdout.flush()
                    return self.current_version

            # Spawn trainer and generators
            n_generators = 4
            trainer_proc = this_host().spawn_procs(per_host={"procs": 1})
            generator_procs = this_host().spawn_procs(per_host={"procs": n_generators})

            trainer = trainer_proc.spawn("trainer", Trainer, weight_size=1024)
            generators = generator_procs.spawn("generators", Generator, weight_size=1024, trainer=trainer)

            print("\n--- Running async RL simulation ---\n")

            # Results storage
            _results = {"trainer": None, "generators": [None] * n_generators}

            def run_trainer(n_steps, step_time):
                import time
                for _ in range(n_steps):
                    time.sleep(step_time)
                    _results["trainer"] = trainer.train_step.call_one().get()

            def run_generator(gen_actor, gen_idx, n_iters, gen_time):
                import time
                for _ in range(n_iters):
                    version = gen_actor.generate_step.call_one().get()
                    if version >= 0:
                        time.sleep(gen_time)
                        print(f"[Gen {gen_idx}] Generated (v{version})")
                    else:
                        time.sleep(0.05)
                _results["generators"][gen_idx] = version

            # Create threads for trainer and each generator
            threads = []

            # Trainer thread
            t = threading.Thread(target=run_trainer, args=(6, 1.0))  # 1 second per step
            threads.append(t)

            # Generator threads
            for i in range(n_generators):
                gen_actor = generators.slice(procs=i)
                t = threading.Thread(target=run_generator, args=(gen_actor, i, 5, 0.25))
                threads.append(t)

            # Start all threads
            for t in threads:
                t.start()

            # Wait for all to complete
            for t in threads:
                t.join()

            print(f"\n--- Done! Trainer published {_results['trainer']} versions ---")
            print(f"Generators ended on versions: {_results['generators']}")
            print("All pulled independently via RDMA, weights verified!")

    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"(Demo failed: {e})")
        show_fallback()
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## 9. Going Further: TorchStore

    All the patterns we've covered - RDMA memory registration, magic pointers, circular buffers,
    pre-computed transfer plans - are building blocks. If you need a **production-ready solution**,
    check out [TorchStore](https://github.com/meta-pytorch/torchstore).

    ### What is TorchStore?

    TorchStore is a **distributed, asynchronous key-value store for PyTorch tensors** built on
    Monarch's actor framework. It abstracts away the RDMA complexity while giving you:

    ```python
    from torchstore import TorchStore

    # Store tensors with async API
    await ts.put("model/layer1/weights", tensor)

    # Retrieve with optional in-place and slice semantics
    await ts.get("model/layer1/weights", inplace_tensor=buffer)

    # Native PyTorch checkpoint support
    await ts.put_state_dict(model.state_dict())
    loaded = await ts.get_state_dict()
    ```

    ### When to Use What

    | Scenario | Solution |
    |----------|----------|
    | Learning RDMA patterns | This notebook + 06b |
    | Custom RL weight sync | See 06b for `RDMABuffer` + `RDMAAction` patterns |
    | General tensor storage | Use TorchStore |
    | Checkpointing | Use TorchStore's `put_state_dict` |
    """)
    return


@app.cell
def _(mo):
    mo.md(r"""
    ## Summary

    ### Key Takeaways

    1. **Bandwidth hierarchy matters**: NVLink (900 GB/s) >> InfiniBand (50 GB/s)
       - Keep frequent operations on NVLink, use RDMA for cross-node

    2. **Collectives block, RL needs async**: High variance in generation times makes
       synchronous operations expensive

    3. **Magic pointer pattern**: Tiny handle over control plane, bulk data over data plane
       - ~100 bytes to describe 10 GB transfer

    4. **CPU staging**: Temporal decoupling for async RL
       - Nothing blocks on the critical path

    5. **Circular buffers**: Version weights without memory churn
       - Pre-register RDMA buffers, reuse slots

    6. **Weight re-sharding**: Different layouts need overlap computation
       - Routed approach avoids redundant transfers

    ### Want More?

    - **06b_weight_sync_deep_dive.py** - ibverbs internals, benchmarks, full implementations
    - **07_async_rl_e2e.py** - Complete async RL system using these patterns
    """)
    return


if __name__ == "__main__":
    app.run()
